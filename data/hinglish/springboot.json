[
  {
    "id": "spring-boot-mastery",
    "category": "Backend Development",
    "title": "Spring Boot Mastery Handbook",
    "subtitle": "A complete, end-to-end guide to Spring Boot covering fundamentals, web development, data access, configuration, security, observability, testing, and microservices architecture.",
    "icon": "spring-boot",
    "stats": {
      "sections": 6,
      "topics": 105
    },
    "sections": [
      {
        "id": "core-di",
        "title": "Core Spring Boot + Dependency Injection",
        "description": "Spring Boot ke fundamentals master karo jaise auto-configuration, embedded servers, IoC container kaise kaam karta hai, dependency injection patterns, stereotype annotations, aur advanced bean wiring strategies.",
        "topics": [
          {
            "id": "core-di-1",
            "title": "SpringApplication & @SpringBootApplication",
            "explanations": {
              "english": "SpringApplication class Spring Boot applications ke liye bootstrap entry point provide karta hai. @SpringBootApplication annotation ek convenience meta-annotation hai jo @Configuration, @EnableAutoConfiguration, aur @ComponentScan ko combine kar deta hai. Jab main class pe lagaya jata hai, toh ye component scanning start karta hai us package se niche ki taraf aur auto-configuration enable kar deta hai classpath contents ke basis pe. Ye single annotation XML configuration ya multiple Java config annotations ki zarurat khatam kar deta hai. Ye application context initialize karta hai, saare beans register karta hai, aur embedded web server start karta hai."
            },
            "code": {
              "title": "Standard Spring Boot Entry Point",
              "language": "java",
              "content": "@SpringBootApplication\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}"
            },
            "codeExplanations": {
              "english": "@SpringBootApplication is class ko configuration class mark karta hai aur auto-scanning enable karta hai. SpringApplication.run() application bootstrap karta hai, ApplicationContext create karta hai, auto-configuration perform karta hai, aur embedded server default port 8080 pe start karta hai."
            },
            "keyPoints": [
              "Combines @Configuration, @EnableAutoConfiguration, and @ComponentScan into one annotation",
              "Must be placed on the class containing the public static void main method",
              "Component scan starts from the package where this annotation is placed",
              "Triggers automatic configuration based on classpath dependencies"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-2",
            "title": "Auto-Configuration Magic",
            "explanations": {
              "english": "Spring Boot ka auto-configuration automatically aapki application configure karta hai classpath mein present jar dependencies ke basis pe. Ye @Conditional annotations jaise @ConditionalOnClass aur @ConditionalOnMissingBean use karta hai ye decide karne ke liye ki specific configurations apply hone chahiye ya nahi. For example, agar spring-webmvc detect hota hai, toh DispatcherServlet auto-configure ho jata hai. Ye convention-over-configuration approach boilerplate eliminate karta hai aur explicit bean overrides ya property settings ke through flexible bhi rehta hai."
            },
            "code": {
              "title": "Conditional Auto-Configuration Example",
              "language": "java",
              "content": "@Configuration\n@ConditionalOnClass(DataSource.class)\npublic class DataSourceAutoConfiguration {\n    @Bean\n    @ConditionalOnMissingBean\n    public DataSource dataSource(DataSourceProperties properties) {\n        return DataSourceBuilder.create()\n            .url(properties.getUrl())\n            .build();\n    }\n}"
            },
            "codeExplanations": {
              "english": "Ye configuration sirf tab activate hota hai jab DataSource.class classpath pe ho. @ConditionalOnMissingBean ensure karta hai ki ye bean sirf tab banaye jab user ne apna DataSource define nahi kiya ho, jo easy customization allow karta hai."
            },
            "keyPoints": [
              "Uses @Conditional annotations to make configuration decisions",
              "Triggered by the presence of specific classes in the classpath",
              "Can be disabled or excluded using @EnableAutoConfiguration(exclude={...})",
              "Auto-configuration classes are loaded from META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-3",
            "title": "Starter Dependencies",
            "explanations": {
              "english": "Starters curated sets hain dependency descriptors ke jo Maven aur Gradle configuration simplify karte hain. Ye transitive dependency management provide karte hain ensuring compatible versions of related libraries (Bill of Materials). Web development ke liye 10+ dependencies manually add karne ke bajaye, spring-boot-starter-web add karne se saare required web components verified compatibility ke saath aa jaate hain. Starters naming pattern spring-boot-starter-* follow karte hain aur version conflict issues eliminate karte hain."
            },
            "code": {
              "title": "Maven Starter Dependencies",
              "language": "xml",
              "content": "<dependencies>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-web</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-data-jpa</artifactId>\n    </dependency>\n</dependencies>"
            },
            "codeExplanations": {
              "english": "Har starter apne domain ke liye saari necessary dependencies pull karta hai. Version numbers parent POM ya BOM se inherit hote hain, ensuring compatibility across Spring Boot ecosystem."
            },
            "keyPoints": [
              "Curated dependency sets that simplify build configuration",
              "Ensure version compatibility through transitive dependency management",
              "Follow naming convention spring-boot-starter-*",
              "Eliminate the need to hunt for compatible library versions"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Starter",
                    "Purpose",
                    "Key Dependencies Included"
                  ],
                  "rows": [
                    [
                      "spring-boot-starter-web",
                      "Web apps with MVC",
                      "Tomcat, Spring MVC, Jackson"
                    ],
                    [
                      "spring-boot-starter-data-jpa",
                      "Database access",
                      "Hibernate, Spring Data, Transaction API"
                    ],
                    [
                      "spring-boot-starter-test",
                      "Testing support",
                      "JUnit, Mockito, AssertJ"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-4",
            "title": "Embedded Servers - Tomcat",
            "explanations": {
              "english": "Spring Boot mein Apache Tomcat default embedded servlet container ke roop mein included hai, jo WAR files ko external server pe deploy karne ki zarurat khatam karta hai. Tomcat programmatically start hota hai application process ke andar jab application run hoti hai, typically port 8080 pe listen karta hai. Ye embedded approach fully executable JAR files banane allow karta hai jo 'just run it' philosophy follow karte hain. Configuration server.port aur server.tomcat.max-threads jaise properties ke through manage hoti hai."
            },
            "code": {
              "title": "Tomcat Configuration Properties",
              "language": "properties",
              "content": "server.port=8080\nserver.tomcat.max-threads=200\nserver.tomcat.connection-timeout=5s"
            },
            "codeExplanations": {
              "english": "Ye properties embedded Tomcat instance ko configure karte hain. server.port listen port change karta hai, max-threads request processing ke liye pool size control karta hai, aur connection-timeout socket timeout values set karta hai."
            },
            "keyPoints": [
              "Default embedded servlet container for Spring Boot",
              "Supports executable JAR deployment model",
              "Configurable via application.properties using server.tomcat.* prefix",
              "Automatically started when spring-boot-starter-web is present"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-5",
            "title": "Embedded Servers - Netty",
            "explanations": {
              "english": "Netty reactive Spring Boot applications using WebFlux ke liye default embedded server ka kaam karta hai. Tomcat ke thread-per-request model ke against, Netty event-driven architecture use karta hai jo high concurrency handle kar sakta hai kam threads ke saath. Ye auto-configure hota hai jab spring-boot-starter-webflux classpath pe hota hai spring-boot-starter-web ke bajaye. Netty non-blocking I/O operations support karta hai jo reactive programming paradigms aur streaming data ke liye essential hain."
            },
            "code": {
              "title": "WebFlux with Netty",
              "language": "java",
              "content": "@SpringBootApplication\npublic class ReactiveApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(ReactiveApplication.class, args);\n    }\n}\n\n// Netty starts automatically on port 8080 when WebFlux starter is used"
            },
            "codeExplanations": {
              "english": "Jab spring-boot-starter-webflux hi single web starter present ho, Spring Boot auto-configure karta hai Netty ko Tomcat ke bajaye. Same SpringApplication bootstrap Netty server start karta hai embedded within the application."
            },
            "keyPoints": [
              "Default server for Spring WebFlux reactive applications",
              "Event-driven architecture supporting high concurrency",
              "Uses non-blocking I/O instead of thread-per-request",
              "Auto-configured when spring-boot-starter-webflux is detected"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "Tomcat",
                    "Netty"
                  ],
                  "rows": [
                    [
                      "Default with",
                      "spring-boot-starter-web",
                      "spring-boot-starter-webflux"
                    ],
                    [
                      "I/O Model",
                      "Blocking (Servlet)",
                      "Non-blocking (Reactive)"
                    ],
                    [
                      "Thread Model",
                      "Thread-per-request",
                      "Event loop with small thread pool"
                    ],
                    [
                      "Use Case",
                      "Traditional MVC",
                      "Reactive/Streaming APIs"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-6",
            "title": "Configuration Files - application.properties",
            "explanations": {
              "english": "application.properties file traditional key-value pairs use karti hai Spring Boot applications configure karne ke liye. Ye dot notation use karti hai hierarchical configuration keys represent karne ke liye jaise server.port=8080. Spring Boot automatically is file ko classpath root se load karta hai. Simple hone ke bawajood, deeply nested structures verbose ho sakti hain YAML ke comparison mein, requiring repetition of prefixes for related configuration groups."
            },
            "code": {
              "title": "Properties Configuration Example",
              "language": "properties",
              "content": "server.port=8080\nserver.servlet.context-path=/api\nspring.datasource.url=jdbc:postgresql://localhost:5432/mydb\nspring.datasource.username=admin\nspring.datasource.password=secret\nlogging.level.org.springframework=INFO"
            },
            "codeExplanations": {
              "english": "Har line specific property configure karti hai. Hierarchy dots use karke flattened hoti hai. Multiple related properties jaise datasource configuration ke liye spring.datasource prefix har line mein repeat karna padta hai."
            },
            "keyPoints": [
              "Key-value format using equals sign separator",
              "Dot notation represents configuration hierarchy",
              "Loaded automatically from classpath root",
              "Verbose for nested configuration structures"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-7",
            "title": "Configuration Files - application.yml",
            "explanations": {
              "english": "YAML (application.yml) hierarchical configuration format provide karta hai indentation aur colons use karke nested structures represent karne ke liye. Ye repetition eliminate karta hai common parent keys ke neeche related properties ko group karke. Complex configurations ke liye zyada readable hota hai jaise database connection pools ya profile-specific settings. Spring Boot dono formats ko simultaneously support karta hai, YAML mein defined properties potentially overriding ho sakti hain properties files mein defined ones ko loading order ke hisaab se."
            },
            "code": {
              "title": "YAML Configuration Structure",
              "language": "yaml",
              "content": "server:\n  port: 8080\n  servlet:\n    context-path: /api\n\nspring:\n  datasource:\n    url: jdbc:postgresql://localhost:5432/mydb\n    username: admin\n    password: secret\n    hikari:\n      maximum-pool-size: 10\n      connection-timeout: 30000"
            },
            "codeExplanations": {
              "english": "YAML indentation use karta hai hierarchy denote karne ke liye, eliminating the need to repeat 'spring.datasource' for every sub-property. Lists aur maps naturally represent ho sakte hain. Hikari configuration datasource ke neeche cleanly nested hai bina prefix repetition ke."
            },
            "keyPoints": [
              "Hierarchical format using indentation and colons",
              "Eliminates repetition of common prefixes",
              "Supports lists and complex objects natively",
              "More readable for deeply nested configurations"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "Properties",
                    "YAML"
                  ],
                  "rows": [
                    [
                      "Syntax",
                      "Key=value pairs",
                      "Indented hierarchy with colons"
                    ],
                    [
                      "Verbosity",
                      "High for nested keys",
                      "Low, DRY principle"
                    ],
                    [
                      "Lists",
                      "Comma-separated or indexed",
                      "Natural hyphen notation"
                    ],
                    [
                      "Profiles",
                      "Separate files or suffixes",
                      "Document separators with ---"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-8",
            "title": "Inversion of Control (IoC)",
            "explanations": {
              "english": "Inversion of Control ek design principle hai jahan object creation aur lifecycle management ka control application code se container ya framework ko transfer kar diya jata hai. Classes ke bajaye 'new' keyword use karke apni dependencies instantiate karne ke, container required dependencies inject karta hai. Ye components ko decouple karta hai, testability enhance karta hai mocking ke through, aur Single Responsibility Principle promote karta hai by removing object creation concerns from business logic."
            },
            "code": {
              "title": "IoC Concept Example",
              "language": "java",
              "content": "// Without IoC - tight coupling\npublic class OrderService {\n    private DatabaseRepository repo = new DatabaseRepository(); // tight coupling\n}\n\n// With IoC - container injects dependency\npublic class OrderService {\n    private final Repository repo;\n    public OrderService(Repository repo) { // injected by container\n        this.repo = repo;\n    }\n}"
            },
            "codeExplanations": {
              "english": "First example concrete implementation ke saath tight coupling create karta hai. Second example IoC dikhata hai jahan dependency externally container ke dwara provide ki jaati hai, allowing different implementations ko bina code changes ke inject karna."
            },
            "keyPoints": [
              "Container manages object creation and wiring instead of application code",
              "Decouples components from their dependencies",
              "Enables easier unit testing through dependency substitution",
              "Foundation principle of the Spring Framework"
            ],
            "extras": {
              "flowDiagram": "Traditional: Application -> new Service() -> new Repository()\nIoC: Container -> creates Repository -> creates Service -> injects Repository -> Application uses Service",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-9",
            "title": "Bean Creation Lifecycle",
            "explanations": {
              "english": "Spring beans defined lifecycle follow karte hain jo container manage karta hai: instantiation using constructors, properties aur dependencies ka population, BeanNameAware/BeanFactoryAware callbacks, pre-initialization BeanPostProcessor calls, initialization via @PostConstruct ya InitializingBean, aur finally destruction via @PreDestroy ya DisposableBean. In phases ko samajhna crucial hai proper resource management, initialization logic, aur cleanup operations ke liye."
            },
            "code": {
              "title": "Lifecycle Callbacks",
              "language": "java",
              "content": "@Component\npublic class MyBean implements InitializingBean, DisposableBean {\n    \n    @PostConstruct\n    public void init() {\n        // Called after properties set\n    }\n    \n    @Override\n    public void afterPropertiesSet() {\n        // Alternative initialization hook\n    }\n    \n    @PreDestroy\n    public void preDestroy() {\n        // Cleanup before container shutdown\n    }\n    \n    @Override\n    public void destroy() {\n        // Alternative destruction hook\n    }\n}"
            },
            "codeExplanations": {
              "english": "@PostConstruct modern annotation-based approach hai initialization ke liye after dependencies inject ho jaate hain. @PreDestroy call hota hai bean destroy hone se pehle. Interfaces InitializingBean aur DisposableBean older alternatives hain lekin same purpose serve karte hain."
            },
            "keyPoints": [
              "Container manages bean instantiation through destruction",
              "@PostConstruct called after dependency injection completes",
              "@PreDestroy called during container shutdown for cleanup",
              "BeanPostProcessors can wrap beans for proxying during initialization"
            ],
            "extras": {
              "flowDiagram": "Instantiate -> Populate Properties -> Set Bean Name/Factory -> BeanPostProcessor.preInit -> @PostConstruct -> BeanPostProcessor.postInit -> Ready -> @PreDestroy -> Destroy",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-10",
            "title": "ApplicationContext vs BeanFactory",
            "explanations": {
              "english": "BeanFactory basic IoC container hai providing bean instantiation aur dependency injection. ApplicationContext BeanFactory extend karta hai aur enterprise application features add karta hai: AOP integration, internationalization (i18n), event propagation, aur web-application specific contexts. ApplicationContext singleton beans eager load karta hai by default startup pe, jabki BeanFactory lazy loading use karta hai. Modern Spring Boot mein, ApplicationContext hamesha use hota hai kyunki ye complete infrastructure provide karta hai enterprise applications ke liye."
            },
            "code": {
              "title": "Container Types",
              "language": "java",
              "content": "// BeanFactory - basic IoC\nBeanFactory factory = new XmlBeanFactory(new ClassPathResource(\"beans.xml\"));\n\n// ApplicationContext - full-featured\nApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);\n// Or in Spring Boot:\nApplicationContext context = SpringApplication.run(Application.class, args);"
            },
            "codeExplanations": {
              "english": "BeanFactory rarely directly use kiya jata hai modern applications mein. ApplicationContext Spring Boot applications ke liye foundation provide karta hai including annotation processing, event publishing, aur resource loading."
            },
            "keyPoints": [
              "BeanFactory provides basic bean management and DI",
              "ApplicationContext extends BeanFactory with enterprise features",
              "ApplicationContext eager-loads singletons by default",
              "ApplicationContext supports AOP, events, and web-specific scopes"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "BeanFactory",
                    "ApplicationContext"
                  ],
                  "rows": [
                    [
                      "Loading",
                      "Lazy (on-demand)",
                      "Eager (by default)"
                    ],
                    [
                      "AOP Integration",
                      "Requires manual setup",
                      "Built-in support"
                    ],
                    [
                      "MessageSource (i18n)",
                      "No",
                      "Yes"
                    ],
                    [
                      "Event Propagation",
                      "No",
                      "ApplicationEventPublisher"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-11",
            "title": "@Autowired",
            "explanations": {
              "english": "@Autowired automatic dependency injection enable karta hai type ke through. Spring application context se dependency resolve karta hai aur inject karta hai fields, constructors, ya setter methods mein. Agar matching bean nahi milta, toh NoSuchBeanDefinitionException throw karta hai. Agar multiple beans type match karte hain, toh NoUniqueBeanDefinitionException throw karta hai unless @Primary ya @Qualifier se resolve kiya jaye. Ye manual bean lookup through ApplicationContext.getBean() calls eliminate karta hai."
            },
            "code": {
              "title": "Autowired Usage Patterns",
              "language": "java",
              "content": "@Service\npublic class UserService {\n    \n    @Autowired // Field injection (not recommended)\n    private UserRepository fieldRepo;\n    \n    private final UserRepository constructorRepo;\n    \n    @Autowired // Constructor injection (recommended)\n    public UserService(UserRepository repo) {\n        this.constructorRepo = repo;\n    }\n    \n    @Autowired // Setter injection\n    public void setEmailService(EmailService service) {\n        this.emailService = service;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Teen injection modes demonstrate karta hai: field (reflection use karta hai), constructor (required dependencies ke liye preferred), aur setter (optional dependencies ke liye). Spring 4.3 se starting, @Autowired constructors pe optional hai agar sirf ek constructor exist karta hai."
            },
            "keyPoints": [
              "Injects dependencies by type from the application context",
              "Can be applied to fields, constructors, and setter methods",
              "Required by default; use required=false for optional dependencies",
              "Requires disambiguation when multiple beans of same type exist"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-12",
            "title": "Constructor Injection vs Field Injection",
            "explanations": {
              "english": "Constructor injection dependencies ko constructor parameters declare karta hai, unhe required banata hai aur class ko immutable banne allow karta hai (final fields). Ye ensure karta hai ki object creation pe valid state mein ho aur dependencies explicit bana deta hai. Field injection reflection use karta hai private fields directly set karne ke liye, dependencies ko public API se chhupata hai aur unit testing mushkil bana deta hai bina Spring container ke. Constructor injection mandatory dependencies ke liye recommended approach hai."
            },
            "code": {
              "title": "Constructor vs Field Comparison",
              "language": "java",
              "content": "// Field Injection - Hidden dependencies\n@Service\npublic class OrderService {\n    @Autowired\n    private PaymentGateway gateway; // Hidden dependency\n}\n\n// Constructor Injection - Explicit contract\n@Service\npublic class OrderService {\n    private final PaymentGateway gateway;\n    \n    public OrderService(PaymentGateway gateway) {\n        this.gateway = gateway; // Null-safe assignment\n    }\n}"
            },
            "codeExplanations": {
              "english": "Field injection cleaner dikhta hai lekin class requirements chhupata hai. Constructor injection contract explicit bana deta hai constructor signature ke through, final fields ke liye immutability support karta hai, aur easy instantiation allow karta hai unit tests mein bina reflection ke."
            },
            "keyPoints": [
              "Constructor injection enforces required dependencies at object creation",
              "Field injection hides dependencies and complicates unit testing",
              "Constructor injection enables immutable dependencies (final fields)",
              "Constructor injection reveals circular dependencies at startup"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Characteristic",
                    "Constructor",
                    "Field"
                  ],
                  "rows": [
                    [
                      "Testability",
                      "Easy - pass mocks to constructor",
                      "Requires reflection or Spring context"
                    ],
                    [
                      "Immutability",
                      "Supports final fields",
                      "Cannot use final"
                    ],
                    [
                      "Null Safety",
                      "Guaranteed non-null after construction",
                      "Possible null if not injected"
                    ],
                    [
                      "Circular Dependencies",
                      "Detected at startup",
                      "May cause runtime issues"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-13",
            "title": "Dependency Injection Best Practices",
            "explanations": {
              "english": "Hamesha constructor injection use karo mandatory dependencies ke liye taki objects instantiation pe valid rahein. Production code mein field injection avoid karo Spring ke test utilities ke bina testability maintain karne ke liye. Injected fields ko final declare karo immutability enforce karne aur reassignment prevent karne ke liye. Constructor arguments ki number manageable rakho; agar 4-5 dependencies se zyada ho jaayein, toh Facade pattern use karke refactoring consider karo ya class ko responsibility ke hisaab se split karo."
            },
            "code": {
              "title": "Recommended Pattern",
              "language": "java",
              "content": "@Service\npublic class OrderProcessingService {\n    private final OrderRepository orderRepository;\n    private final PaymentService paymentService;\n    private final NotificationService notificationService;\n    \n    public OrderProcessingService(OrderRepository orderRepository,\n                                  PaymentService paymentService,\n                                  NotificationService notificationService) {\n        this.orderRepository = orderRepository;\n        this.paymentService = paymentService;\n        this.notificationService = notificationService;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Ye example proper constructor injection dikhata hai final fields ke saath. Dependencies immutable hain, explicitly required hain, aur easily mockable hain unit tests mein. Lombok ka @RequiredArgsConstructor is pattern ke liye boilerplate reduce kar sakta hai."
            },
            "keyPoints": [
              "Use constructor injection for all mandatory dependencies",
              "Declare injected fields as final when possible",
              "Limit constructor parameters to avoid God classes",
              "Avoid field injection except in configuration classes"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-14",
            "title": "@Component",
            "explanations": {
              "english": "@Component generic stereotype annotation hai jo class ko Spring-managed bean mark karta hai. Component scanning ke dauraan, Spring @Component se annotated classes detect karta hai aur unhe ApplicationContext mein beans ke roop mein register karta hai. Ye @Service, @Repository, aur @Controller ka parent meta-annotation hai. @Component use karo jab class specialized stereotypes mein fit nahi hoti lekin Spring management phir bhi chahiye."
            },
            "code": {
              "title": "Generic Component",
              "language": "java",
              "content": "@Component\npublic class CacheManager {\n    private final Map<String, Object> cache = new ConcurrentHashMap<>();\n    \n    public Object get(String key) {\n        return cache.get(key);\n    }\n    \n    public void put(String key, Object value) {\n        cache.put(key, value);\n    }\n}"
            },
            "codeExplanations": {
              "english": "CacheManager ek generic utility class hai jo service, repository, ya controller categories mein fit nahi hoti lekin Spring se singleton lifecycle management chahiye. @Component automatic detection aur registration enable karta hai."
            },
            "keyPoints": [
              "Generic stereotype for any Spring-managed component",
              "Enables detection during classpath scanning",
              "Parent annotation for @Service, @Repository, @Controller",
              "Use when no specialized stereotype applies"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-15",
            "title": "@Service",
            "explanations": {
              "english": "@Service @Component ka specialization hai jo indicate karta hai ki class service layer mein business logic hold karti hai. Ye @Component ke beyond koi additional technical behavior carry nahi karta lekin semantic marker ka kaam karta hai jo class ke architectural role ko clarify karta hai. Service classes typically transactional boundaries, business rules, aur repositories ke beech coordination contain karti hain. Ye annotation AOP pointcut targeting mein help karta hai aur code readability improve karta hai."
            },
            "code": {
              "title": "Service Layer Implementation",
              "language": "java",
              "content": "@Service\npublic class AccountService {\n    private final AccountRepository repository;\n    \n    public AccountService(AccountRepository repository) {\n        this.repository = repository;\n    }\n    \n    @Transactional\n    public void transfer(Long fromId, Long toId, BigDecimal amount) {\n        Account from = repository.findById(fromId).orElseThrow();\n        Account to = repository.findById(toId).orElseThrow();\n        from.debit(amount);\n        to.credit(amount);\n        repository.save(from);\n        repository.save(to);\n    }\n}"
            },
            "codeExplanations": {
              "english": "@Service annotation isse business logic holder mark karta hai. Transfer method ek transactional business operation represent karta hai multiple repository calls coordinate karne wala. Annotation indicate karta hai ki ye class domain-driven design mein service layer ko belong karti hai."
            },
            "keyPoints": [
              "Semantic marker for service layer business logic",
              "Specialization of @Component",
              "Typical location for @Transactional boundaries",
              "Improves architectural clarity and AOP targeting"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-16",
            "title": "@Repository",
            "explanations": {
              "english": "@Repository persistence layer mein classes mark karta hai jo data access aur storage deal karte hain. Ye persistence-specific exceptions (jaise SQLException) ko Spring ke unified DataAccessException hierarchy mein translate karta hai. Ye exception translation enable karta hai across different data access technologies (JPA, JDBC, MongoDB). Ye automatic exception translation aur transaction management ke liye proxy creation trigger karne ke liye marker ka kaam bhi karta hai kuch configurations mein."
            },
            "code": {
              "title": "Data Repository",
              "language": "java",
              "content": "@Repository\npublic class JdbcProductRepository {\n    private final JdbcTemplate jdbcTemplate;\n    \n    public JdbcProductRepository(JdbcTemplate jdbcTemplate) {\n        this.jdbcTemplate = jdbcTemplate;\n    }\n    \n    public Product findById(Long id) {\n        try {\n            return jdbcTemplate.queryForObject(\n                \"SELECT * FROM products WHERE id = ?\", \n                new ProductRowMapper(), id);\n        } catch (DataAccessException e) {\n            throw new ProductNotFoundException(\"Product not found: \" + id, e);\n        }\n    }\n}"
            },
            "codeExplanations": {
              "english": "@Repository SQL exceptions ko Spring ke DataAccessException mein automatic translation enable karta hai. Ye class ko persistence layer mein data access object mark karta hai, distinct from services ya controllers."
            },
            "keyPoints": [
              "Marks persistence layer components",
              "Enables exception translation to DataAccessException",
              "Indicates the class interacts with data storage",
              "Specialization of @Component for DAOs"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-17",
            "title": "@Controller",
            "explanations": {
              "english": "@Controller Spring MVC controller indicate karta hai jo traditional server-rendered web applications mein HTTP requests handle karta hai. @Controller se annotated classes view names return karte hain jo ViewResolver implementations (jaise Thymeleaf ya JSP resolvers) dwara resolve hote hain. Ye @Component ko web-specific functionality ke saath combine karta hai. Methods typically String view names ya ModelAndView objects return karte hain jo view name aur model data dono contain karte hain."
            },
            "code": {
              "title": "MVC Controller",
              "language": "java",
              "content": "@Controller\n@RequestMapping(\"/products\")\npublic class ProductController {\n    private final ProductService productService;\n    \n    @GetMapping\n    public String listProducts(Model model) {\n        model.addAttribute(\"products\", productService.findAll());\n        return \"product/list\"; // View name resolved by Thymeleaf\n    }\n    \n    @GetMapping(\"/{id}\")\n    public ModelAndView productDetail(@PathVariable Long id) {\n        ModelAndView mav = new ModelAndView(\"product/detail\");\n        mav.addObject(\"product\", productService.findById(id));\n        return mav;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Ye controller traditional web requests handle karta hai view names return karke. 'product/list' string ViewResolver dwara actual HTML template mein resolve hoti hai. Model attributes view template mein accessible hote hain."
            },
            "keyPoints": [
              "Used for traditional MVC with server-rendered views",
              "Methods return view names resolved by ViewResolver",
              "Combines @Component with web controller semantics",
              "Works with template engines like Thymeleaf, JSP, FreeMarker"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-18",
            "title": "@RestController",
            "explanations": {
              "english": "@RestController ek convenience annotation hai jo @Controller aur @ResponseBody combine karta hai. Ye indicate karta hai ki class REST API requests handle karta hai data (JSON ya XML) return karke rather than views. @ResponseBody annotation ensure karta hai ki return values directly HTTP response body mein serialize hote hain HttpMessageConverters use karke. Ye har handler method pe @ResponseBody annotate karne ki zarurat khatam karta hai jab RESTful services build kar rahe ho."
            },
            "code": {
              "title": "REST API Controller",
              "language": "java",
              "content": "@RestController\n@RequestMapping(\"/api/users\")\npublic class UserRestController {\n    private final UserService userService;\n    \n    @GetMapping\n    public List<User> getAllUsers() {\n        return userService.findAll();\n    }\n    \n    @PostMapping\n    public ResponseEntity<User> createUser(@RequestBody User user) {\n        User created = userService.save(user);\n        URI location = ServletUriComponentsBuilder\n            .fromCurrentRequest().path(\"/{id}\")\n            .buildAndExpand(created.getId()).toUri();\n        return ResponseEntity.created(location).body(created);\n    }\n}"
            },
            "codeExplanations": {
              "english": "@RestController ensure karta hai ki saare methods data directly client ko return karein. User objects automatically JSON mein convert hote hain. ResponseEntity HTTP status codes aur headers pe full control provide karta hai."
            },
            "keyPoints": [
              "Combines @Controller and @ResponseBody",
              "Returns data objects serialized to HTTP body (JSON/XML)",
              "Eliminates repetitive @ResponseBody annotations",
              "Ideal for building RESTful APIs without view resolution"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "@Controller",
                    "@RestController"
                  ],
                  "rows": [
                    [
                      "Return Type",
                      "View name (String)",
                      "Data object (JSON/XML)"
                    ],
                    [
                      "View Resolution",
                      "Yes, via ViewResolver",
                      "No, direct HTTP body"
                    ],
                    [
                      "Use Case",
                      "Server-rendered web apps",
                      "REST APIs"
                    ],
                    [
                      "Implicit Annotation",
                      "@Component",
                      "@Controller + @ResponseBody"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-19",
            "title": "@Configuration",
            "explanations": {
              "english": "@Configuration ek class ko bean definitions ka source mark karta hai using explicit Java code XML ke bajaye. Class @Bean methods declare karti hai jo Spring container process karta hai bean definitions generate aur service requests ke liye runtime pe. Ye indicate karta hai ki class beans create karne ke liye factory methods contain karti hai. Configuration classes method calls ke through inter-bean dependencies support karti hain aur @Import use karke doosri configurations mein import ho sakti hain."
            },
            "code": {
              "title": "Java Configuration Class",
              "language": "java",
              "content": "@Configuration\npublic class AppConfig {\n    \n    @Bean\n    public RestTemplate restTemplate() {\n        return new RestTemplateBuilder()\n            .setConnectTimeout(Duration.ofSeconds(5))\n            .build();\n    }\n    \n    @Bean\n    public CacheManager cacheManager() {\n        return new ConcurrentMapCacheManager(\"users\", \"products\");\n    }\n}"
            },
            "codeExplanations": {
              "english": "@Configuration classes ke andar @Bean se annotated methods beans produce karte hain Spring ke dwara manage ki jaane wali. Jab ek @Bean method doosra call karta hai, Spring call intercept karke existing singleton bean return karta hai rather than naya instance create karne ke."
            },
            "keyPoints": [
              "Indicates the class contains bean factory methods",
              "Alternative to XML configuration",
              "Supports inter-bean dependencies via method calls",
              "Can be combined with @ComponentScan for mixed configuration"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-20",
            "title": "@Bean",
            "explanations": {
              "english": "@Bean declare karta hai ki method ek bean produce karta hai Spring container ke dwara manage hone ke liye. @Configuration classes mein use kiya jata hai, method name typically bean name ban jaata hai. Ye bean instantiation pe explicit control allow karta hai, particularly useful third-party classes ke liye jo @Component se annotate nahi ho sakti ya jinhe complex initialization logic chahiye. initMethod, destroyMethod, aur scope attributes specify karne ko support karta hai."
            },
            "code": {
              "title": "Explicit Bean Definition",
              "language": "java",
              "content": "@Configuration\npublic class DatabaseConfig {\n    \n    @Bean(initMethod = \"start\", destroyMethod = \"stop\")\n    public DataSource dataSource(\n            @Value(\"${db.url}\") String url,\n            @Value(\"${db.username}\") String username) {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(url);\n        config.setUsername(username);\n        return new HikariDataSource(config);\n    }\n    \n    @Bean(name = \"customObjectMapper\")\n    @Primary\n    public ObjectMapper objectMapper() {\n        ObjectMapper mapper = new ObjectMapper();\n        mapper.registerModule(new JavaTimeModule());\n        return mapper;\n    }\n}"
            },
            "codeExplanations": {
              "english": "dataSource bean third-party HikariCP ko external property injection ke saath integrate karna dikhata hai. objectMapper bean custom naming @Bean(name) aur @Primary ke saath demonstrate karta hai preference ke liye jab multiple beans exist karte hain."
            },
            "keyPoints": [
              "Declares explicit bean creation in configuration classes",
              "Method name becomes bean name by default",
              "Used for third-party library integration",
              "Supports lifecycle callbacks and custom bean naming"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-21",
            "title": "Manual Bean Wiring",
            "explanations": {
              "english": "Manual bean wiring @Bean methods use karke complete control provide karta hai object construction pe jab auto-scanning insufficient ya impossible ho. Ye third-party libraries integrate karne ke liye, conditions ke basis pe beans create karne ke liye profiles use karke, ya complex initialization sequences implement karne ke liye required hota hai. Component scanning ke against, manual wiring dependencies ko configuration class mein explicit banata hai aur bean instantiation order aur property setting pe precise control allow karta hai."
            },
            "code": {
              "title": "Complex Bean Wiring",
              "language": "java",
              "content": "@Configuration\npublic class IntegrationConfig {\n    \n    private final ApiProperties properties;\n    \n    public IntegrationConfig(ApiProperties properties) {\n        this.properties = properties;\n    }\n    \n    @Bean\n    public ApiClient apiClient() {\n        // Complex construction logic not possible with simple @Component\n        ApiClient client = new ApiClient(properties.getEndpoint());\n        client.setRateLimiter(new RateLimiter(properties.getRateLimit()));\n        client.setRetryPolicy(new ExponentialBackoffRetry());\n        return client;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Ye example third-party ApiClient ko wire karna dikhata hai jo constructor arguments aur setter configuration require karta hai jo simple component scanning se achieve nahi ho sakta. Configuration class khud constructor injection use karta hai apni dependencies ke liye."
            },
            "keyPoints": [
              "Required for third-party classes without @Component",
              "Enables complex initialization logic",
              "Allows conditional bean creation based on environment",
              "Provides explicit control over dependency instantiation"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-22",
            "title": "@Qualifier",
            "explanations": {
              "english": "@Qualifier @Autowired ke saath use hota hai Spring container mein same type ke multiple beans ke beech disambiguate karne ke liye. Ye specify karta hai ki kaunsa named bean inject kiya jana chahiye jab type-based resolution multiple candidates find karti hai. Value attribute bean identifier se match karna chahiye (either method name @Bean definitions ke liye ya component name). Ye NoUniqueBeanDefinitionException resolve karta hai available candidates mein se explicitly select karke."
            },
            "code": {
              "title": "Disambiguating Bean Selection",
              "language": "java",
              "content": "@Configuration\npublic class NotificationConfig {\n    @Bean\n    public NotificationService emailNotifier() {\n        return new EmailNotificationService();\n    }\n    \n    @Bean\n    public NotificationService smsNotifier() {\n        return new SmsNotificationService();\n    }\n}\n\n@Service\npublic class AlertService {\n    private final NotificationService notifier;\n    \n    public AlertService(@Qualifier(\"emailNotifier\") NotificationService notifier) {\n        this.notifier = notifier;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Do NotificationService beans exist karte hain. Bina @Qualifier ke, Spring NoUniqueBeanDefinitionException throw karega. @Qualifier annotation specify karta hai ki 'emailNotifier' naam ka bean AlertService mein inject kiya jaana chahiye."
            },
            "keyPoints": [
              "Resolves ambiguity when multiple beans of same type exist",
              "Used with @Autowired to specify bean by name",
              "Value must match the target bean's identifier",
              "Prevents NoUniqueBeanDefinitionException"
            ],
            "extras": {
              "flowDiagram": "AlertService -> @Qualifier(\"emailNotifier\") -> emailNotifier bean\n               -> smsNotifier bean (ignored)",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "core-di-23",
            "title": "@Primary",
            "explanations": {
              "english": "@Primary ek bean ko preferred candidate mark karta hai jab same type ke multiple beans autowiring ke candidates hote hain. Jab us type ki dependency request ki jaati hai aur koi specific @Qualifier provide nahi kiya gaya hai, toh @Primary bean choose hota hai. Ye common case ke liye @Qualifier annotations ki zarurat kam karta hai jabki specific injection non-primary beans ki allow karta hai jab zarurat ho. Sirf ek @Primary bean allowed hai per type given context mein."
            },
            "code": {
              "title": "Primary Bean Preference",
              "language": "java",
              "content": "@Configuration\npublic class StorageConfig {\n    @Primary\n    @Bean\n    public FileStorageService localStorage() {\n        return new LocalFileStorageService();\n    }\n    \n    @Bean\n    public FileStorageService cloudStorage() {\n        return new CloudFileStorageService();\n    }\n}\n\n@Service\npublic class DocumentService {\n    // Injects localStorage (the primary bean)\n    public DocumentService(FileStorageService storage) {}\n}\n\n@Service\npublic class BackupService {\n    // Explicitly injects cloudStorage\n    public BackupService(@Qualifier(\"cloudStorage\") FileStorageService storage) {}\n}"
            },
            "codeExplanations": {
              "english": "FileStorageService ke do implementations hain. localStorage pe @Primary isse default banata hai sab injection points ke liye. BackupService phir bhi cloudStorage access kar sakta hai @Qualifier use karke jab specific behavior required ho."
            },
            "keyPoints": [
              "Marks the default bean when multiple candidates exist",
              "Eliminates need for @Qualifier for the common case",
              "Only one bean per type can be @Primary",
              "Can be overridden by explicit @Qualifier usage"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Scenario",
                    "Behavior"
                  ],
                  "rows": [
                    [
                      "No @Primary, no @Qualifier",
                      "NoUniqueBeanDefinitionException thrown"
                    ],
                    [
                      "@Primary present, no @Qualifier",
                      "Primary bean injected"
                    ],
                    [
                      "@Qualifier present",
                      "Specified bean injected (ignores @Primary)"
                    ],
                    [
                      "Multiple @Primary",
                      "BeanDefinitionException at startup"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "core-di-24",
            "title": "Resolving Multiple Bean Conflicts",
            "explanations": {
              "english": "Jab Spring single injection point ke liye multiple eligible beans detect karta hai, toh NoUniqueBeanDefinitionException throw karta hai. Resolution strategies include: @Primary use karna default candidate mark karne ke liye us type ke liye, @Qualifier use karna explicitly desired bean specify karne ke liye, ya saare candidates ko List ya Map mein collect karna dependency type ke. Collections ke liye, Spring matching type ke saare beans inject karta hai. In resolution mechanisms ko samajhna startup failures prevent karta hai aur complex application architectures mein flexibility provide karta hai."
            },
            "code": {
              "title": "Conflict Resolution Strategies",
              "language": "java",
              "content": "@Service\npublic class NotificationDispatcher {\n    // Strategy 1: Inject all implementations into a List\n    private final List<NotificationService> allNotifiers;\n    \n    // Strategy 2: Inject all into a Map (key = bean name)\n    private final Map<String, NotificationService> notifierMap;\n    \n    // Strategy 3: Use @Primary for default\n    private final NotificationService primaryNotifier;\n    \n    // Strategy 4: Use @Qualifier for specific\n    public NotificationDispatcher(\n            List<NotificationService> allNotifiers,\n            Map<String, NotificationService> notifierMap,\n            NotificationService primaryNotifier,\n            @Qualifier(\"smsNotifier\") NotificationService specificNotifier) {\n        this.allNotifiers = allNotifiers;\n        this.notifierMap = notifierMap;\n        this.primaryNotifier = primaryNotifier;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Char resolution approaches demonstrate karta hai: 1) List injection saare beans receive karta hai, 2) Map injection saare beans unke names as keys ke saath receive karta hai, 3) @Primary default determine karta hai, 4) @Qualifier exact bean specify karta hai. Ye same application mein requirements ke basis pe combine kiye ja sakte hain."
            },
            "keyPoints": [
              "Use @Primary to designate the default bean",
              "Use @Qualifier to select specific beans by name",
              "Inject List<Type> or Map<String, Type> to receive all beans",
              "Plan bean naming strategy to support @Qualifier usage"
            ],
            "extras": {
              "flowDiagram": "Multiple Beans -> Decision: @Qualifier? -> Yes -> Specific Bean\n                        -> No -> @Primary? -> Yes -> Primary Bean\n                                    -> No -> Collection? -> Yes -> All Beans\n                                                -> No -> Exception",
              "comparisonTable": [],
              "examples": []
            }
          }
        ]
      },
      {
        "id": "web-rest-layer",
        "title": "Web / REST Layer",
        "description": "Spring MVC architecture, RESTful controller design, request/response handling, aur global exception management master karo robust web APIs aur traditional web applications build karne ke liye.",
        "topics": [
          {
            "id": "web-rest-1",
            "title": "DispatcherServlet (Front Controller)",
            "explanations": {
              "english": "DispatcherServlet Spring MVC mein front controller ka kaam karta hai, saare incoming HTTP requests receive karta hai aur unhe appropriate handler components ko delegate karta hai. Ye central entry point hai jo request processing coordinate karta hai HandlerMappings consult karke target controller determine karne ke liye, HandlerAdapters invoke karke methods execute karne ke liye, aur ViewResolvers process karke rendering ke liye. Spring Boot mein ye auto-configured rehta hai aur root path pe mapped hota hai by default. Ye poora request lifecycle manage karta hai including exception handling, interceptor execution, aur view rendering coordination."
            },
            "code": {
              "title": "Custom DispatcherServlet Configuration",
              "language": "java",
              "content": "@Configuration\npublic class WebConfig {\n    \n    @Bean\n    public ServletRegistrationBean<DispatcherServlet> dispatcherServletRegistration() {\n        ServletRegistrationBean<DispatcherServlet> registration = new ServletRegistrationBean<>(\n            new DispatcherServlet(), \"/api/*\"\n        );\n        registration.setLoadOnStartup(1);\n        registration.setName(\"api-dispatcher\");\n        return registration;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Jabki Spring Boot auto-configure karta hai DispatcherServlet ko, ye dikhata hai ki isse kaise customize kiya jaaye uska mapping. Servlet /api/* pe map kiya gaya hai root ke bajaye, load order startup priority 1 pe set kiya gaya hai. Normally customization properties ke through kiya jata hai explicit bean definition ke bajaye."
            },
            "keyPoints": [
              "Central entry point for all HTTP requests in Spring MVC",
              "Delegates to HandlerMappings, HandlerAdapters, and ViewResolvers",
              "Auto-configured by Spring Boot with default mapping to root path",
              "Manages the complete request processing workflow and exception translation"
            ],
            "extras": {
              "flowDiagram": "HTTP Request -> DispatcherServlet -> HandlerMapping -> Controller -> HandlerAdapter -> ViewResolver -> View -> HTTP Response",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "web-rest-2",
            "title": "Handler Mappings",
            "explanations": {
              "english": "Handler Mappings incoming HTTP requests ko appropriate controller handler methods pe map karte hain URL patterns, HTTP methods, request parameters, aur headers jaise criteria ke basis pe. RequestMappingHandlerMapping default implementation hai jo @RequestMapping annotations process karta hai. Ye saare controller methods ka mapping registry maintain karta hai aur incoming requests ke against pattern matching perform karta hai. Advanced configurations mein path prefixes, custom matching strategies, aur interceptor mappings include ho sakte hain."
            },
            "code": {
              "title": "Handler Mapping Configuration",
              "language": "java",
              "content": "@Configuration\npublic class WebMvcConfig implements WebMvcConfigurer {\n    \n    @Override\n    public void configurePathMatch(PathMatchConfigurer configurer) {\n        configurer.addPathPrefix(\"/api/v1\", HandlerTypePredicate.forAnnotation(RestController.class));\n    }\n    \n    @Bean\n    public HandlerMapping customHandlerMapping() {\n        RequestMappingHandlerMapping mapping = new RequestMappingHandlerMapping();\n        mapping.setUseSuffixPatternMatch(false);\n        mapping.setUseTrailingSlashMatch(true);\n        return mapping;\n    }\n}"
            },
            "codeExplanations": {
              "english": "configurePathMatch saare @RestController classes ke liye global /api/v1 prefix add karta hai. Custom HandlerMapping bean suffix pattern matching disable karta hai (.json/.xml extensions avoid karne ke liye) jabki trailing slashes ko URLs mein allow karta hai flexibility ke liye."
            },
            "keyPoints": [
              "Maps requests to controller methods based on URL patterns and HTTP methods",
              "RequestMappingHandlerMapping processes @RequestMapping annotations by default",
              "Supports path prefixes and custom matching strategies",
              "Integrates with interceptors for cross-cutting request processing"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "web-rest-3",
            "title": "View Resolvers",
            "explanations": {
              "english": "View Resolvers logical view names jo controllers return karte hain unhe actual view implementations mein translate karte hain response render karne ke liye. Traditional MVC applications mein, InternalResourceViewResolver JSP views resolve karta hai jabki ThymeleafViewResolver Thymeleaf templates handle karta hai. Ye view technology determine karta hai, template resource locate karta hai, aur view instance prepare karta hai necessary configuration ke saath. REST applications mein @RestController ke saath, view resolution bypass ho jata hai message converters ke favor mein jo objects directly response body mein serialize karte hain."
            },
            "code": {
              "title": "View Resolver Configuration",
              "language": "java",
              "content": "@Configuration\npublic class ViewConfig {\n    \n    @Bean\n    public ViewResolver thymeleafViewResolver(SpringTemplateEngine templateEngine) {\n        ThymeleafViewResolver resolver = new ThymeleafViewResolver();\n        resolver.setTemplateEngine(templateEngine);\n        resolver.setCharacterEncoding(\"UTF-8\");\n        resolver.setContentType(\"text/html\");\n        resolver.setViewNames(new String[]{\"*.html\"});\n        return resolver;\n    }\n    \n    @Bean\n    public ViewResolver internalResourceViewResolver() {\n        InternalResourceViewResolver resolver = new InternalResourceViewResolver();\n        resolver.setPrefix(\"/WEB-INF/views/\");\n        resolver.setSuffix(\".jsp\");\n        resolver.setViewClass(JstlView.class);\n        return resolver;\n    }\n}"
            },
            "codeExplanations": {
              "english": "Do view resolvers configure kiye gaye hain: Thymeleaf HTML templates ke liye UTF-8 encoding ke saath, aur InternalResourceViewResolver JSP files ke liye jo /WEB-INF/views/ mein located hain. Registration ka order resolution priority determine karta hai."
            },
            "keyPoints": [
              "Resolves logical view names to physical view implementations",
              "Different resolvers handle different view technologies (JSP, Thymeleaf, FreeMarker)",
              "InternalResourceViewResolver is standard for JSP-based applications",
              "Chained resolvers attempt resolution in order until a match is found"
            ],
            "extras": {
              "flowDiagram": "Controller returns 'user/detail' -> ViewResolver -> /WEB-INF/views/user/detail.jsp -> Render",
              "comparisonTable": [
                {
                  "headers": [
                    "View Technology",
                    "Resolver Class",
                    "Template Location"
                  ],
                  "rows": [
                    [
                      "JSP",
                      "InternalResourceViewResolver",
                      "/WEB-INF/views/"
                    ],
                    [
                      "Thymeleaf",
                      "ThymeleafViewResolver",
                      "classpath:/templates/"
                    ],
                    [
                      "FreeMarker",
                      "FreeMarkerViewResolver",
                      "classpath:/templates/"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "web-rest-4",
            "title": "@Controller vs @RestController",
            "explanations": {
              "english": "Web layer context mein, @Controller traditional server-rendered applications ke liye use hota hai jahan methods view names return karte hain jo ViewResolvers dwara resolve hote hain. @RestController specifically RESTful APIs ke liye design kiya gaya hai, @Controller ko @ResponseBody ke saath combine karke return values ko directly HTTP response body mein serialize karta hai HttpMessageConverters use karke. Jab microservices ya API backends build kar rahe ho, @RestController standard choice hai, jabki @Controller relevant rehta hai MVC applications ke liye jo HTML views use karte hain."
            },
            "code": {
              "title": "Web Layer Usage Patterns",
              "language": "java",
              "content": "@Controller\npublic class PageController {\n    @GetMapping(\"/dashboard\")\n    public String dashboard(Model model) {\n        model.addAttribute(\"stats\", statsService.getStats());\n        return \"admin/dashboard\"; // Resolves to dashboard.html\n    }\n}\n\n@RestController\n@RequestMapping(\"/api/analytics\")\npublic class AnalyticsRestController {\n    @GetMapping(\"/stats\")\n    public ResponseEntity<Stats> getStats() {\n        return ResponseEntity.ok(statsService.getStats());\n    }\n}"
            },
            "codeExplanations": {
              "english": "PageController traditional MVC demonstrate karta hai view names return karke HTML rendering ke liye. AnalyticsRestController REST API development dikhata hai data objects return karke jo message converters dwara automatically JSON mein convert hote hain."
            },
            "keyPoints": [
              "@Controller for server-rendered views and page navigation",
              "@RestController for API endpoints returning serialized data",
              "@RestController eliminates need for @ResponseBody on each method",
              "Both support the same request mapping annotations and injection capabilities"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "@Controller",
                    "@RestController"
                  ],
                  "rows": [
                    [
                      "Primary Use",
                      "Server-rendered web pages",
                      "REST APIs / JSON endpoints"
                    ],
                    [
                      "Return Value",
                      "View name string",
                      "Domain object / ResponseEntity"
                    ],
                    [
                      "View Resolution",
                      "Processed by ViewResolver",
                      "Skipped, uses HttpMessageConverter"
                    ],
                    [
                      "HTTP Response",
                      "Rendered HTML page",
                      "Raw data (JSON/XML)"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "web-rest-5",
            "title": "HTTP Method Mappings",
            "explanations": {
              "english": "Spring MVC specialized annotations provide karta hai HTTP methods ko handler methods pe map karne ke liye: @GetMapping resource retrieval ke liye, @PostMapping resource creation ke liye, @PutMapping full resource updates ke liye, aur @DeleteMapping resource deletion ke liye. Ye composed annotations hain jo @RequestMapping ko specific HTTP method ke saath combine karte hain. Ye URI templates, specific content types consume karna, aur specific response formats produce karna support karte hain. In semantic mappings ka proper use RESTful interfaces create karta hai jo HTTP protocol conventions follow karte hain."
            },
            "code": {
              "title": "CRUD Operation Mappings",
              "language": "java",
              "content": "@RestController\n@RequestMapping(\"/api/products\")\npublic class ProductController {\n    \n    @GetMapping\n    public List<Product> listAll() { return service.findAll(); }\n    \n    @GetMapping(\"/{id}\")\n    public Product getById(@PathVariable Long id) { return service.findById(id); }\n    \n    @PostMapping(consumes = MediaType.APPLICATION_JSON_VALUE)\n    public ResponseEntity<Product> create(@RequestBody @Valid Product product) {\n        Product saved = service.save(product);\n        URI location = ServletUriComponentsBuilder\n            .fromCurrentRequest().path(\"/{id}\").buildAndExpand(saved.getId()).toUri();\n        return ResponseEntity.created(location).body(saved);\n    }\n    \n    @PutMapping(\"/{id}\")\n    public Product update(@PathVariable Long id, @RequestBody Product product) {\n        return service.update(id, product);\n    }\n    \n    @DeleteMapping(\"/{id}\")\n    @ResponseStatus(HttpStatus.NO_CONTENT)\n    public void delete(@PathVariable Long id) {\n        service.delete(id);\n    }\n}"
            },
            "codeExplanations": {
              "english": "Full CRUD mapping demonstrate karta hai: GET retrieval ke liye (collection aur single), POST creation ke liye JSON consumption aur 201 CREATED response ke saath Location header, PUT updates ke liye, DELETE 204 NO_CONTENT ke saath. Har mapping appropriate HTTP semantics aur status codes use karta hai."
            },
            "keyPoints": [
              "@GetMapping retrieves resources without side effects",
              "@PostMapping creates new resources and returns 201 Created with Location header",
              "@PutMapping performs full resource updates (idempotent)",
              "@DeleteMapping removes resources and typically returns 204 No Content"
            ],
            "extras": {
              "flowDiagram": "Client -> GET /products -> List\n       -> GET /products/1 -> Single\n       -> POST /products -> Create -> 201\n       -> PUT /products/1 -> Update -> 200\n       -> DELETE /products/1 -> Remove -> 204",
              "comparisonTable": [
                {
                  "headers": [
                    "Annotation",
                    "HTTP Method",
                    "Idempotent",
                    "Safe"
                  ],
                  "rows": [
                    [
                      "@GetMapping",
                      "GET",
                      "Yes",
                      "Yes"
                    ],
                    [
                      "@PostMapping",
                      "POST",
                      "No",
                      "No"
                    ],
                    [
                      "@PutMapping",
                      "PUT",
                      "Yes",
                      "No"
                    ],
                    [
                      "@DeleteMapping",
                      "DELETE",
                      "Yes",
                      "No"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "web-rest-6",
            "title": "@PathVariable",
            "explanations": {
              "english": "@PathVariable URI template variables se values extract karta hai jo request mapping path mein define kiye gaye hain. Ye path segments ko method parameters mein bind karta hai, Spring ke type conversion system use karke automatic type conversion perform karta hai. Annotation mein variable name path mein template variable name se match karna chahiye, ya explicitly value attribute use karke specify kiya ja sakta hai. Ye optional parameters ko required=false ke through support karta hai aur custom regular expressions validation ke liye path pattern mein hi."
            },
            "code": {
              "title": "Path Variable Extraction",
              "language": "java",
              "content": "@RestController\npublic class OrderController {\n    \n    @GetMapping(\"/orders/{orderId}\")\n    public Order getOrder(@PathVariable Long orderId) {\n        return orderService.findById(orderId);\n    }\n    \n    @GetMapping(\"/users/{userId}/orders/{orderId}\")\n    public Order getUserOrder(\n            @PathVariable(\"userId\") Long userId,\n            @PathVariable Long orderId) {\n        return orderService.findByUserAndId(userId, orderId);\n    }\n    \n    @GetMapping(\"/files/{filename:.+}\")  // Regex to capture dots in filename\n    public ResponseEntity<Resource> downloadFile(@PathVariable String filename) {\n        return ResponseEntity.ok(fileService.load(filename));\n    }\n}"
            },
            "codeExplanations": {
              "english": "First example simple extraction dikhata hai jahan parameter name path variable se match karta hai. Second dikhata hai explicit mapping jab parameter name path variable se differ karta hai. Third regex patterns demonstrate karta hai file extensions handle karne ke liye jo otherwise truncate ho jaate."
            },
            "keyPoints": [
              "Extracts URI template variables into method parameters",
              "Supports type conversion from String to target parameter type",
              "Variable name must match path template or be explicitly specified",
              "Can include regular expressions for advanced path matching"
            ],
            "extras": {
              "flowDiagram": "URL /orders/123 -> @PathVariable Long orderId -> value 123L",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "web-rest-7",
            "title": "@RequestParam",
            "explanations": {
              "english": "@RequestParam HTTP query parameters aur form data ko controller methods ke method parameters mein bind karta hai. Ye automatic type conversion handle karta hai aur defaultValue attribute ke through default values support karta hai, jisse parameters effectively required=false ho jaate hain. Optional parameters ke liye bina defaults ke, required ko false pe set kiya ja sakta hai null values allow karne ke liye. Ye commonly use hota hai filtering, pagination, aur search operations ke liye GET requests mein jahan data URL mein pass hota hai path mein nahi."
            },
            "code": {
              "title": "Query Parameter Handling",
              "language": "java",
              "content": "@GetMapping(\"/products\")\npublic Page<Product> searchProducts(\n        @RequestParam(required = false) String category,\n        @RequestParam(defaultValue = \"0\") int page,\n        @RequestParam(defaultValue = \"20\") int size,\n        @RequestParam(name = \"sort_by\", defaultValue = \"name\") String sortBy,\n        @RequestParam(required = false) List<String> tags) {\n    \n    Pageable pageable = PageRequest.of(page, size, Sort.by(sortBy));\n    return productService.search(category, tags, pageable);\n}"
            },
            "codeExplanations": {
              "english": "Various @RequestParam patterns demonstrate karta hai: optional category filter, pagination defaults ke saath (page 0, size 20), aliased parameter name (sort_by ko sortBy mein map kiya gaya), aur multi-value parameters tags ke liye. Saare parameters automatically String se target types mein convert ho jaate hain."
            },
            "keyPoints": [
              "Binds query string parameters and form data to method arguments",
              "Supports default values for missing parameters",
              "Handles automatic type conversion from String",
              "Can map different query parameter names to method argument names"
            ],
            "extras": {
              "flowDiagram": "URL /products?category=electronics&page=0 -> @RequestParam category=\"electronics\", page=0",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "@PathVariable",
                    "@RequestParam"
                  ],
                  "rows": [
                    [
                      "Source",
                      "URI path /users/{id}",
                      "Query string ?key=value"
                    ],
                    [
                      "Use Case",
                      "Resource identifiers",
                      "Filters, pagination, options"
                    ],
                    [
                      "Required",
                      "Typically yes",
                      "Often optional with defaults"
                    ],
                    [
                      "URL Example",
                      "/orders/123",
                      "/orders?status=pending"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "web-rest-8",
            "title": "@RequestBody",
            "explanations": {
              "english": "@RequestBody HTTP request body content (typically JSON ya XML) ko deserialize karta hai Java objects mein HttpMessageConverters use karke. Ye REST APIs ke liye essential hai jo POST aur PUT requests process karte hain payload data ke saath. Annotation validation support karta hai jab @Valid ya @Validated ke saath combine kiya jaaye, JSR-303 Bean Validation trigger karta hai. Spring content negotiation use karta hai converter determine karne ke liye Content-Type header ke basis pe. JSON ke liye, MappingJackson2HttpMessageConverter typically Spring Boot applications mein default ke roop mein use hota hai."
            },
            "code": {
              "title": "Request Body Deserialization",
              "language": "java",
              "content": "@PostMapping(\"/users\")\npublic ResponseEntity<User> createUser(\n        @RequestBody @Valid UserDto userDto,\n        @RequestHeader(\"X-Api-Key\") String apiKey) {\n    \n    User created = userService.create(userDto, apiKey);\n    return ResponseEntity.status(HttpStatus.CREATED).body(created);\n}\n\npublic record UserDto(\n    @NotBlank String username,\n    @Email String email,\n    @Size(min = 8) String password\n) {}"
            },
            "codeExplanations": {
              "english": "@RequestBody incoming JSON ko UserDto object mein convert karta hai. @Valid DTO fields pe validation annotations trigger karta hai. Agar validation fail hota hai, MethodArgumentNotValidException throw hota hai method body reach karne se pehle."
            },
            "keyPoints": [
              "Deserializes HTTP request body into Java objects using HttpMessageConverters",
              "Works with @Valid to enforce bean validation rules",
              "Requires Content-Type header matching supported media types",
              "Commonly used with JSON payloads in REST APIs"
            ],
            "extras": {
              "flowDiagram": "JSON Request Body -> HttpMessageConverter -> Java Object -> @Valid -> Method Parameter",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "web-rest-9",
            "title": "@ResponseBody",
            "explanations": {
              "english": "@ResponseBody indicate karta hai ki method return value HTTP response body mein bind hona chahiye view name resolve karne ke bajaye. Spring return object ko response format (JSON, XML, etc.) mein convert karta hai registered HttpMessageConverters use karke. Jab individual methods pe @Controller class ke andar lagaya jaaye, ye specific methods ko REST endpoints ki tarah act karne deta hai jabki baaki views return karte hain. Ye implicitly @RestController mein included hai, making explicit usage redundant in those classes."
            },
            "code": {
              "title": "Mixed Controller Usage",
              "language": "java",
              "content": "@Controller\n@RequestMapping(\"/items\")\npublic class ItemController {\n    \n    @GetMapping(\"/{id}/view\")\n    public String viewItem(@PathVariable Long id, Model model) {\n        model.addAttribute(\"item\", itemService.findById(id));\n        return \"item/detail\"; // View resolution\n    }\n    \n    @GetMapping(\"/{id}\")\n    @ResponseBody\n    public Item getItemJson(@PathVariable Long id) {\n        return itemService.findById(id); // Serialized to JSON\n    }\n    \n    @GetMapping(value = \"/{id}\", produces = MediaType.APPLICATION_XML_VALUE)\n    @ResponseBody\n    public ItemXmlWrapper getItemXml(@PathVariable Long id) {\n        return new ItemXmlWrapper(itemService.findById(id));\n    }\n}"
            },
            "codeExplanations": {
              "english": "Same controller mix karta hai view-returning methods ko @ResponseBody methods ke saath. JSON endpoint default MappingJackson2HttpMessageConverter use karta hai. XML endpoint produces specify karta hai XML converter selection trigger karne ke liye Accept headers ke basis pe."
            },
            "keyPoints": [
              "Binds method return value directly to HTTP response body",
              "Triggers message conversion based on Accept header and produces attribute",
              "Allows selective REST endpoints within traditional @Controller classes",
              "Implicitly present on all methods when using @RestController"
            ],
            "extras": {
              "flowDiagram": "Java Object -> HttpMessageConverter -> HTTP Response Body (JSON/XML)",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "web-rest-10",
            "title": "ResponseEntity",
            "explanations": {
              "english": "ResponseEntity full programmatic control provide karta hai HTTP response pe including status codes, headers, aur body content. Ye allow karta hai different status codes return karna business logic ke basis pe (200 OK, 201 Created, 404 Not Found), custom headers set karna jaise Location newly created resources ke liye, aur cache directives control karna. As a generic wrapper, ye response body ke liye type safety maintain rakhta hai aur fluent builder API offer karta hai responses construct karne ke liye. Ye preferred hai @ResponseStatus annotations ke against jab dynamic response construction chahiye hota hai."
            },
            "code": {
              "title": "Advanced Response Building",
              "language": "java",
              "content": "@GetMapping(\"/{id}\")\npublic ResponseEntity<Resource> downloadDocument(@PathVariable String id) {\n    Resource resource = documentService.loadAsResource(id);\n    \n    if (resource == null) {\n        return ResponseEntity.notFound().build();\n    }\n    \n    return ResponseEntity.ok()\n        .contentType(MediaType.APPLICATION_PDF)\n        .header(HttpHeaders.CONTENT_DISPOSITION, \n                \"attachment; filename=\\\"\" + resource.getFilename() + \"\\\"\")\n        .cacheControl(CacheControl.maxAge(24, TimeUnit.HOURS))\n        .eTag(resource.getMd5())\n        .body(resource);\n}\n\n@PostMapping\npublic ResponseEntity<Void> createResource(@RequestBody ResourceRequest request) {\n    String id = service.create(request);\n    URI location = ServletUriComponentsBuilder\n        .fromCurrentRequest().path(\"/{id}\").buildAndExpand(id).toUri();\n    \n    return ResponseEntity.created(location).build();\n}"
            },
            "codeExplanations": {
              "english": "First example comprehensive response building dikhata hai conditional 404 ke saath, content type setting, download headers, caching directives, aur ETag HTTP caching ke liye. Second demonstrate karta hai 201 Created Location header ke saath REST compliance ke liye builder pattern use karke."
            },
            "keyPoints": [
              "Provides complete control over HTTP response status, headers, and body",
              "Supports fluent API for building responses (ok(), created(), notFound())",
              "Enables dynamic status code selection based on business logic",
              "Type-safe wrapper for response body content"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Approach",
                    "Flexibility",
                    "Use Case"
                  ],
                  "rows": [
                    [
                      "@ResponseStatus",
                      "Static per method/class",
                      "Fixed success codes"
                    ],
                    [
                      "ResponseEntity",
                      "Dynamic per execution",
                      "Conditional responses, custom headers"
                    ],
                    [
                      "ResponseStatusException",
                      "Exception-based",
                      "Error handling in catch blocks"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "web-rest-11",
            "title": "Global Exception Handling (@ControllerAdvice)",
            "explanations": {
              "english": "@ControllerAdvice global exception handling provide karta hai across multiple controllers @ExceptionHandler, @InitBinder, aur @ModelAttribute methods ko single class mein consolidate karke. Ye kisi bhi controller dwara throw kiye gaye exceptions intercept karta hai aur unhe standardized HTTP responses mein transform karta hai, stack traces ko clients tak leak hone se prevent karta hai. Common use cases mein validation errors handle karna, resource not found scenarios, aur business logic exceptions include hain. Ye specific controller packages ya annotations pe restrict ho sakta hai basePackages ya basePackageClasses attributes use karke."
            },
            "code": {
              "title": "Global Exception Handler",
              "language": "java",
              "content": "@ControllerAdvice\npublic class GlobalExceptionHandler {\n    \n    private static final Logger log = LoggerFactory.getLogger(GlobalExceptionHandler.class);\n    \n    @ExceptionHandler(ResourceNotFoundException.class)\n    public ResponseEntity<ErrorResponse> handleNotFound(ResourceNotFoundException ex, WebRequest request) {\n        ErrorResponse error = new ErrorResponse(\n            HttpStatus.NOT_FOUND.value(),\n            ex.getMessage(),\n            request.getDescription(false)\n        );\n        return new ResponseEntity<>(error, HttpStatus.NOT_FOUND);\n    }\n    \n    @ExceptionHandler(MethodArgumentNotValidException.class)\n    public ResponseEntity<Map<String, Object>> handleValidationErrors(MethodArgumentNotValidException ex) {\n        Map<String, String> errors = ex.getBindingResult().getFieldErrors().stream()\n            .collect(Collectors.toMap(FieldError::getField, FieldError::getDefaultMessage));\n        \n        return ResponseEntity.badRequest().body(Map.of(\"errors\", errors));\n    }\n    \n    @ExceptionHandler(Exception.class)\n    public ResponseEntity<ErrorResponse> handleGeneric(Exception ex, WebRequest request) {\n        log.error(\"Unhandled exception\", ex);\n        ErrorResponse error = new ErrorResponse(\n            HttpStatus.INTERNAL_SERVER_ERROR.value(),\n            \"An unexpected error occurred\",\n            request.getDescription(false)\n        );\n        return new ResponseEntity<>(error, HttpStatus.INTERNAL_SERVER_ERROR);\n    }\n    \n    public record ErrorResponse(int status, String message, String path) {}\n}"
            },
            "codeExplanations": {
              "english": "Ye advice class three exception types handle karti hai: custom ResourceNotFoundException 404 return karta hai details ke saath, validation errors 400 return karte hain field-specific error maps ke saath, aur generic exceptions 500 return karte hain jabki stack trace internally log ho jata hai security ke liye."
            },
            "keyPoints": [
              "Centralizes exception handling logic for multiple controllers",
              "Eliminates duplicate try-catch blocks across controllers",
              "Supports fine-grained mapping via exception class hierarchy",
              "Can include @ModelAttribute and @InitBinder for shared controller preparation"
            ],
            "extras": {
              "flowDiagram": "Controller throws Exception -> @ControllerAdvice intercepts -> @ExceptionHandler method -> Standardized Error Response",
              "comparisonTable": [],
              "examples": []
            }
          }
        ]
      },
      {
        "id": "data-database-layer",
        "title": "Data / Database Layer",
        "description": "Spring Data JPA ke saath data persistence master karo, Hibernate integration, query optimization strategies, transaction management, aur enterprise applications mein common ORM pitfalls handle karna.",
        "topics": [
          {
            "id": "data-1",
            "title": "CrudRepository Interface",
            "explanations": {
              "english": "CrudRepository Spring Data mein foundational interface hai jo generic CRUD (Create, Read, Update, Delete) operations provide karta hai entity management ke liye. Ye Repository extend karta hai aur methods offer karta hai jaise save(), findById(), findAll(), deleteById() bina implementation code ki zarurat ke. Jab domain-specific interface extend karta hai, Spring Data dynamically runtime pe implementation generate karta hai proxy objects use karke. Ye simple data access needs ke liye suitable hai jahan basic persistence operations sufficient hain."
            },
            "code": {
              "title": "Basic CrudRepository Usage",
              "language": "java",
              "content": "public interface CustomerRepository extends CrudRepository<Customer, Long> {\n    // Basic CRUD methods inherited: save, findById, findAll, delete, etc.\n}\n\n@Service\npublic class CustomerService {\n    private final CustomerRepository repository;\n    \n    public Customer createCustomer(Customer customer) {\n        return repository.save(customer);\n    }\n    \n    public Optional<Customer> findCustomer(Long id) {\n        return repository.findById(id);\n    }\n    \n    public void deleteCustomer(Long id) {\n        repository.deleteById(id);\n    }\n}"
            },
            "codeExplanations": {
              "english": "CustomerRepository interface CrudRepository extend karta hai Customer as entity type aur Long as identifier type ke saath. Koi implementation required nahi hai; Spring Data runtime pe actual implementation provide karta hai. Service layer basic persistence operations ke liye in methods ka use karta hai."
            },
            "keyPoints": [
              "Provides basic CRUD operations out of the box",
              "No implementation code required - Spring generates proxy at runtime",
              "Generic types: <Entity, ID>",
              "Suitable for simple data access without pagination or sorting needs"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "data-2",
            "title": "JpaRepository Interface",
            "explanations": {
              "english": "JpaRepository CrudRepository aur PagingAndSortingRepository extend karke JPA-specific operations provide karta hai. Ye batch operations jaise saveAll(), flushing capabilities saveAndFlush() ke saath, aur retrieval methods add karta hai jo List return karte hain Iterable ke bajaye easy processing ke liye. Ye CRUD methods bhi provide karta hai jo directly JPA EntityManager ke saath operate karte hain. Ye preferred interface hai JPA-based applications ke liye jo pagination, sorting, ya advanced persistence features require karte hain."
            },
            "code": {
              "title": "JpaRepository with Pagination",
              "language": "java",
              "content": "public interface ProductRepository extends JpaRepository<Product, Long> {\n    // Inherits: findAll(Pageable), findAll(Sort), saveAndFlush, etc.\n}\n\n@Service\npublic class ProductService {\n    private final ProductRepository repository;\n    \n    public Page<Product> getProducts(int page, int size, String sortBy) {\n        Pageable pageable = PageRequest.of(page, size, Sort.by(sortBy).ascending());\n        return repository.findAll(pageable);\n    }\n    \n    public List<Product> saveAllProducts(List<Product> products) {\n        return repository.saveAll(products);\n    }\n    \n    public void flushChanges() {\n        repository.flush();\n    }\n}"
            },
            "codeExplanations": {
              "english": "JpaRepository pageable queries provide karta hai findAll(Pageable) through ek Page object return karta hai jo results plus metadata contain karta hai (total elements, total pages). saveAll() efficiently batch inserts perform karta hai. flush() persistence context state ko database ke saath synchronize karta hai."
            },
            "keyPoints": [
              "Extends CrudRepository with JPA-specific methods",
              "Supports pagination and sorting through Pageable and Sort parameters",
              "Returns List<T> instead of Iterable<T> for convenience",
              "Includes flush() and saveAndFlush() for immediate persistence"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "CrudRepository",
                    "JpaRepository"
                  ],
                  "rows": [
                    [
                      "Pagination",
                      "No",
                      "Yes (via PagingAndSorting)"
                    ],
                    [
                      "Sorting",
                      "No",
                      "Yes"
                    ],
                    [
                      "Return Type",
                      "Iterable",
                      "List"
                    ],
                    [
                      "Flush Operations",
                      "No",
                      "Yes"
                    ],
                    [
                      "Batch Save",
                      "No",
                      "saveAll()"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-3",
            "title": "@Entity",
            "explanations": {
              "english": "@Entity annotation ek Java class ko JPA entity mark karta hai, matlab ye database table ke saath mapped hai. Class ke paas ek no-argument constructor hona chahiye (protected ya public ho sakta hai) aur final nahi hona chahiye. By default, entity name class name se match karta hai, aur table name entity name se match karta hai unless @Table se override kiya gaya ho. Entity classes JPA mein domain model represent karti hain aur inke instances persistence context dwara manage hote hain, enabling dirty checking aur automatic state synchronization database ke saath."
            },
            "code": {
              "title": "JPA Entity Definition",
              "language": "java",
              "content": "@Entity\n@Table(name = \"users\", schema = \"public\")\npublic class User {\n    \n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n    \n    @Column(name = \"username\", nullable = false, unique = true, length = 50)\n    private String username;\n    \n    @Column(name = \"email\", nullable = false)\n    private String email;\n    \n    @Column(name = \"created_at\", updatable = false)\n    private LocalDateTime createdAt;\n    \n    protected User() {\n        // JPA requires no-arg constructor\n    }\n    \n    public User(String username, String email) {\n        this.username = username;\n        this.email = email;\n        this.createdAt = LocalDateTime.now();\n    }\n}"
            },
            "codeExplanations": {
              "english": "User ko entity mark kiya gaya hai 'users' table ke saath map karne ke liye. @Column database column attributes customize karta hai jaise nullability aur length. Ek protected no-arg constructor JPA requirements satisfy karta hai jabki public constructor required fields enforce karta hai."
            },
            "keyPoints": [
              "Marks the class as mappable to a database table",
              "Must have a no-argument constructor (protected or public)",
              "Class should not be final (proxying limitations)",
              "Fields are mapped to columns by default unless annotated otherwise"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "data-4",
            "title": "@Id and @GeneratedValue",
            "explanations": {
              "english": "@Id entity ke primary key field ko designate karta hai. Har entity ke paas exactly ek @Id field hona chahiye. @GeneratedValue automatic primary key generation strategies configure karta hai: IDENTITY (database auto-increment), SEQUENCE (database sequence), TABLE (sequence table), ya AUTO (provider chooses). IDENTITY common hai MySQL aur PostgreSQL ke liye, jabki SEQUENCE preferred hai Oracle aur PostgreSQL ke liye batch inserts optimize karne ke liye. Strategy choice performance aur batch operation capabilities ko affect karta hai."
            },
            "code": {
              "title": "Primary Key Generation Strategies",
              "language": "java",
              "content": "@Entity\npublic class Order {\n    \n    @Id\n    @GeneratedValue(strategy = GenerationType.IDENTITY)\n    private Long id;\n    \n    // Alternative: Sequence generation (better for batch)\n    @Id\n    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = \"order_seq\")\n    @SequenceGenerator(name = \"order_seq\", sequenceName = \"order_sequence\", allocationSize = 50)\n    private Long id;\n    \n    // Alternative: UUID generation\n    @Id\n    @GeneratedValue(generator = \"UUID\")\n    @GenericGenerator(name = \"UUID\", strategy = \"org.hibernate.id.UUIDGenerator\")\n    @Column(name = \"id\", updatable = false, nullable = false)\n    private UUID id;\n}"
            },
            "codeExplanations": {
              "english": "Teen strategies dikhata hai: IDENTITY database auto-increment pe rely karta hai. SEQUENCE database sequences use karta hai allocationSize ke saath batch optimization ke liye (ek baar mein 50 IDs fetch karta hai). UUID Hibernate ke UUID generator ka use karta hai distributed systems ke liye bina centralized ID generation ke."
            },
            "keyPoints": [
              "@Id marks the primary key field",
              "GenerationType.IDENTITY: Database auto-increment, immediate insert on persist",
              "GenerationType.SEQUENCE: Database sequence, allows batching with allocationSize",
              "Never manually assign IDs when using generated values"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Strategy",
                    "Database Support",
                    "Batch Insert",
                    "Use Case"
                  ],
                  "rows": [
                    [
                      "IDENTITY",
                      "MySQL, PostgreSQL, SQL Server",
                      "No",
                      "Standard auto-increment"
                    ],
                    [
                      "SEQUENCE",
                      "PostgreSQL, Oracle, H2",
                      "Yes",
                      "High-performance batch operations"
                    ],
                    [
                      "UUID",
                      "All",
                      "Yes",
                      "Distributed systems, security"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-5",
            "title": "Entity Associations",
            "explanations": {
              "english": "JPA associations entities ke beech relationships map karte hain: @OneToOne (single reference dono taraf), @OneToMany / @ManyToOne (parent-child hierarchies), aur @ManyToMany (complex relationships via join tables). mappedBy attribute bidirectional relationships ka inverse side indicate karta hai, jabki default unidirectional hota hai. FetchType loading strategy determine karta hai: LAZY (on demand loaded) ya EAGER (immediately loaded). Bidirectional relationships mein consistency maintain karni padti hai dono sides pe Java code mein."
            },
            "code": {
              "title": "Relationship Mapping",
              "language": "java",
              "content": "@Entity\npublic class Department {\n    @Id\n    @GeneratedValue\n    private Long id;\n    \n    @OneToMany(mappedBy = \"department\", cascade = CascadeType.ALL, orphanRemoval = true)\n    private List<Employee> employees = new ArrayList<>();\n    \n    public void addEmployee(Employee emp) {\n        employees.add(emp);\n        emp.setDepartment(this);\n    }\n}\n\n@Entity\npublic class Employee {\n    @Id\n    @GeneratedValue\n    private Long id;\n    \n    @ManyToOne(fetch = FetchType.LAZY)\n    @JoinColumn(name = \"dept_id\")\n    private Department department;\n    \n    @ManyToMany\n    @JoinTable(name = \"employee_project\",\n               joinColumns = @JoinColumn(name = \"emp_id\"),\n               inverseJoinColumns = @JoinColumn(name = \"proj_id\"))\n    private Set<Project> projects = new HashSet<>();\n}"
            },
            "codeExplanations": {
              "english": "Department ke paas bidirectional OneToMany hai Employee ke saath (mappedBy indicate karta hai ki Employee foreign key own karta hai). CascadeType.ALL operations propagate karta hai, orphanRemoval list se remove kiye gaye employees delete kar deta hai. Employee ke paas ManyToOne hai (LAZY taki department unnecessarily load na ho) aur ManyToMany join table ke saath projects ke liye."
            },
            "keyPoints": [
              "mappedBy indicates the inverse side of bidirectional relationships",
              "Always default to FetchType.LAZY to avoid unintended data loading",
              "CascadeType determines operation propagation to related entities",
              "Bidirectional relationships require helper methods to maintain consistency"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Association",
                    "Cardinality",
                    "Ownership"
                  ],
                  "rows": [
                    [
                      "@OneToOne",
                      "1:1",
                      "either side with join column"
                    ],
                    [
                      "@ManyToOne",
                      "N:1",
                      "always the many side (child)"
                    ],
                    [
                      "@OneToMany",
                      "1:N",
                      "inverse side (mappedBy)"
                    ],
                    [
                      "@ManyToMany",
                      "N:M",
                      "either side with @JoinTable"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-6",
            "title": "Derived Query Methods",
            "explanations": {
              "english": "Spring Data JPA specific naming conventions follow karke method names se queries derive karta hai. Methods jo findBy, readBy, ya getBy se start hote hain followed by property names aur operations (And, Or, LessThan, Like, etc.) automatically implement hote hain. Parser method name ko property expressions aur operators mein split karta hai, appropriate JPQL query generate karta hai. While convenient simple queries ke liye, complex queries jaise bahut saare conditions wali unwieldy ho jaati hain aur @Query use karni chahiye."
            },
            "code": {
              "title": "Method Name Queries",
              "language": "java",
              "content": "public interface OrderRepository extends JpaRepository<Order, Long> {\n    \n    List<Order> findByStatus(OrderStatus status);\n    \n    List<Order> findByStatusAndCreatedDateGreaterThan(OrderStatus status, LocalDateTime date);\n    \n    List<Order> findByCustomerEmailContainingIgnoreCase(String email);\n    \n    Optional<Order> findFirstByStatusOrderByCreatedDateDesc(OrderStatus status);\n    \n    boolean existsByOrderNumber(String orderNumber);\n    \n    long countByStatus(OrderStatus status);\n    \n    void deleteByStatusAndCreatedDateBefore(OrderStatus status, LocalDateTime date);\n}"
            },
            "codeExplanations": {
              "english": "Spring Data in method names ko parse karke queries generate karta hai: findByStatus create karta hai WHERE status = ?. And conditions combine karta hai. GreaterThan translate karta hai > comparator mein. FirstBy OrderBy ke saath LIMIT 1 aur ORDER BY clauses add karta hai. existsBy entity ke bajaye boolean return karta hai."
            },
            "keyPoints": [
              "Query derived from method name at startup",
              "Supports operations: And, Or, Like, GreaterThan, LessThan, Between, etc.",
              "First/Top keywords limit results (e.g., findFirstBy...)",
              "OrderBy adds sorting (ASC/DESC)",
              "Exists and Count prefixes return boolean and long respectively"
            ],
            "extras": {
              "flowDiagram": "Method Name: findByLastNameAndFirstNameIgnoreCase\nParsing: Property 'lastName' + Operator 'And' + Property 'firstName' + Modifier 'IgnoreCase'\nGenerated JPQL: SELECT o FROM Order o WHERE o.lastName = ?1 AND UPPER(o.firstName) = UPPER(?2)",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "data-7",
            "title": "@Query Annotation",
            "explanations": {
              "english": "@Query allow karta hai custom JPQL (Java Persistence Query Language) ya native SQL queries directly repository methods pe define karne ke liye. Ye query derivation mechanism ko override karta hai complex operations ke liye jo method naming conventions se unsupported hain. Named parameters (e.g., :name) ya positional parameters (?1) method arguments bind kar sakte hain. nativeQuery=true set karke SQL JPQL ke bajaye switch kiya ja sakta hai. @Query UPDATE ya DELETE operations ke liye bhi use hota hai jab @Modifying ke saath combine kiya jaaye."
            },
            "code": {
              "title": "Custom Query Definitions",
              "language": "java",
              "content": "public interface UserRepository extends JpaRepository<User, Long> {\n    \n    @Query(\"SELECT u FROM User u WHERE u.department.name = :deptName AND u.active = true\")\n    List<User> findActiveByDepartmentName(@Param(\"deptName\") String departmentName);\n    \n    @Query(value = \"SELECT * FROM users WHERE created_date > ?1\", nativeQuery = true)\n    List<User> findRecentUsersNative(LocalDateTime since);\n    \n    @Modifying\n    @Query(\"UPDATE User u SET u.lastLogin = CURRENT_TIMESTAMP WHERE u.id = :userId\")\n    int updateLastLogin(@Param(\"userId\") Long userId);\n    \n    @Query(\"SELECT new com.example.UserDto(u.id, u.email) FROM User u\")\n    List<UserDto> findAllForExport();\n}"
            },
            "codeExplanations": {
              "english": "First query named parameters use karta hai JPQL navigation ke saath department.name. Second native SQL dikhata hai positional parameter (?1) ke saath. @Modifying update operation indicate karta hai affected rows return karta hai. Fourth constructor expression (new) use karta hai DTO projections return karne ke liye entities ke bajaye."
            },
            "keyPoints": [
              "Defines explicit JPQL or SQL queries when derivation is insufficient",
              "Use @Param to bind named parameters in queries",
              "Set nativeQuery=true for database-specific SQL",
              "@Modifying required for UPDATE/DELETE operations",
              "Constructor expressions (new) enable DTO projections"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "Derived Methods",
                    "@Query"
                  ],
                  "rows": [
                    [
                      "Complexity",
                      "Simple conditions",
                      "Complex joins, aggregations, subqueries"
                    ],
                    [
                      "Optimization",
                      "Generic",
                      "Custom SQL hints, specific columns"
                    ],
                    [
                      "Flexibility",
                      "Limited to naming convention",
                      "Full JPQL/SQL support"
                    ],
                    [
                      "Maintainability",
                      "Can be verbose",
                      "Explicit and documented"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-8",
            "title": "JPQL vs Native SQL",
            "explanations": {
              "english": "JPQL (Java Persistence Query Language) entity objects aur unke relationships pe operate karta hai rather than database tables, providing database independence aur type safety. Ye polymorphic queries aur entity inheritance naturally support karta hai. Native SQL queries directly database tables aur columns target karte hain, enabling vendor-specific features, complex optimizations, aur database functions jo JPQL se support nahi hote. While JPQL portability ke liye preferred hai, native SQL necessary hai performance tuning aur specific database capabilities leverage karne ke liye."
            },
            "code": {
              "title": "Query Language Comparison",
              "language": "java",
              "content": "// JPQL - Object-oriented, database agnostic\n@Query(\"SELECT o FROM Order o JOIN FETCH o.customer c WHERE c.region = :region\")\nList<Order> findOrdersWithCustomersByRegion(@Param(\"region\") String region);\n\n// Native SQL - Database specific, table-centric\n@Query(value = \"SELECT o.*, c.name as customer_name FROM orders o \" +\n               \"JOIN customers c ON o.customer_id = c.id \" +\n               \"WHERE c.region = :region \" +\n               \"AND o.created_date > CURRENT_DATE - INTERVAL '30 days'\", \n       nativeQuery = true)\nList<Order> findRecentOrdersNative(@Param(\"region\") String region);\n\n// Native with Entity mapping\n@Query(value = \"SELECT * FROM orders WHERE status = ?1\", nativeQuery = true)\n@Modifying\nList<Order> findByStatusNative(String status);"
            },
            "codeExplanations": {
              "english": "JPQL example entity names (Order, Customer) aur association navigation (o.customer) use karta hai. JOIN FETCH eagerly load karta hai related entities N+1 prevent karne ke liye. Native SQL actual table names aur SQL syntax use karta hai, database-specific functions jaise INTERVAL operations allow karta hai."
            },
            "keyPoints": [
              "JPQL queries entities and relationships; SQL queries tables and columns",
              "JPQL provides database portability and polymorphic queries",
              "Native SQL enables vendor-specific optimizations and functions",
              "Use JPQL by default; fallback to native SQL for performance or complexity"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Characteristic",
                    "JPQL",
                    "Native SQL"
                  ],
                  "rows": [
                    [
                      "Syntax Target",
                      "Entity classes",
                      "Database tables"
                    ],
                    [
                      "Portability",
                      "Database independent",
                      "Vendor specific"
                    ],
                    [
                      "Caching",
                      "Supports 2nd level cache",
                      "May bypass cache"
                    ],
                    [
                      "Features",
                      "Standard JPA operations",
                      "Full SQL dialect features"
                    ],
                    [
                      "Return Type",
                      "Managed entities",
                      "May require @SqlResultSetMapping"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-9",
            "title": "Pagination Support",
            "explanations": {
              "english": "Pagination large result sets ko manageable chunks mein split karta hai Pageable parameters use karke. Spring Data JPA automatically LIMIT/OFFSET (ya equivalent) clauses add karta hai queries mein jab Pageable pass kiya jaata hai. Page<T> return type current page content aur metadata dono include karta hai (total elements, total pages, current page number). Slice<T> ek lighter alternative hai jo sirf indicate karta hai ki aur pages hain ya nahi bina total rows count kiye, large datasets mein performance improve karta hai jahan exact counts expensive hote hain."
            },
            "code": {
              "title": "Paginated Queries",
              "language": "java",
              "content": "public interface DocumentRepository extends JpaRepository<Document, Long> {\n    \n    // Automatic pagination for derived query\n    Page<Document> findByStatus(DocumentStatus status, Pageable pageable);\n    \n    // Custom query with pageable\n    @Query(\"SELECT d FROM Document d WHERE d.owner = :user\")\n    Page<Document> findByOwner(@Param(\"user\") User user, Pageable pageable);\n    \n    // Slice for performance (no total count)\n    Slice<Document> findByCategory(String category, Pageable pageable);\n}\n\n@Service\npublic class DocumentService {\n    public Page<Document> getDocuments(int page, int size, String sort) {\n        PageRequest pageable = PageRequest.of(\n            page, size, \n            Sort.by(\"createdDate\").descending()\n        );\n        return repository.findByStatus(DocumentStatus.ACTIVE, pageable);\n    }\n}"
            },
            "codeExplanations": {
              "english": "Pageable parameter pagination logic trigger karta hai. PageRequest.of request create karta hai page number (0-indexed), size, aur optional sorting ke saath. Page contain karta hai getContent(), getTotalPages(), getTotalElements(). Slice similar hai lekin hasNext() use karta hai total counts ke bajaye, expensive COUNT(*) queries avoid karta hai."
            },
            "keyPoints": [
              "Pass Pageable parameter to enable pagination",
              "Page<T> includes total count metadata; Slice<T> does not (better performance)",
              "PageRequest.of(page, size, Sort) creates pagination requests",
              "Sorting can be dynamic via Sort.by() or Sort.Direction"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "Page<T>",
                    "Slice<T>"
                  ],
                  "rows": [
                    [
                      "Total Count",
                      "Available (expensive query)",
                      "Not available"
                    ],
                    [
                      "Total Pages",
                      "Calculated",
                      "Unknown"
                    ],
                    [
                      "Performance",
                      "Slower (COUNT required)",
                      "Faster"
                    ],
                    [
                      "Navigation",
                      "Full page numbers",
                      "Previous/Next only"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-10",
            "title": "Projections",
            "explanations": {
              "english": "Projections entities se sirf specific columns retrieve karte hain full objects ke bajaye, memory usage reduce karte hain aur query performance improve karte hain. Interface-based projections getter methods define karte hain required properties ke liye; Spring Data proxy implementations create karta hai. Class-based projections (DTOs) constructor expressions use karte hain @Query mein specific classes instantiate karne ke liye selected fields ke saath. Projections essential hain reporting queries aur API responses ke liye jo sirf entity data ka subset require karte hain, unnecessary large fields jaise BLOBs ya extensive relationships load karne se bachate hain."
            },
            "code": {
              "title": "Projection Strategies",
              "language": "java",
              "content": "// Interface projection - closed projection (specific fields)\npublic interface UserSummary {\n    String getUsername();\n    String getEmail();\n    DepartmentInfo getDepartment();\n    \n    interface DepartmentInfo {\n        String getName();\n    }\n}\n\n// Repository using projection\npublic interface UserRepository extends JpaRepository<User, Long> {\n    List<UserSummary> findByActiveTrue();\n    \n    @Query(\"SELECT new com.example.UserDto(u.id, u.username, d.name) \" +\n           \"FROM User u JOIN u.department d WHERE u.active = true\")\n    List<UserDto> findActiveUsersWithDept();\n}\n\n// DTO Class for constructor projection\n@Data\n@AllArgsConstructor\npublic class UserDto {\n    private Long id;\n    private String username;\n    private String departmentName;\n}"
            },
            "codeExplanations": {
              "english": "UserSummary ek interface projection hai jahan Spring Data ek proxy generate karta hai sirf specified getters implement karne ke liye. Ye ek closed projection hai SQL SELECT clause optimize karta hai. @Query with 'new' constructor expression use karta hai UserDto directly query results se instantiate karne ke liye."
            },
            "keyPoints": [
              "Interface projections create proxies with only required fields",
              "Class projections (DTOs) use constructor expressions in JPQL",
              "Reduces data transfer when only specific fields are needed",
              "Closed projections (interface) allow SQL optimization (select specific columns)"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Type",
                    "Implementation",
                    "Use Case"
                  ],
                  "rows": [
                    [
                      "Interface (Closed)",
                      "Spring proxy",
                      "Simple read-only views"
                    ],
                    [
                      "Interface (Open)",
                      "Entity proxy",
                      "Full entity with restricted view"
                    ],
                    [
                      "Class (DTO)",
                      "Constructor instantiation",
                      "Complex aggregations, reporting"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-11",
            "title": "@Transactional",
            "explanations": {
              "english": "@Transactional declare karta hai ki ek method (ya class) transactional context mein execute hona chahiye. Spring transaction boundaries manage karta hai, method entry se pehle transaction shuru karta hai aur successful completion pe commit ya runtime exceptions pe rollback karta hai. Class level pe, ye saare public methods pe apply hota hai. Ye Spring AOP proxies ke through kaam karta hai, isliye sirf external method calls transaction interception trigger karte hain. Annotation propagation, isolation, timeout, aur rollback rules customize karne ko support karta hai."
            },
            "code": {
              "title": "Transactional Usage",
              "language": "java",
              "content": "@Service\n@Transactional(readOnly = true)  // Default for all methods in class\npublic class AccountService {\n    \n    @Transactional  // Overrides class-level, readOnly = false\n    public void transfer(Long fromId, Long toId, BigDecimal amount) {\n        Account from = accountRepo.findById(fromId).orElseThrow();\n        Account to = accountRepo.findById(toId).orElseThrow();\n        \n        from.debit(amount);\n        to.credit(amount);\n        \n        // Both saves happen in same transaction\n        accountRepo.save(from);\n        accountRepo.save(to);\n    }\n    \n    public Account getAccount(Long id) {  // Inherits readOnly = true\n        return accountRepo.findById(id).orElseThrow();\n    }\n    \n    @Transactional(rollbackFor = InsufficientFundsException.class)\n    public void strictTransfer(Long fromId, Long toId, BigDecimal amount) \n            throws InsufficientFundsException {\n        // Rolls back for checked exception too\n    }\n}"
            },
            "codeExplanations": {
              "english": "Class-level @Transactional(readOnly=true) read operations optimize karta hai (no dirty checking, no snapshot). Method-level @Transactional without attributes write operations enable karta hai. strictTransfer example rollbackFor dikhata hai checked exceptions ko bhi rollback rules mein include karne ke liye."
            },
            "keyPoints": [
              "Declares transactional boundaries semantically",
              "readOnly=true optimizes performance for query methods",
              "Rollback occurs automatically for unchecked exceptions",
              "Use rollbackFor to include checked exceptions",
              "Propagation defaults to REQUIRED (join existing or create new)"
            ],
            "extras": {
              "flowDiagram": "Method Entry -> Proxy intercepts -> Begin TX -> Execute Method -> Success? -> Commit : Rollback -> Return",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "data-12",
            "title": "Transaction Propagation",
            "explanations": {
              "english": "Propagation define karta hai transactional behavior jab ek method existing transaction context mein invoke hota hai. REQUIRED (default) existing transaction join karta hai ya naya create karta hai agar koi nahi hai. REQUIRES_NEW current transaction suspend karke ek new independent transaction create karta hai, separately commit ya rollback hota hai. NESTED current transaction mein ek savepoint create karta hai, partial rollback allow karta hai. MANDATORY existing transaction require karta hai aur exception throw karta hai agar koi nahi hai. NEVER exception throw karta hai agar transaction mein call kiya jaye. SUPPORTS existing join karta hai ya non-transactionally execute karta hai agar koi nahi hai."
            },
            "code": {
              "title": "Propagation Behaviors",
              "language": "java",
              "content": "@Service\npublic class OrderProcessingService {\n    \n    @Transactional\n    public void processOrder(Order order) {\n        orderRepo.save(order);\n        // Joins outer transaction (default REQUIRED)\n        inventoryService.reserveInventory(order.getItems());\n        \n        // Independent transaction - commits even if outer rolls back\n        auditService.logAuditEvent(\"Order processed: \" + order.getId());\n    }\n}\n\n@Service\npublic class AuditService {\n    \n    @Transactional(propagation = Propagation.REQUIRES_NEW)\n    public void logAuditEvent(String message) {\n        auditRepo.save(new AuditLog(message));\n    }\n    \n    @Transactional(propagation = Propagation.MANDATORY)\n    public void mandatoryOperation() {\n        // Must be called within existing transaction\n    }\n}"
            },
            "codeExplanations": {
              "english": "processOrder ek transaction mein run hota hai. inventoryService call same transaction join karta hai (REQUIRED). auditService.logAuditEvent order transaction suspend karta hai, audit ke liye naya transaction create karta hai, immediately commit karta hai, phir order transaction resume karta hai. Ye ensure karta hai ki audit records persist hain even if main order fail ho."
            },
            "keyPoints": [
              "REQUIRED: Default, joins existing or creates new",
              "REQUIRES_NEW: Suspends current, creates independent transaction",
              "NESTED: Creates savepoint for partial rollback",
              "MANDATORY: Requires existing transaction, throws if none",
              "NEVER: Throws if called within a transaction"
            ],
            "extras": {
              "flowDiagram": "Outer TX[REQUIRED] -> calls Inner1[REQUIRED] -> Joins Outer\n                     -> calls Inner2[REQUIRES_NEW] -> Suspends Outer, creates Inner2 TX\n                     -> Inner2 Commits -> Resumes Outer",
              "comparisonTable": [
                {
                  "headers": [
                    "Propagation",
                    "Current TX Present",
                    "No Current TX"
                  ],
                  "rows": [
                    [
                      "REQUIRED",
                      "Join",
                      "Create new"
                    ],
                    [
                      "REQUIRES_NEW",
                      "Suspend, create new",
                      "Create new"
                    ],
                    [
                      "NESTED",
                      "Create savepoint",
                      "Create new"
                    ],
                    [
                      "MANDATORY",
                      "Join",
                      "Throw exception"
                    ],
                    [
                      "NEVER",
                      "Throw exception",
                      "Execute without TX"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-13",
            "title": "Transaction Isolation Levels",
            "explanations": {
              "english": "Isolation levels determine karte hain ki transaction integrity other transactions ko kaise visible hoti hai, consistency ko concurrency ke against balance karte hain. READ_UNCOMMITTED dirty reads allow karta hai (uncommitted changes padhna). READ_COMMITTED dirty reads prevent karta hai lekin non-repeatable reads allow karta hai (most databases mein default). REPEATABLE_READ non-repeatable reads prevent karta hai lekin phantom reads allow karta hai. SERIALIZABLE complete isolation provide karta hai sab anomalies prevent karke lekin concurrency severely limit karta hai. Spring default underlying database ke default level pe hota hai lekin explicit configuration per transaction allow karta hai."
            },
            "code": {
              "title": "Isolation Configuration",
              "language": "java",
              "content": "@Service\npublic class FinancialService {\n    \n    // Strict isolation for critical operations\n    @Transactional(isolation = Isolation.SERIALIZABLE)\n    public void transferFunds(Account from, Account to, BigDecimal amount) {\n        // Prevents phantom reads and non-repeatable reads\n    }\n    \n    // Read committed for standard operations\n    @Transactional(isolation = Isolation.READ_COMMITTED)\n    public Account getAccountSummary(Long accountId) {\n        return accountRepository.findById(accountId).orElseThrow();\n    }\n    \n    // Optimistic locking alternative\n    @Transactional\n    public Account updateBalance(Long id, BigDecimal delta) {\n        Account account = accountRepository.findById(id).orElseThrow();\n        account.setBalance(account.getBalance().add(delta));\n        return accountRepository.save(account); // Version check prevents lost updates\n    }\n}"
            },
            "codeExplanations": {
              "english": "SERIALIZABLE transferFunds ke liye use kiya jata hai absolute consistency ensure karne ke liye. READ_COMMITTED standard queries ke liye hota hai. Alternatively, optimistic locking via @Version fields strict isolation replace kar sakta hai better performance ke saath lost updates prevent karte hue."
            },
            "keyPoints": [
              "DEFAULT: Database default (usually READ_COMMITTED)",
              "READ_UNCOMMITTED: Lowest isolation, allows dirty reads",
              "READ_COMMITTED: Prevents dirty reads, allows non-repeatable reads",
              "REPEATABLE_READ: Prevents non-repeatable reads, allows phantom reads",
              "SERIALIZABLE: Highest isolation, prevents all anomalies, lowest concurrency"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Isolation Level",
                    "Dirty Read",
                    "Non-Repeatable",
                    "Phantom Read"
                  ],
                  "rows": [
                    [
                      "READ_UNCOMMITTED",
                      "Possible",
                      "Possible",
                      "Possible"
                    ],
                    [
                      "READ_COMMITTED",
                      "Prevented",
                      "Possible",
                      "Possible"
                    ],
                    [
                      "REPEATABLE_READ",
                      "Prevented",
                      "Prevented",
                      "Possible"
                    ],
                    [
                      "SERIALIZABLE",
                      "Prevented",
                      "Prevented",
                      "Prevented"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-14",
            "title": "N+1 Problem",
            "explanations": {
              "english": "N+1 problem tab occur hota hai jab ORM ek query execute karta hai parent entity fetch karne ke liye, phir N parent entities ke liye additional queries execute karta hai related children fetch karne ke liye (lazy loading). Ye N+1 total queries result deta hai single joined query ke bajaye, severe performance degradation cause karta hai. Solutions mein include hain JPQL mein JOIN FETCH use karna eagerly load karne ke liye associations ko ek query mein, EntityGraph use karna fetch plans define karne ke liye, ya batch fetching enable karna multiple children ko fewer queries mein load karne ke liye."
            },
            "code": {
              "title": "Solving N+1",
              "language": "java",
              "content": "// Problem: N+1 queries\n@Entity\npublic class Order {\n    @OneToMany(mappedBy = \"order\", fetch = FetchType.LAZY)\n    private List<OrderItem> items;\n}\n\n// Bad: 101 queries for 100 orders\nList<Order> orders = orderRepository.findAll();  // 1 query\nfor (Order o : orders) {\n    o.getItems().size();  // 100 additional queries\n}\n\n// Solution 1: JOIN FETCH\n@Query(\"SELECT DISTINCT o FROM Order o JOIN FETCH o.items\")\nList<Order> findAllWithItems();\n\n// Solution 2: Entity Graph\n@EntityGraph(attributePaths = {\"items\", \"items.product\"})\n@Query(\"SELECT o FROM Order o\")\nList<Order> findAllWithGraph();\n\n// Solution 3: Batch Size\n@Entity\npublic class OrderItem {\n    @ManyToOne(fetch = FetchType.LAZY)\n    @BatchSize(size = 25)\n    private Order order;\n}"
            },
            "codeExplanations": {
              "english": "N+1 example 101 queries issue karta hai. Solution 1 JOIN FETCH use karta hai orders aur items ko ek SQL mein load karne ke liye JOIN ke saath. Solution 2 @EntityGraph use karta hai query time pe fetch plan specify karne ke liye. Solution 3 @BatchSize use karta hai items ko batches mein load karne ke liye 25 ka size lekar instead of one by one."
            },
            "keyPoints": [
              "Occurs when accessing lazy-loaded collections in loops",
              "JOIN FETCH loads data in single query with SQL JOIN",
              "EntityGraph provides declarative fetch plans",
              "Batch fetching reduces queries by loading collections in batches",
              "Use lazy loading carefully; fetch eagerly when needed"
            ],
            "extras": {
              "flowDiagram": "N+1 Problem: Select Orders (1) -> Loop -> Select Items for Order 1 -> Select Items for Order 2 ... (N queries)\nJOIN FETCH: Select Orders JOIN Items (1 query total)",
              "comparisonTable": [
                {
                  "headers": [
                    "Solution",
                    "Approach",
                    "Best For"
                  ],
                  "rows": [
                    [
                      "JOIN FETCH",
                      "Eager join in query",
                      "Always need the data"
                    ],
                    [
                      "Entity Graph",
                      "Dynamic fetch plan",
                      "Sometimes need the data"
                    ],
                    [
                      "Batch Size",
                      "Multiple lazy loads batched",
                      "Deep object graphs"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "data-15",
            "title": "LazyInitializationException",
            "explanations": {
              "english": "LazyInitializationException occur hota hai jab code attempt karta hai ek lazy-loaded association access karne ke liye Hibernate Session (Persistence Context) close hone ke baad, typically jab collection properties access kiye jaate hain transactional boundary ke bahar. Since lazy loading requires active session proxy database access karne ke liye, ye exception prevent karta hai uninitialized proxies access karne se. Solutions mein include hain transactions ko open rakhna kaafi time tak (Open Session in View pattern), required data ko transaction ke andar eagerly fetch karna using JOIN FETCH, ya DTO projections use karna upfront necessary data fetch karne ke liye."
            },
            "code": {
              "title": "Handling Lazy Loading",
              "language": "java",
              "content": "// Problem: Session closed\n@Service\npublic class OrderService {\n    @Transactional\n    public Order getOrder(Long id) {\n        return orderRepository.findById(id).orElseThrow(); // Session closes here\n    }\n}\n\n// Controller - outside transaction\n@GetMapping(\"/{id}/items\")\npublic List<ItemDto> getItems(@PathVariable Long id) {\n    Order order = orderService.getOrder(id);\n    return order.getItems().stream()  // LazyInitializationException!\n        .map(this::toDto)\n        .collect(Collectors.toList());\n}\n\n// Solutions:\n\n// 1. Transactional at higher level or OSIV (Open Session in View)\n@GetMapping(\"/{id}/items\")\n@Transactional(readOnly = true)  // Keeps session open\npublic List<ItemDto> getItemsFixed(@PathVariable Long id) { ... }\n\n// 2. Fetch within transaction\n@Transactional\npublic Order getOrderWithItems(Long id) {\n    Order order = orderRepository.findById(id).orElseThrow();\n    order.getItems().size();  // Force initialization within TX\n    return order;\n}\n\n// 3. Use projections to avoid entity loading\n@Query(\"SELECT new com.example.OrderDto(o.id, o.total, i.name) \" +\n       \"FROM Order o JOIN o.items i WHERE o.id = :id\")\nList<OrderDto> findOrderDetails(@Param(\"id\") Long id);"
            },
            "codeExplanations": {
              "english": "Problem dikhata hai lazy collection access karna @Transactional service method end hone aur session close hone ke baad. Solutions dikhate hain: controller mein transaction open rakhna (OSIV anti-pattern caution), transaction ke andar initialization force karna collection touch karke, ya entities entirely avoid karna DTO projection queries ke saath."
            },
            "keyPoints": [
              "Thrown when accessing uninitialized lazy association outside Session",
              "Occurs after transaction commits and session closes",
              "Solutions: eager fetching, Open Session in View (anti-pattern), DTOs",
              "Avoid returning entities to web layer; use DTOs instead",
              "Hibernate.initialize() or touching the collection forces initialization"
            ],
            "extras": {
              "flowDiagram": "Service Method [TX Open] -> Load Entity (proxy) -> TX Commit [Session Closed] -> Controller accesses proxy -> No Session -> LazyInitializationException",
              "comparisonTable": [],
              "examples": []
            }
          }
        ]
      },
      {
        "id": "config-security",
        "title": "Configuration, Profiles & Security",
        "description": "Externalized configuration management, environment-specific profiles, aur comprehensive application security master karo including authentication, authorization, JWT, aur OAuth2 integration.",
        "topics": [
          {
            "id": "config-1",
            "title": "Configuration Management",
            "explanations": {
              "english": "Spring Boot externalizes configuration property files ya YAML ke through, behavior change karne allow karta hai bina code modification ke. @Value individual properties inject karta hai SpEL expressions use karke, jabki @ConfigurationProperties structured hierarchical settings ko type-safe POJOs mein bind karta hai validation support ke saath. Configuration properties relaxed binding support karte hain (kebab-case, camelCase, snake_case) aur type conversion. @Validated JSR-303 bean validation enable karta hai configuration classes pe, ensuring constraints jaise @NotNull ya @Min startup pe check hote hain."
            },
            "code": {
              "title": "Configuration Strategies",
              "language": "java",
              "content": "@Configuration\npublic class AppConfig {\n    \n    @Value(\"${app.name:DefaultApp}\")\n    private String appName;\n    \n    @Value(\"${app.features}\")\n    private List<String> features;\n    \n    @Bean\n    public RestTemplate restTemplate(@Value(\"${api.timeout:5000}\") int timeout) {\n        return new RestTemplateBuilder()\n            .setConnectTimeout(Duration.ofMillis(timeout))\n            .build();\n    }\n}\n\n@Configuration\n@ConfigurationProperties(prefix = \"app.mail\")\n@Validated\n@Data\npublic class MailProperties {\n    @NotNull\n    private String host;\n    \n    @Min(1024)\n    @Max(65535)\n    private int port = 587;\n    \n    @NotEmpty\n    private String username;\n    \n    @NotEmpty\n    private String password;\n    \n    private Map<String, String> headers = new HashMap<>();\n}\n\n// application.yml:\n// app:\n//   mail:\n//     host: smtp.gmail.com\n//     port: 587\n//     username: user@gmail.com\n//     password: ${MAIL_PASSWORD}"
            },
            "codeExplanations": {
              "english": "@Value demonstrate karta hai individual property injection default values aur SpEL ke saath lists ke liye. MailProperties dikhata hai type-safe structured configuration validation constraints ke saath. Prefix 'app.mail' nested YAML structures ko map karta hai. @Validated context initialization ke dauraan constraint validation trigger karta hai, invalid configuration pe fast fail karta hai."
            },
            "keyPoints": [
              "@Value for simple property injection with SpEL support and defaults",
              "@ConfigurationProperties for type-safe hierarchical configuration",
              "Relaxed binding allows app.mail.camelCase, app.mail.kebab-case, or APP_MAIL_SNAKE_CASE",
              "@Validated enables bean validation on configuration classes",
              "Use @ConstructorBinding on record classes for immutable configuration"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "@Value",
                    "@ConfigurationProperties"
                  ],
                  "rows": [
                    [
                      "Type Safety",
                      "String conversion required",
                      "Automatic type binding"
                    ],
                    [
                      "Validation",
                      "Manual",
                      "@Validated support"
                    ],
                    [
                      "Complex Objects",
                      "Not supported",
                      "Nested objects and lists"
                    ],
                    [
                      "IDE Support",
                      "Limited",
                      "Metadata for auto-completion"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "profiles-1",
            "title": "Spring Profiles",
            "explanations": {
              "english": "Profiles environment-specific configuration separation provide karte hain, alag alag settings development, testing, aur production ke liye allow karte hain. Profile-specific files application-{profile}.properties naming convention follow karte hain aur default properties override karte hain. @Profile annotation conditionally beans ya @Configuration classes activate karta hai sirf jab specified profiles active hain. Profiles activate hote hain spring.profiles.active property ke through command-line arguments, environment variables, ya SpringApplicationBuilder mein programmatic configuration ke through."
            },
            "code": {
              "title": "Profile Configuration",
              "language": "java",
              "content": "@Configuration\n@Profile(\"dev\")\npublic class DevConfig {\n    @Bean\n    public DataSource h2DataSource() {\n        return new EmbeddedDatabaseBuilder()\n            .setType(EmbeddedDatabaseType.H2)\n            .build();\n    }\n}\n\n@Configuration\n@Profile(\"prod\")\npublic class ProdConfig {\n    @Bean\n    public DataSource postgresDataSource(\n            @Value(\"${DB_URL}\") String url,\n            @Value(\"${DB_USER}\") String user) {\n        HikariConfig config = new HikariConfig();\n        config.setJdbcUrl(url);\n        config.setUsername(user);\n        return new HikariDataSource(config);\n    }\n}\n\n@Service\n@Profile({\"dev\", \"test\"})\npublic class MockEmailService implements EmailService {\n    // Only active in dev or test\n}\n\n// application-dev.yml (loaded only when 'dev' profile active)\n// server:\n//   port: 8081\n// logging:\n//   level:\n//     root: DEBUG"
            },
            "codeExplanations": {
              "english": "DevConfig provide karta hai H2 embedded database sirf jab 'dev' profile active hai. ProdConfig activate hota hai 'prod' profile ke liye externalized database credentials ke saath. MockEmailService multiple profile names use karta hai. Profile-specific YAML files override karte hain shared configuration sirf jab unka profile active ho."
            },
            "keyPoints": [
              "application-{profile}.properties/yaml for profile-specific settings",
              "@Profile on @Component, @Service, @Configuration to conditionally enable beans",
              "Multiple profiles can be active simultaneously: spring.profiles.active=dev,local",
              "Profiles can be activated via command line: --spring.profiles.active=prod",
              "Use ! prefix for negative profiles: @Profile(\"!test\") excludes test profile"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Activation Method",
                    "Command",
                    "Scope"
                  ],
                  "rows": [
                    [
                      "Command Line",
                      "--spring.profiles.active=dev",
                      "Single execution"
                    ],
                    [
                      "Environment Variable",
                      "SPRING_PROFILES_ACTIVE=dev",
                      "System-wide"
                    ],
                    [
                      "application.properties",
                      "spring.profiles.active=dev",
                      "Default for app"
                    ],
                    [
                      "Programmatic",
                      "setAdditionalProfiles()",
                      "Specific to application instance"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "sec-auth-1",
            "title": "Authentication vs Authorization",
            "explanations": {
              "english": "Authentication identity verify karta hai (proving who you are) credentials jaise passwords, tokens, ya certificates ke through, resulting in an Authentication object jo SecurityContext mein store hota hai. Authorization permitted actions determine karta hai (what you can do) GrantedAuthority roles ko access rules evaluate karke. Spring Security filters intercept karte hain requests ko authorization se pehle authentication perform karne ke liye. Process identity verification ko permission checking se separate karta hai, allowing flexible security models jahan authenticated users ke paas different privilege levels ho sakte hain."
            },
            "code": {
              "title": "Security Context",
              "language": "java",
              "content": "@Service\npublic class SecurityAuditService {\n    \n    public void logAccessAttempt() {\n        Authentication auth = SecurityContextHolder.getContext().getAuthentication();\n        \n        if (auth != null && auth.isAuthenticated()) {\n            String username = auth.getName();\n            Collection<? extends GrantedAuthority> authorities = auth.getAuthorities();\n            \n            boolean isAdmin = authorities.stream()\n                .anyMatch(a -> a.getAuthority().equals(\"ROLE_ADMIN\"));\n            \n            // Authentication: Who (username)\n            // Authorization: What they can do (isAdmin)\n        }\n    }\n}\n\n@Configuration\n@EnableWebSecurity\npublic class SecurityConfig {\n    @Bean\n    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n        http\n            .authorizeHttpRequests(auth -> auth\n                .requestMatchers(\"/public/**\").permitAll()\n                .requestMatchers(\"/admin/**\").hasRole(\"ADMIN\")  // Authorization\n                .anyRequest().authenticated()  // Requires Authentication\n            )\n            .formLogin(withDefaults());  // Authentication mechanism\n        return http.build();\n    }\n}"
            },
            "codeExplanations": {
              "english": "SecurityContextHolder provide karta hai access current Authentication object ko jo principal (identity) aur authorities (permissions) contain karta hai. Authentication filters populate karte hain is context ko. SecurityFilterChain demonstrate karta hai separation: formLogin authentication handle karta hai, jabki authorizeHttpRules authorization decisions handle karta hai roles (hasRole) pe ya koi bhi authentication require karke."
            },
            "keyPoints": [
              "Authentication: Establishing identity (username/password verification)",
              "Authorization: Checking permissions (role-based access control)",
              "SecurityContext holds Authentication object for current thread",
              "Authentication results in GrantedAuthority collection for authorization decisions",
              "Spring Security filters perform authentication before authorization checks"
            ],
            "extras": {
              "flowDiagram": "Request -> AuthenticationFilter (verify credentials) -> SecurityContext (store Authentication) -> AuthorizationFilter (check roles) -> Resource",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "Authentication",
                    "Authorization"
                  ],
                  "rows": [
                    [
                      "Question",
                      "Who are you?",
                      "What can you do?"
                    ],
                    [
                      "Process",
                      "Credential verification",
                      "Permission evaluation"
                    ],
                    [
                      "Result",
                      "Authentication object",
                      "Access granted/denied"
                    ],
                    [
                      "Mechanisms",
                      "Form, JWT, OAuth2",
                      "Roles, ACLs, SpEL expressions"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "sec-form-1",
            "title": "Form Login",
            "explanations": {
              "english": "Form-based authentication HTTP sessions use karta hai requests ke beech state maintain karne ke liye. Spring Security default login page provide karta hai ya custom login endpoints allow karta hai. Successful authentication pe, ek session create hoti hai (default in-memory ya JDBC-backed clustering ke liye) aur ek cookie (JSESSIONID) user track karta hai. CSRF protection automatically enable ho jata hai state-changing operations ke liye. Session fixation protection login ke baad session IDs regenerate karta hai session hijacking prevent karne ke liye. Ye approach traditional server-rendered web applications ke liye suitable hai."
            },
            "code": {
              "title": "Form Login Configuration",
              "language": "java",
              "content": "@Bean\npublic SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n    http\n        .authorizeHttpRequests(auth -> auth\n            .requestMatchers(\"/login\", \"/css/**\", \"/js/**\").permitAll()\n            .requestMatchers(\"/dashboard\").hasAnyRole(\"USER\", \"ADMIN\")\n            .requestMatchers(\"/admin/**\").hasRole(\"ADMIN\")\n        )\n        .formLogin(form -> form\n            .loginPage(\"/login\")\n            .loginProcessingUrl(\"/perform-login\")\n            .defaultSuccessUrl(\"/dashboard\", false)\n            .failureUrl(\"/login?error=true\")\n            .usernameParameter(\"email\")\n            .passwordParameter(\"pass\")\n        )\n        .logout(logout -> logout\n            .logoutUrl(\"/logout\")\n            .logoutSuccessUrl(\"/login?logout=true\")\n            .invalidateHttpSession(true)\n            .deleteCookies(\"JSESSIONID\")\n        )\n        .sessionManagement(session -> session\n            .sessionCreationPolicy(SessionCreationPolicy.IF_REQUIRED)\n            .maximumSessions(1)\n            .maxSessionsPreventsLogin(false)\n        );\n    return http.build();\n}\n\n@Bean\npublic UserDetailsService userDetailsService() {\n    UserDetails user = User.builder()\n        .username(\"user@example.com\")\n        .password(passwordEncoder().encode(\"password\"))\n        .roles(\"USER\")\n        .build();\n    return new InMemoryUserDetailsManager(user);\n}"
            },
            "codeExplanations": {
              "english": "Configure karta hai custom login page /login pe jabki static resources permit kiye gaye hain. loginProcessingUrl specify karta hai jahan form POST karta hai credentials. defaultSuccessUrl set karta hai post-login destination ko false ke saath allowing redirect to originally requested page. Session management restrict karta hai single concurrent session per user. In-memory user store demos ke liye suitable hai; production mein database-backed UserDetailsService use hota hai."
            },
            "keyPoints": [
              "Session-based authentication with JSESSIONID cookie",
              "CSRF protection automatically enabled for non-GET requests",
              "Session fixation protection regenerates ID after login",
              "Customizable login pages, success handlers, and failure URLs",
              "Concurrent session control prevents account sharing"
            ],
            "extras": {
              "flowDiagram": "Browser -> GET /login -> Render Form\n       -> POST /perform-login -> Validate -> Create Session -> Set Cookie -> Redirect /dashboard\n       -> Subsequent Requests -> Auto-authenticate via Session Cookie",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "sec-jwt-1",
            "title": "JWT (Stateless Authentication)",
            "explanations": {
              "english": "JSON Web Tokens stateless authentication enable karte hain jahan server signed tokens validate karta hai containing claims rather than maintaining session state. Ek JWT three parts consist karta hai: header (algorithm), payload (claims jaise username, roles, expiration), aur signature (verification). Login pe, server token issue karta hai; subsequent requests mein ye Authorization header mein include hota hai (Bearer scheme). Spring Security signature validate karta hai aur authorities extract karta hai bina database lookups ke. Ye horizontally scale karta hai bina session clustering ke lekin token expiration handling aur secure client storage require karta hai."
            },
            "code": {
              "title": "JWT Security Configuration",
              "language": "java",
              "content": "@Component\npublic class JwtAuthenticationFilter extends OncePerRequestFilter {\n    \n    @Override\n    protected void doFilterInternal(HttpServletRequest request, \n                                    HttpServletResponse response, \n                                    FilterChain chain) throws ServletException, IOException {\n        String header = request.getHeader(\"Authorization\");\n        \n        if (header != null && header.startsWith(\"Bearer \")) {\n            String token = header.substring(7);\n            if (JwtUtil.validateToken(token)) {\n                String username = JwtUtil.getUsernameFromToken(token);\n                List<GrantedAuthority> authorities = JwtUtil.getAuthorities(token);\n                \n                UsernamePasswordAuthenticationToken auth = \n                    new UsernamePasswordAuthenticationToken(username, null, authorities);\n                auth.setDetails(new WebAuthenticationDetailsSource().buildDetails(request));\n                SecurityContextHolder.getContext().setAuthentication(auth);\n            }\n        }\n        chain.doFilter(request, response);\n    }\n}\n\n@Bean\npublic SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n    http\n        .csrf(csrf -> csrf.disable())  // Stateless doesn't need CSRF\n        .sessionManagement(session -> \n            session.sessionCreationPolicy(SessionCreationPolicy.STATELESS))\n        .authorizeHttpRequests(auth -> auth\n            .requestMatchers(\"/api/auth/**\").permitAll()\n            .anyRequest().authenticated()\n        )\n        .addFilterBefore(jwtFilter, UsernamePasswordAuthenticationFilter.class);\n    return http.build();\n}\n\n// Token generation utility\npublic class JwtUtil {\n    private static final String SECRET = \"mySecretKey\";\n    private static final long EXPIRATION = 86400000; // 24 hours\n    \n    public static String generateToken(UserDetails userDetails) {\n        return Jwts.builder()\n            .setSubject(userDetails.getUsername())\n            .claim(\"roles\", userDetails.getAuthorities())\n            .setIssuedAt(new Date())\n            .setExpiration(new Date(System.currentTimeMillis() + EXPIRATION))\n            .signWith(SignatureAlgorithm.HS256, SECRET)\n            .compact();\n    }\n}"
            },
            "codeExplanations": {
              "english": "JwtAuthenticationFilter intercept karta hai requests Authorization header se Bearer tokens extract aur validate karne ke liye. Valid tokens Authentication objects create karte hain bina session creation ke. SecurityFilterChain CSRF aur sessions disable karta hai (STATELESS), JWT filter add karta hai standard authentication filters se pehle. Utility signed tokens generate karta hai claims including roles aur expiration ke saath."
            },
            "keyPoints": [
              "JWT contains claims (identity, roles) signed with secret key",
              "No server-side session storage required - truly stateless",
              "Sent in Authorization: Bearer <token> header",
              "Signature verification ensures token integrity",
              "Token expiration requires refresh strategy or re-login",
              "Scales horizontally without session clustering"
            ],
            "extras": {
              "flowDiagram": "Login -> Validate Credentials -> Generate JWT -> Return to Client\nRequest -> Extract JWT from Header -> Validate Signature -> Parse Claims -> Authenticate -> Access Resource",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "Session",
                    "JWT"
                  ],
                  "rows": [
                    [
                      "Storage",
                      "Server memory/database",
                      "Client-side (browser storage)"
                    ],
                    [
                      "Scalability",
                      "Requires sticky sessions or shared store",
                      "Any server can validate"
                    ],
                    [
                      "CSRF",
                      "Vulnerable, needs tokens",
                      "Immune (stateless)"
                    ],
                    [
                      "Logout",
                      "Invalidate session",
                      "Wait for expiration or blacklist"
                    ],
                    [
                      "Size",
                      "Small cookie",
                      "Larger header payload"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "sec-method-1",
            "title": "Method Security (@PreAuthorize)",
            "explanations": {
              "english": "Method security fine-grained authorization provide karta hai service layer pe Spring Expression Language (SpEL) use karke. @PreAuthorize expressions evaluate karta hai method execution se pehle, preventing invocation agar conditions fail ho. @PostAuthorize execution ke baad check karta hai, useful hota hai return object ownership verify karne ke liye. @Secured ek older alternative hai jo simple role lists support karta hai lekin SpEL flexibility nahi hai. Ye annotations row-level security scenarios enable karte hain jaise 'user can only view their own orders' expressions jaise 'returnObject.owner == authentication.name' ke through. Method security @EnableMethodSecurity configuration require karta hai."
            },
            "code": {
              "title": "Method-Level Authorization",
              "language": "java",
              "content": "@Configuration\n@EnableMethodSecurity(prePostEnabled = true, securedEnabled = true)\npublic class MethodSecurityConfig {\n}\n\n@Service\npublic class OrderService {\n    \n    @PreAuthorize(\"hasRole('ADMIN') or #userId == authentication.principal.id\")\n    public List<Order> getUserOrders(Long userId) {\n        return orderRepository.findByUserId(userId);\n    }\n    \n    @PreAuthorize(\"hasAuthority('ORDER_DELETE') and #order.status != 'SHIPPED'\")\n    public void cancelOrder(Order order) {\n        order.cancel();\n    }\n    \n    @PostAuthorize(\"returnObject.owner == authentication.name or hasRole('ADMIN')\")\n    public Document getDocument(Long id) {\n        return documentRepository.findById(id).orElseThrow();\n    }\n    \n    @PreAuthorize(\"hasRole('MANAGER')\")\n    @Transactional\n    public void bulkUpdateOrders(List<Long> orderIds, Status newStatus) {\n        // Only managers can execute bulk updates\n    }\n}\n\n// Using @Secured (simpler, no SpEL)\n@Secured({\"ROLE_ADMIN\", \"ROLE_MANAGER\"})\npublic void sensitiveOperation() {}"
            },
            "codeExplanations": {
              "english": "@PreAuthorize SpEL use karta hai: hasRole ROLE_ADMIN check karta hai, #userId method parameter reference karta hai, authentication.principal.id current user ID access karta hai. @PostAuthorize prevent karta hai method return agar document owner current user se match nahi karta (unless admin). Ye web layer se independent row-level security implement karta hai."
            },
            "keyPoints": [
              "@PreAuthorize: Validate before method execution using SpEL",
              "@PostAuthorize: Validate after execution, can inspect returnObject",
              "SpEL supports method parameters (#argName), roles (hasRole), and authentication object",
              "Use @EnableMethodSecurity to activate",
              "Method security uses AOP proxies; internal calls bypass security",
              "@Secured is simpler but less powerful than @PreAuthorize"
            ],
            "extras": {
              "flowDiagram": "Request -> Controller (authenticated) -> Service Method -> @PreAuthorize Check -> Proceed/Reject\n                                         -> Return Object -> @PostAuthorize Check -> Return to Caller",
              "comparisonTable": [
                {
                  "headers": [
                    "Annotation",
                    "Timing",
                    "SpEL Support",
                    "Use Case"
                  ],
                  "rows": [
                    [
                      "@PreAuthorize",
                      "Before execution",
                      "Yes",
                      "Input validation, role checks"
                    ],
                    [
                      "@PostAuthorize",
                      "After execution",
                      "Yes",
                      "Result filtering, ownership verification"
                    ],
                    [
                      "@Secured",
                      "Before execution",
                      "No (roles only)",
                      "Simple role-based access"
                    ],
                    [
                      "@PreFilter",
                      "Before execution",
                      "Yes",
                      "Filter collection arguments"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "sec-oauth-1",
            "title": "OAuth2 & Social Login",
            "explanations": {
              "english": "OAuth2 ek authorization framework hai jo delegated access enable karta hai. Spring Boot teen roles support karta hai: Client (Login with Google/GitHub), Resource Server (APIs protect karna JWT validation ke saath), aur Authorization Server (tokens issue karna - ab separate Spring Authorization Server project mein move ho gaya hai). Client ke roop mein, Spring Security users ko provider (Google) pe redirect karta hai, callback handle karta hai, code ko tokens ke liye exchange karta hai, aur local user session create karta hai. Ye password storage eliminate karta hai while leveraging established identity providers. Resource Server configuration incoming JWTs ko authorization server se validate karta hai."
            },
            "code": {
              "title": "OAuth2 Client and Resource Server",
              "language": "java",
              "content": "// OAuth2 Client (Login with Google)\n@Configuration\npublic class OAuth2ClientConfig {\n    @Bean\n    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {\n        http\n            .oauth2Login(oauth2 -> oauth2\n                .loginPage(\"/login\")\n                .defaultSuccessUrl(\"/dashboard\", true)\n                .userInfoEndpoint(userInfo -> userInfo\n                    .oidcUserService(customOidcUserService())\n                )\n            )\n            .authorizeHttpRequests(auth -> auth\n                .anyRequest().authenticated()\n            );\n        return http.build();\n    }\n}\n\n// application.yml for OAuth2 Client:\n// spring:\n//   security:\n//     oauth2:\n//       client:\n//         registration:\n//           google:\n//             client-id: ${GOOGLE_CLIENT_ID}\n//             client-secret: ${GOOGLE_CLIENT_SECRET}\n//             scope: openid,profile,email\n\n// OAuth2 Resource Server (API JWT validation)\n@Bean\npublic SecurityFilterChain resourceServerChain(HttpSecurity http) throws Exception {\n    http\n        .csrf(csrf -> csrf.disable())\n        .oauth2ResourceServer(oauth2 -> oauth2\n            .jwt(jwt -> jwt\n                .jwtDecoder(jwtDecoder())\n                .jwtAuthenticationConverter(jwtAuthenticationConverter())\n            )\n        )\n        .authorizeHttpRequests(auth -> auth\n            .requestMatchers(\"/api/public\").permitAll()\n            .requestMatchers(\"/api/admin\").hasAuthority(\"SCOPE_admin\")\n            .anyRequest().authenticated()\n        );\n    return http.build();\n}\n\n@Bean\npublic JwtDecoder jwtDecoder() {\n    return NimbusJwtDecoder.withJwkSetUri(\"https://auth-server/.well-known/jwks.json\").build();\n}"
            },
            "codeExplanations": {
              "english": "OAuth2 Client configuration 'Login with Google' enable karta hai using OIDC (OpenID Connect). userInfoEndpoint customize karta hai kaise user details load hote hain. Resource Server configuration JWTs ko validate karta hai ek authorization server se JWKS (JSON Web Key Set) URI use karke signature verification ke liye. SCOPE_admin specific OAuth2 scopes check karta hai token mein."
            },
            "keyPoints": [
              "OAuth2 Client: Delegate authentication to Google/GitHub/Auth0",
              "OAuth2 Resource Server: Protect APIs by validating JWT access tokens",
              "Authorization Server: Now in separate Spring Authorization Server project",
              "OIDC (OpenID Connect) built on OAuth2 for identity verification",
              "Resource Server extracts scopes/roles from JWT for authorization",
              "Social login eliminates local password storage and management"
            ],
            "extras": {
              "flowDiagram": "User -> App -> Redirect to Google -> Google Auth -> Callback with Code -> Exchange Code for Tokens -> Create Session\nAPI Client -> Request with Access Token -> Resource Server -> Validate JWT Signature -> Extract Claims -> Check Scopes -> Allow/Deny",
              "comparisonTable": [
                {
                  "headers": [
                    "Role",
                    "Purpose",
                    "Spring Module"
                  ],
                  "rows": [
                    [
                      "Client",
                      "Login via external providers",
                      "spring-boot-starter-oauth2-client"
                    ],
                    [
                      "Resource Server",
                      "Validate tokens, protect APIs",
                      "spring-boot-starter-oauth2-resource-server"
                    ],
                    [
                      "Authorization Server",
                      "Issue tokens",
                      "Spring Authorization Server"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          }
        ]
      },
      {
        "id": "actuator-monitoring-testing",
        "title": "Actuator, Monitoring & Testing",
        "description": "Production-ready features ka comprehensive coverage including health checks, metrics collection, structured logging, aur testing strategies unit tests se lekar full integration scenarios tak containerized dependencies ke saath.",
        "topics": [
          {
            "id": "act-mon-1",
            "title": "Health Endpoints",
            "explanations": {
              "english": "Spring Boot Actuator /actuator/health endpoint provide karta hai jo application health status expose karta hai monitoring systems aur orchestration platforms ke liye. Ye endpoint aggregate karta hai information HealthIndicator beans se jo database connectivity, disk space, message broker availability, aur custom system states check karte hain. By default, ye simple UP ya DOWN status return karta hai, lekin configure kiya ja sakta hai detailed health information show karne ke liye authorized users ke liye. Ye endpoint crucial hai load balancers ke liye traffic routing determine karne ke liye aur Kubernetes ke liye restart decisions lene ke liye."
            },
            "code": {
              "title": "Custom Health Indicator",
              "language": "java",
              "content": "@Component\npublic class ExternalApiHealthIndicator implements HealthIndicator {\n    \n    private final RestTemplate restTemplate;\n    \n    public ExternalApiHealthIndicator(RestTemplate restTemplate) {\n        this.restTemplate = restTemplate;\n    }\n    \n    @Override\n    public Health health() {\n        try {\n            ResponseEntity<String> response = restTemplate.getForEntity(\n                \"https://api.service.com/health \", String.class);\n            \n            if (response.getStatusCode().is2xxSuccessful()) {\n                return Health.up()\n                    .withDetail(\"api.status\", \"Available\")\n                    .withDetail(\"responseTimeMs\", response.getHeaders().getFirst(\"X-Response-Time\"))\n                    .build();\n            }\n            return Health.down()\n                .withDetail(\"api.status\", \"Unhealthy\")\n                .withDetail(\"statusCode\", response.getStatusCode())\n                .build();\n        } catch (Exception e) {\n            return Health.down()\n                .withException(e)\n                .build();\n        }\n    }\n}"
            },
            "codeExplanations": {
              "english": "Ye custom indicator ek external API dependency check karta hai. Ye HealthIndicator implement karta hai aur Health.up() ya Health.down() return karta hai contextual details ke saath. Ye details health JSON mein appear hote hain jab authorized ho. Exceptions capture kiye jaate hain aur health details mein include kiye jaate hain debugging ke liye."
            },
            "keyPoints": [
              "/actuator/health aggregates all HealthIndicator beans",
              "Custom indicators implement HealthIndicator interface",
              "Return Health.up() or Health.down() with optional details",
              "Sensitive details hidden by default; configure management.endpoint.health.show-details for visibility",
              "Used by load balancers and Kubernetes probes to assess application state"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "act-mon-2",
            "title": "Metrics",
            "explanations": {
              "english": "Spring Boot Micrometer use karta hai as metrics facade code instrument karne aur export karne ke liye monitoring systems jaise Prometheus, CloudWatch, ya Datadog ko. Common metrics mein include hain JVM metrics (memory, GC, threads), system metrics (CPU, file descriptors), aur application metrics (HTTP request counts/timers, datasource connection pool usage). Developers custom metrics create kar sakte hain Counter, Timer, Gauge, aur DistributionSummary meters use karke. Ye metrics application performance aur resource utilization ki visibility provide karte hain capacity planning aur alerting ke liye."
            },
            "code": {
              "title": "Custom Metrics",
              "language": "java",
              "content": "@Service\npublic class OrderService {\n    private final MeterRegistry meterRegistry;\n    private final Counter orderCounter;\n    private final Timer orderProcessingTimer;\n    \n    public OrderService(MeterRegistry meterRegistry) {\n        this.meterRegistry = meterRegistry;\n        this.orderCounter = Counter.builder(\"orders.created\")\n            .description(\"Total orders created\")\n            .register(meterRegistry);\n        this.orderProcessingTimer = Timer.builder(\"order.processing.time\")\n            .description(\"Time taken to process orders\")\n            .publishPercentiles(0.5, 0.95, 0.99)\n            .register(meterRegistry);\n    }\n    \n    public void processOrder(Order order) {\n        orderProcessingTimer.record(() -> {\n            // Processing logic\n            saveToDatabase(order);\n            sendNotification(order);\n        });\n        orderCounter.increment();\n        \n        // Gauge for current queue size\n        meterRegistry.gauge(\"orders.queue.size\", \n            Tags.of(\"priority\", order.getPriority()), \n            orderQueue.size());\n    }\n}"
            },
            "codeExplanations": {
              "english": "Counter track karta hai monotonically increasing values jaise total orders. Timer duration measure karta hai aur percentiles publish kar sakta hai SLA monitoring ke liye. Gauges current values track karte hain jaise queue sizes. Saare metrics automatically export hote hain configured registries ke through (e.g., Prometheus). Tags allow dimensional metrics filtering aur aggregation ke liye."
            },
            "keyPoints": [
              "Micrometer provides vendor-neutral metrics abstraction",
              "Counter for incrementing counts, Timer for durations, Gauge for current values",
              "Tags enable multi-dimensional metrics (e.g., status=200, endpoint=/api/users)",
              "Percentile histograms track SLA compliance (p95, p99)",
              "Auto-configured metrics include JVM, system, and web request statistics"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "act-mon-3",
            "title": "Environment Endpoints",
            "explanations": {
              "english": "Actuator several endpoints provide karta hai application configuration inspect karne ke liye bina server access ke. /actuator/env display karta hai properties saare PropertySource instances se including system properties, environment variables, aur application properties placeholder resolution ke saath. /actuator/configprops show karta hai @ConfigurationProperties beans aur unke binding results. /actuator/conditions display karta hai auto-configuration reports showing kaunsi configurations match hui ya fail hui. Ye endpoints invaluable hain debugging configuration issues production environments mein jahan debugger attach nahi kar sakte."
            },
            "code": {
              "title": "Configuration Inspection",
              "language": "java",
              "content": "// Access via: GET /actuator/env\n// Response snippet:\n{\n  \"activeProfiles\": [\"prod\"],\n  \"propertySources\": [\n    {\n      \"name\": \"systemEnvironment\",\n      \"properties\": {\n        \"DB_PASSWORD\": {\"value\": \"******\", \"origin\": \"System Environment\"}\n      }\n    },\n    {\n      \"name\": \"applicationConfig: [classpath:/application-prod.yml]\",\n      \"properties\": {\n        \"server.port\": {\"value\": 8080}\n      }\n    }\n  ]\n}\n\n// GET /actuator/configprops\n{\n  \"app.mail-org.springframework.boot.actuate.autoconfigure.context.properties.ConfigurationPropertiesReportEndpoint\": {\n    \"prefix\": \"app.mail\",\n    \"properties\": {\n      \"host\": \"smtp.gmail.com\",\n      \"port\": 587\n    }\n  }\n}"
            },
            "codeExplanations": {
              "english": "env endpoint saare properties ke final resolved values dikhata hai saare sources se unke origins ke saath. Sensitive values jaise passwords asterisks ke saath mask kiye jaate hain security ke liye. configprops endpoint expose karta hai bound @ConfigurationProperties beans, showing exactly kaise external configuration Java objects mein map hui hai."
            },
            "keyPoints": [
              "/actuator/env shows all properties from all sources with resolution order",
              "/actuator/configprops displays bound @ConfigurationProperties beans",
              "/actuator/conditions shows auto-configuration match results",
              "Sensitive data automatically masked based on property name patterns",
              "Useful for verifying production configuration without shell access"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "act-mon-4",
            "title": "Kubernetes Readiness & Liveness",
            "explanations": {
              "english": "Spring Boot first-class support provide karta hai Kubernetes health probes ke liye specialized endpoints ke through. Liveness probes (/actuator/health/liveness) indicate karta hai ki application running hai aur restart hona chahiye agar fail ho, deadlocks ya infinite loops catch karne ke liye. Readiness probes (/actuator/health/readiness) indicate karta hai ki application ready hai traffic accept karne ke liye, startup pe ya overloaded hone pe false rehta hai. Ye general health endpoint se separate hain taaki Kubernetes distinct decisions le sake restarts versus traffic routing ke baare mein. ApplicationAvailability interface programmatic state management allow karta hai."
            },
            "code": {
              "title": "Probe Configuration",
              "language": "java",
              "content": "@Component\npublic class DatabaseStartupValidator implements ApplicationListener<ApplicationReadyEvent> {\n    \n    private final ApplicationAvailability availability;\n    private final DataSource dataSource;\n    \n    public DatabaseStartupValidator(ApplicationAvailability availability, DataSource dataSource) {\n        this.availability = availability;\n        this.dataSource = dataSource;\n    }\n    \n    @Override\n    public void onApplicationEvent(ApplicationReadyEvent event) {\n        try (Connection conn = dataSource.getConnection()) {\n            if (conn.isValid(5)) {\n                // Signal readiness\n                availability.markAsReady();\n            } else {\n                availability.markAsNotReady();\n            }\n        } catch (SQLException e) {\n            availability.markAsNotReady();\n        }\n    }\n}\n\n// application.yml:\n// management:\n//   endpoint:\n//     health:\n//       probes:\n//         enabled: true\n//       group:\n//         readiness:\n//           include: readinessState,db\n//         liveness:\n//           include: livenessState"
            },
            "codeExplanations": {
              "english": "ApplicationAvailability interface programmatic control allow karta hai readiness state ka. Validator check karta hai database connectivity startup pe before marking the application ready. Kubernetes alag se call karta hai /actuator/health/readiness aur /actuator/health/liveness pod lifecycle decisions lene ke liye. Probes explicitly enable aur configure kiye jaane chahiye relevant health indicators include karke."
            },
            "keyPoints": [
              "Liveness probe: App is running (restart if false)",
              "Readiness probe: App is ready to accept traffic (remove from service if false)",
              "Endpoints: /actuator/health/liveness and /actuator/health/readiness",
              "ApplicationAvailability interface for programmatic state changes",
              "Enabled via management.endpoint.health.probes.enabled=true"
            ],
            "extras": {
              "flowDiagram": "Pod Start -> Liveness=UP (default) -> App initializes -> Readiness=UP -> Traffic allowed\nApp deadlock -> Liveness=DOWN -> Kubernetes restarts pod\nDB connection lost -> Readiness=DOWN -> Traffic routed to other pods",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "act-mon-5",
            "title": "Prometheus & Grafana Integration",
            "explanations": {
              "english": "Prometheus ek popular open-source monitoring solution hai jo scrape karta hai metrics applications se HTTP endpoints ke through. Spring Boot Micrometer ke saath metrics expose karta hai Prometheus format mein /actuator/prometheus pe micrometer-registry-prometheus dependency add karke. Prometheus scrape karta hai in metrics ko intervals pe (typically 15-30 seconds), store karta hai time-series database mein, aur alerting rules evaluate karta hai. Grafana connect karta hai Prometheus se dashboards create karne ke liye visualizing metrics jaise request rates, error rates, aur latency histograms. Ye stack comprehensive observability provide karta hai Spring Boot applications ke liye production environments mein."
            },
            "code": {
              "title": "Prometheus Configuration",
              "language": "yaml",
              "content": "# pom.xml dependency\n// <dependency>\n//     <groupId>io.micrometer</groupId>\n//     <artifactId>micrometer-registry-prometheus</artifactId>\n// </dependency>\n\n# application.yml\nmanagement:\n  endpoints:\n    web:\n      exposure:\n        include: health,info,prometheus,metrics\n  endpoint:\n    prometheus:\n      enabled: true\n  metrics:\n    tags:\n      application: ${spring.application.name}\n      environment: ${spring.profiles.active:default}\n    distribution:\n      percentiles-histogram:\n        http.server.requests: true\n      slo:\n        http.server.requests: 100ms,500ms,1s,5s\n\n# Scraped by Prometheus at: GET /actuator/prometheus\n# http_server_requests_seconds_count{application=\"order-service\",uri=\"/api/orders\",status=\"200\"} 1024"
            },
            "codeExplanations": {
              "english": "micrometer-registry-prometheus dependency /actuator/prometheus endpoint expose karta hai Prometheus exposition format mein. Tags add karte hain dimensions jaise application name filtering ke liye. SLAs define karte hain histograms ke buckets latency objectives track karne ke liye. Prometheus server scrape karta hai is endpoint ko periodically time-series data collect karne ke liye alerting aur visualization ke liye Grafana mein."
            },
            "keyPoints": [
              "Add micrometer-registry-prometheus to expose /actuator/prometheus",
              "Prometheus scrapes metrics in text format (Pull model)",
              "Grafana queries Prometheus for dashboards and alerts",
              "Configure tags for multi-dimensional filtering (app, instance, endpoint)",
              "Histogram percentiles track latency distribution (p50, p95, p99)"
            ],
            "extras": {
              "flowDiagram": "Spring Boot App (/actuator/prometheus) <- scrape <- Prometheus (TSDB) <- query <- Grafana (Dashboards/Alerts)",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "log-1",
            "title": "SLF4J",
            "explanations": {
              "english": "SLF4J (Simple Logging Facade for Java) abstraction layer ka kaam karta hai various logging frameworks jaise Logback, Log4j2, aur java.util.logging ke liye. Ye developers ko allow karta hai SLF4J API use karke logging code likhne ke liye jabki actual implementation deployment time pe determine hota hai. Ye vendor lock-in prevent karta hai aur allow karta hai easily switching between logging frameworks bina code changes ke. Spring Boot internally SLF4J use karta hai aur automatically Logback provide karta hai as default implementation. LoggerFactory logger instances create karta hai jo parameterized messages support karte hain, string concatenation overhead avoid karte hain jab logging disabled ho."
            },
            "code": {
              "title": "SLF4J Usage",
              "language": "java",
              "content": "import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n@Service\npublic class PaymentService {\n    private static final Logger logger = LoggerFactory.getLogger(PaymentService.class);\n    \n    public void processPayment(Payment payment) {\n        // Parameterized logging - no string concatenation if DEBUG disabled\n        logger.debug(\"Processing payment for order: {}, amount: {}\", \n                     payment.getOrderId(), payment.getAmount());\n        \n        try {\n            gateway.charge(payment);\n            logger.info(\"Payment successful for order: {}\", payment.getOrderId());\n        } catch (PaymentException e) {\n            // SLF4J accepts exception as last argument for stack traces\n            logger.error(\"Payment failed for order: {}\", payment.getOrderId(), e);\n        }\n        \n        // Guard for expensive computation\n        if (logger.isTraceEnabled()) {\n            logger.trace(\"Full payment payload: {}\", expensiveSerialization(payment));\n        }\n    }\n}"
            },
            "codeExplanations": {
              "english": "LoggerFactory.getLogger(Class) logger instance create karta hai class ke liye. Parameterized logging {} placeholders use karta hai string formatting defer karne ke liye until level enabled check ho. Exceptions last argument ke roop mein pass kiye jaate hain stack traces ke saath. isTraceEnabled guard karta hai expensive operations ko jo sirf tab run hone chahiye jab trace logging active ho."
            },
            "keyPoints": [
              "Facade pattern decouples logging API from implementation",
              "Parameterized messages avoid performance cost of disabled log levels",
              "LoggerFactory creates logger instances specific to the class",
              "Pass exception as last argument to include stack trace",
              "Spring Boot auto-configures SLF4J with Logback"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "log-2",
            "title": "Logback Configuration",
            "explanations": {
              "english": "Logback Spring Boot mein default logging implementation hai, configure kiya ja sakta hai logback-spring.xml se classpath mein ya application properties ke through. Ye console aur file appenders support karta hai rolling policies ke saath disk space manage karne ke liye. Pattern layouts customize karte hain log format including timestamps, log levels, class names, aur messages. Spring Boot sane defaults provide karta hai lekin complete customization allow karta hai different profiles ke liye. Configuration filters support karte hain control karne ka kaunse log levels kaunse appenders pe jaate hain, separating INFO logs console pe aur ERROR logs specific files mein alerting ke liye."
            },
            "code": {
              "title": "Logback Configuration",
              "language": "xml",
              "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<configuration>\n    <springProfile name=\"dev\">\n        <appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\">\n            <encoder>\n                <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>\n            </encoder>\n        </appender>\n        <root level=\"DEBUG\">\n            <appender-ref ref=\"CONSOLE\" />\n        </root>\n    </springProfile>\n    \n    <springProfile name=\"prod\">\n        <appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n            <file>/var/log/myapp/application.log</file>\n            <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n                <fileNamePattern>/var/log/myapp/application.%d{yyyy-MM-dd}.%i.log</fileNamePattern>\n                <maxHistory>30</maxHistory>\n                <maxFileSize>100MB</maxFileSize>\n            </rollingPolicy>\n            <encoder>\n                <pattern>%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n</pattern>\n            </encoder>\n        </appender>\n        <root level=\"INFO\">\n            <appender-ref ref=\"FILE\" />\n        </root>\n    </springProfile>\n</configuration>"
            },
            "codeExplanations": {
              "english": "logback-spring.xml Spring profiles support karta hai different configurations apply karne ke liye per environment. Dev profile DEBUG level pe console pe log karta hai compact format ke saath. Prod profile INFO level pe rolling files mein log karta hai time aur size based policies ke saath, 30 days ki history rakhte hue max 100MB files ke saath. Pattern syntax control karta hai timestamp format, thread names, padding, aur logger abbreviation."
            },
            "keyPoints": [
              "logback-spring.xml supports Spring profiles (<springProfile>)",
              "RollingFileAppender manages log rotation by time or size",
              "Pattern layout controls visual format of log lines",
              "Root logger level filters which messages are logged",
              "Appenders determine output destination (console, file, remote)"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "log-3",
            "title": "Structured JSON Logging",
            "explanations": {
              "english": "Structured JSON logging format karta hai log entries ko JSON objects ke roop mein rather than plain text lines, unhe easily parseable banata hai log aggregation systems jaise ELK (Elasticsearch, Logstash, Kibana), Splunk, ya Fluentd ke liye. Har log entry contain karta hai fields jaise timestamp, level, logger, message, aur MDC context JSON properties ke roop mein. Ye complex regex parsing ki zarurat khatam karta hai structured data extract karne ke liye. Logstash-logback-encoder ya Jackson-based encoders configure kiye ja sakte hain application metadata, trace IDs, aur custom fields include karne ke liye, enabling efficient filtering aur correlation centralized logging platforms mein."
            },
            "code": {
              "title": "JSON Logging Setup",
              "language": "xml",
              "content": "<configuration>\n    <appender name=\"JSON\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\">\n            <includeContext>true</includeContext>\n            <includeMdc>true</includeMdc>\n            <customFields>{\"service\":\"order-service\",\"version\":\"1.2.3\"}</customFields>\n        </encoder>\n    </appender>\n    \n    <!-- Alternative: Pattern based JSON -->\n    <appender name=\"JSON_PATTERN\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <encoder class=\"ch.qos.logback.core.encoder.LayoutWrappingEncoder\">\n            <layout class=\"ch.qos.logback.contrib.json.classic.JsonLayout\">\n                <jsonFormatter class=\"ch.qos.logback.contrib.jackson.JacksonJsonFormatter\">\n                    <prettyPrint>false</prettyPrint>\n                </jsonFormatter>\n                <timestampFormat>yyyy-MM-dd' 'HH:mm:ss.SSS</timestampFormat>\n            </layout>\n        </encoder>\n    </appender>\n    \n    <root level=\"INFO\">\n        <appender-ref ref=\"JSON\" />\n    </root>\n</configuration>"
            },
            "codeExplanations": {
              "english": "LogstashEncoder produce karta hai JSON formatted logs Logstash compatible. includeMdc add karta hai Mapped Diagnostic Context fields JSON structure mein. customFields inject karta hai static metadata jaise service name aur version identification ke liye multi-service environments mein. Alternative Jackson-based layout zyada control provide karta hai JSON structure aur formatting pe."
            },
            "keyPoints": [
              "JSON format enables easy parsing by log aggregators (ELK, Splunk)",
              "LogstashEncoder or JacksonJsonFormatter convert logs to JSON",
              "Static customFields identify service, version, environment",
              "MDC fields automatically included as JSON properties",
              "Eliminates fragile regex parsing of plain text logs"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Format",
                    "Human Readable",
                    "Machine Parseable",
                    "Query Complexity"
                  ],
                  "rows": [
                    [
                      "Plain Text",
                      "Yes",
                      "Requires regex",
                      "High"
                    ],
                    [
                      "JSON",
                      "Readable with formatting",
                      "Native",
                      "Low (field-based)"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "log-4",
            "title": "MDC for Request Tracing",
            "explanations": {
              "english": "MDC (Mapped Diagnostic Context) ek thread-local map hai jo contextual data store karta hai logging frameworks ke liye accessible. Ye essential hai log messages correlate karne ke liye jo same request ke belong karte hain across multiple classes aur methods. Common use cases mein include hain request IDs, user IDs, ya session IDs store karna jo har log line mein appear hote hain us request ke. MDC cleanup require karta hai request processing ke baad memory leaks prevent karne ke liye thread pools mein. Async ya reactive programming mein, MDC context explicitly propagate karna padta hai threads ke beech using MDC.getCopyOfContextMap() aur MDC.setContextMap()."
            },
            "code": {
              "title": "MDC Implementation",
              "language": "java",
              "content": "@Component\npublic class RequestIdFilter extends OncePerRequestFilter {\n    private static final String REQUEST_ID_HEADER = \"X-Request-ID\";\n    \n    @Override\n    protected void doFilterInternal(HttpServletRequest request, \n                                    HttpServletResponse response, \n                                    FilterChain chain) throws ServletException, IOException {\n        try {\n            String requestId = request.getHeader(REQUEST_ID_HEADER);\n            if (requestId == null) {\n                requestId = UUID.randomUUID().toString();\n            }\n            \n            MDC.put(\"requestId\", requestId);\n            MDC.put(\"userId\", SecurityContextHolder.getContext().getAuthentication().getName());\n            \n            response.setHeader(REQUEST_ID_HEADER, requestId);\n            chain.doFilter(request, response);\n        } finally {\n            MDC.clear(); // Critical to prevent leaks\n        }\n    }\n}\n\n// Log pattern: %X{requestId} %X{userId} %msg\n// Output: 2024-01-15 10:23:45 [abc-123] [john.doe] Processing order 456"
            },
            "codeExplanations": {
              "english": "Filter extract ya generate karta hai request ID, store karta hai MDC mein request duration ke liye, aur ensure karta hai cleanup finally block mein thread pollution prevent karne ke liye container ke thread pool mein. %X{key} logback patterns mein MDC values output karta hai. Ye allow karta hai specific request trace karna across all log lines even complex service calls mein."
            },
            "keyPoints": [
              "MDC is thread-local storage for logging context",
              "Stores requestId, userId, traceId for correlation",
              "Must call MDC.clear() in finally block to prevent leaks",
              "Access in log patterns via %X{key} or included in JSON logs",
              "Requires manual propagation in async/reactive contexts"
            ],
            "extras": {
              "flowDiagram": "Request -> Filter (MDC.put requestId)) -> Controller (log includes requestId) -> Service (log includes requestId) -> Filter (MDC.clear())",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "test-1",
            "title": "@WebMvcTest",
            "explanations": {
              "english": "@WebMvcTest ek test slice annotation hai jo Spring MVC infrastructure auto-configure karta hai controller layer testing ke liye bina full ApplicationContext start kiye. Ye MockMvc configure karta hai HTTP requests perform karne aur responses validate karne ke liye bina actual web server ke. Sirf @Controller, @ControllerAdvice, @JsonComponent, Filter, aur WebMvcConfigurer beans load hote hain. Dependencies typically mock kiye jaate hain @MockBean use karke. Ye approach significantly faster hai @SpringBootTest se request mappings, input validation, exception handling, aur response serialization verify karne ke liye web layer mein."
            },
            "code": {
              "title": "Controller Testing",
              "language": "java",
              "content": "@WebMvcTest(OrderController.class)\n@AutoConfigureMockMvc(addFilters = false) // Skip security filters for unit test\npublic class OrderControllerTest {\n    \n    @Autowired\n    private MockMvc mockMvc;\n    \n    @MockBean\n    private OrderService orderService;\n    \n    @Test\n    void shouldCreateOrder() throws Exception {\n        Order order = new Order(1L, \"PRODUCT-123\", 2);\n        when(orderService.create(any())).thenReturn(order);\n        \n        mockMvc.perform(post(\"/api/orders\")\n                .contentType(MediaType.APPLICATION_JSON)\n                .content(\"{\\\"productId\\\":\\\"PRODUCT-123\\\",\\\"quantity\\\":2}\"))\n            .andExpect(status().isCreated())\n            .andExpect(jsonPath(\"$.id\", is(1)))\n            .andExpect(jsonPath(\"$.productId\", is(\"PRODUCT-123\")))\n            .andExpect(header().string(\"Location\", containsString(\"/api/orders/1\")));\n    }\n    \n    @Test\n    void shouldReturnValidationError() throws Exception {\n        mockMvc.perform(post(\"/api/orders\")\n                .contentType(MediaType.APPLICATION_JSON)\n                .content(\"{\\\"productId\\\":\\\"\\\",\\\"quantity\\\":-1}\"))\n            .andExpect(status().isBadRequest())\n            .andExpect(jsonPath(\"$.errors\").isArray());\n    }\n}"
            },
            "codeExplanations": {
              "english": "@WebMvcTest sirf web layer components load karta hai. MockMvc simulate karta hai HTTP requests bina server run kiye. @MockBean create karta hai Mockito mocks service dependencies ke liye. Test verify karta hai HTTP status, JSON response structure using jsonPath, aur response headers. addFilters=false parameter security filters disable karta hai controller logic ko isolation mein test karne ke liye."
            },
            "keyPoints": [
              "Test slice for MVC layer only (controllers, advices, filters)",
              "Configures MockMvc for simulated HTTP requests",
              "Does not start embedded server (faster than @SpringBootTest)",
              "Use @MockBean for service layer dependencies",
              "Validates request mapping, serialization, validation, and exception handling"
            ],
            "extras": {
              "flowDiagram": "Test -> @WebMvcTest -> Load Controller + MockMvc -> Mock Service -> Execute Request -> Assert Response",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "test-2",
            "title": "@DataJpaTest",
            "explanations": {
              "english": "@DataJpaTest ek test slice hai JPA repository testing ke liye jo auto-configure karta hai embedded (in-memory) database by default, Spring Data JPA repositories configure karta hai, aur EntityManager setup karta hai. Ye web components, component scanning, aur other infrastructure exclude karta hai, solely data layer pe focus karta hai. Tests @Transactional hote hain by default aur rollback kar dete hain har test method ke baad test isolation maintain karne ke liye. While it defaults to H2, @AutoConfigureTestDatabase replace kar sakta hai real databases ke saath. Ye slice ideal hai custom queries, derived methods, aur entity mappings verify karne ke liye bina full application context load kiye."
            },
            "code": {
              "title": "Repository Testing",
              "language": "java",
              "content": "@DataJpaTest\n@AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE) // Use real DB defined in properties\npublic class OrderRepositoryTest {\n    \n    @Autowired\n    private OrderRepository repository;\n    \n    @Autowired\n    private TestEntityManager entityManager;\n    \n    @Test\n    void shouldFindOrdersByStatus() {\n        Order pending = new Order(Status.PENDING);\n        Order completed = new Order(Status.COMPLETED);\n        entityManager.persist(pending);\n        entityManager.persist(completed);\n        entityManager.flush();\n        \n        List<Order> results = repository.findByStatus(Status.PENDING);\n        \n        assertThat(results).hasSize(1);\n        assertThat(results.get(0).getStatus()).isEqualTo(Status.PENDING);\n    }\n    \n    @Test\n    void shouldExecuteCustomQuery() {\n        repository.save(new Order(\"ITEM-1\", LocalDateTime.now()));\n        repository.save(new Order(\"ITEM-2\", LocalDateTime.now().minusDays(1)));\n        \n        List<Order> recent = repository.findOrdersFromLast24Hours();\n        \n        assertThat(recent).hasSize(1);\n    }\n}"
            },
            "codeExplanations": {
              "english": "@DataJpaTest JPA components aur in-memory database configure karta hai. TestEntityManager provide karta hai JPA operations directly test setup ke liye. Default @Transactional rollback karta hai har test ke baad tests ko isolated rakhne ke liye. Replace.NONE use karta hai configured datasource ko embedded database replace karne ke bajaye, useful hota hai real database behavior test karne ke liye."
            },
            "keyPoints": [
              "Test slice for repositories and JPA components",
              "Auto-configures in-memory database (H2) by default",
              "Automatic entity scanning and repository configuration",
              "Tests run in transactions rolled back automatically",
              "TestEntityManager provides utility methods for setup and verification"
            ],
            "extras": {
              "flowDiagram": "Test -> @DataJpaTest -> Configure JPA + Embedded DB -> Test Method (Transaction) -> Auto Rollback",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "test-3",
            "title": "@JsonTest",
            "explanations": {
              "english": "@JsonTest ek test slice hai JSON serialization aur deserialization testing ke liye. Ye auto-configure karta hai Jackson ObjectMapper (ya Gson agar preferred ho), @JsonComponent beans, aur various Jackson modules. Ye slice useful hai verify karne ke liye ki domain objects correctly serialize hote hain JSON mein appropriate field names, date formats, aur custom serializers ke saath, ya correctly deserialize hote hain JSON se including unknown properties handle karna ya validations. Ye web layer ya data layer load nahi karta, making these tests extremely fast jabki ensuring API contract compliance."
            },
            "code": {
              "title": "JSON Serialization Testing",
              "language": "java",
              "content": "@JsonTest\npublic class OrderJsonTest {\n    \n    @Autowired\n    private JacksonTester<Order> json;\n    \n    @Test\n    void shouldSerializeOrder() throws IOException {\n        Order order = new Order(1L, \"PRODUCT-ABC\", \n                               LocalDateTime.of(2024, 1, 15, 10, 0));\n        \n        assertThat(json.write(order)).isStrictlyEqualToJson(\"expected.json\");\n        assertThat(json.write(order)).hasJsonPathStringValue(\"@.productId\", \"PRODUCT-ABC\");\n        assertThat(json.write(order)).extractingJsonPathNumberValue(\"@.id\").isEqualTo(1);\n    }\n    \n    @Test\n    void shouldDeserializeOrder() throws IOException {\n        String content = \"{\\\"id\\\":2,\\\"productId\\\":\\\"PRODUCT-XYZ\\\",\\\"createdAt\\\":\\\"2024-01-15T10:00:00\\\"}\";\n        \n        assertThat(json.parse(content))\n            .usingRecursiveComparison()\n            .isEqualTo(new Order(2L, \"PRODUCT-XYZ\", \n                                LocalDateTime.of(2024, 1, 15, 10, 0)));\n    }\n    \n    @Test\n    void shouldIgnoreUnknownProperties() throws IOException {\n        String content = \"{\\\"id\\\":3,\\\"productId\\\":\\\"P\\\",\\\"unknownField\\\":\\\"value\\\"}\";\n        assertThat(json.parse(content).getObject().getId()).isEqualTo(3);\n    }\n}"
            },
            "codeExplanations": {
              "english": "JacksonTester provide karta hai fluent assertions JSON content ke liye. isStrictlyEqualToJson compare karta hai test resources mein file ke against. hasJsonPathStringValue verify karta hai specific JSON path values. Test confirm karta hai proper serialization of Java 8 date/time, unknown properties handle karna (via @JsonIgnoreProperties(ignoreUnknown=true)), aur deserialization integrity bina web ya database contexts load kiye."
            },
            "keyPoints": [
              "Test slice for ObjectMapper and JSON mapping",
              "Auto-configures Jackson and custom serializers",
              "Does not load web or data layers (very fast)",
              "JacksonTester provides JSON-specific assertions",
              "Verifies API contract compliance for payloads"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "test-4",
            "title": "@SpringBootTest",
            "explanations": {
              "english": "@SpringBootTest complete ApplicationContext load karta hai full integration testing ke liye, allowing tests ko entire stack exercise karne from controllers through services to repositories. Ye embedded servlet container start kar sakta hai random ya defined ports pe using webEnvironment options. Ye @Value properties resolve karta hai, configuration files load karta hai, aur auto-configuration execute karta hai jaise production mein hota hai. While comprehensive, ye tests slow hote hain full context initialization ki wajah se, isliye inhe sparingly use karna chahiye critical end-to-end flows ke liye jabki favoring test slices isolated component testing ke liye."
            },
            "code": {
              "title": "Integration Testing",
              "language": "java",
              "content": "@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT)\n@Testcontainers\npublic class OrderIntegrationTest {\n    \n    @LocalServerPort\n    private int port;\n    \n    @Container\n    static PostgreSQLContainer<?> postgres = new PostgreSQLContainer<>(\"postgres:15\")\n        .withDatabaseName(\"testdb\")\n        .withUsername(\"test\")\n        .withPassword(\"test\");\n    \n    @DynamicPropertySource\n    static void configureProperties(DynamicPropertyRegistry registry) {\n        registry.add(\"spring.datasource.url\", postgres::getJdbcUrl);\n        registry.add(\"spring.datasource.username\", postgres::getUsername);\n        registry.add(\"spring.datasource.password\", postgres::getPassword);\n    }\n    \n    @Autowired\n    private TestRestTemplate restTemplate;\n    \n    @Autowired\n    private OrderRepository repository;\n    \n    @Test\n    void fullOrderLifecycle() {\n        OrderRequest request = new OrderRequest(\"ITEM-001\", 5);\n        \n        ResponseEntity<Order> response = restTemplate.postForEntity(\n            \"http://localhost:\" + port + \"/api/orders\", \n            request, Order.class);\n        \n        assertThat(response.getStatusCode()).isEqualTo(HttpStatus.CREATED);\n        assertThat(repository.findById(response.getBody().getId())).isPresent();\n    }\n}"
            },
            "codeExplanations": {
              "english": "@SpringBootTest with RANDOM_PORT full application context start karta hai embedded Tomcat ke saath random port pe. @Testcontainers provide karta hai PostgreSQL via Docker. DynamicPropertySource override karta hai datasource properties container-specific connection details point karne ke liye. TestRestTemplate actual HTTP calls make karta hai running server ko, complete stack test karta hai including serialization, validation, persistence, aur HTTP status handling."
            },
            "keyPoints": [
              "Loads complete ApplicationContext including all beans",
              "Starts embedded server with webEnvironment options (MOCK, RANDOM_PORT, DEFINED_PORT)",
              "@LocalServerPort injects the actual port used",
              "Slowest test type due to full context startup; use sparingly",
              "Ideal for end-to-end smoke tests and critical path validation"
            ],
            "extras": {
              "flowDiagram": "Test -> @SpringBootTest -> Start Full Context + Embedded Server -> HTTP Request -> Full Stack Processing -> Assert",
              "comparisonTable": [
                {
                  "headers": [
                    "Annotation",
                    "Scope",
                    "Speed",
                    "Server"
                  ],
                  "rows": [
                    [
                      "@WebMvcTest",
                      "Controller layer only",
                      "Fast",
                      "MockMvc (no server)"
                    ],
                    [
                      "@DataJpaTest",
                      "Repository layer only",
                      "Fast",
                      "None"
                    ],
                    [
                      "@SpringBootTest",
                      "Full context",
                      "Slow",
                      "Embedded Tomcat"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "test-5",
            "title": "@MockBean",
            "explanations": {
              "english": "@MockBean replace karta hai existing bean ko Spring ApplicationContext mein Mockito mock ke saath. Ye primarily use hota hai @SpringBootTest aur test slices mein jab class under test ko uski dependencies se isolate karna ho. Mock context mein inject hota hai aur configure kiya ja sakta hai standard Mockito methods jaise when() aur verify() use karke. Har test ke baad mock automatically reset ho jata hai. Ye essential hai service layers test karne ke liye bina actually external APIs ya databases call kiye, ya error conditions simulate karne ke liye jo real implementations se reproduce karna mushkil hota hai."
            },
            "code": {
              "title": "Mocking Dependencies",
              "language": "java",
              "content": "@SpringBootTest\npublic class OrderServiceIntegrationTest {\n    \n    @Autowired\n    private OrderService orderService;\n    \n    @MockBean\n    private PaymentGatewayClient paymentGateway;\n    \n    @MockBean\n    private NotificationService notificationService;\n    \n    @Test\n    void shouldProcessOrderWhenPaymentSucceeds() {\n        when(paymentGateway.charge(any(), any()))\n            .thenReturn(new PaymentResult(true, \"TX-123\"));\n        \n        Order order = orderService.processOrder(new OrderRequest(\"ITEM\", 100.0));\n        \n        assertThat(order.getStatus()).isEqualTo(OrderStatus.COMPLETED);\n        verify(paymentGateway).charge(eq(\"ITEM\"), eq(BigDecimal.valueOf(100.0)));\n        verify(notificationService).sendOrderConfirmation(order);\n    }\n    \n    @Test\n    void shouldRollbackWhenPaymentFails() {\n        when(paymentGateway.charge(any(), any()))\n            .thenThrow(new PaymentException(\"Insufficient funds\"));\n        \n        assertThatThrownBy(() -> orderService.processOrder(new OrderRequest(\"ITEM\", 100.0)))\n            .isInstanceOf(OrderProcessingException.class);\n        \n        verify(notificationService, never()).sendOrderConfirmation(any());\n    }\n}"
            },
            "codeExplanations": {
              "english": "@MockBean create karta hai Mockito mocks jo real beans ko replace karte hain Spring context mein. Test configure karta hai mock behavior happy path aur error scenarios ke liye. verify() ensure karta hai ki expected interactions hue ya nahi. Ye service logic ko external payment systems aur notification services se isolate karta hai jabki verifying karta hai ki integration points correctly call hue."
            },
            "keyPoints": [
              "Replaces bean in ApplicationContext with Mockito mock",
              "Used in @SpringBootTest and test slices",
              "Automatically reset after each test method",
              "Configure with when().thenReturn() or when().thenThrow()",
              "Verify interactions with verify() to test collaboration"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "test-6",
            "title": "Mockito",
            "explanations": {
              "english": "Mockito Java unit tests ke liye standard mocking framework hai. Ye proxy objects create karta hai jo real collaborators simulate karte hain bina actual implementation ki zarurat ke. Annotations mein include hain @Mock dependencies ke mock instances create karne ke liye, @InjectMocks class under test instantiate karne aur usme mocks inject karne ke liye, aur @Spy partial mocking ke liye real objects ka. Stubbing define karta hai behavior when().thenReturn() se, jabki verification check karta hai method invocation counts verify() se. ArgumentMatchers jaise any(), eq(), aur argThat() flexible argument matching provide karte hain. Strict stubbing mode help karta hai identify karne mein unnecessary stubs."
            },
            "code": {
              "title": "Mockito Fundamentals",
              "language": "java",
              "content": "@ExtendWith(MockitoExtension.class)\npublic class InventoryServiceTest {\n    \n    @Mock\n    private InventoryRepository repository;\n    \n    @Mock\n    private EventPublisher eventPublisher;\n    \n    @InjectMocks\n    private InventoryService inventoryService;\n    \n    @Captor\n    private ArgumentCaptor<StockUpdateEvent> eventCaptor;\n    \n    @Test\n    void shouldUpdateStockAndPublishEvent() {\n        // Stubbing\n        when(repository.findByProductId(\"PROD-1\"))\n            .thenReturn(Optional.of(new Inventory(\"PROD-1\", 100)));\n        when(repository.save(any(Inventory.class)))\n            .thenAnswer(inv -> inv.getArgument(0));\n        \n        // Execute\n        Inventory result = inventoryService.decrementStock(\"PROD-1\", 5);\n        \n        // Verification\n        assertThat(result.getQuantity()).isEqualTo(95);\n        verify(repository).save(argThat(inv -> inv.getQuantity() == 95));\n        verify(eventPublisher).publish(eventCaptor.capture());\n        assertThat(eventCaptor.getValue().getProductId()).isEqualTo(\"PROD-1\");\n        \n        verifyNoMoreInteractions(repository);\n    }\n    \n    @Test\n    void shouldThrowWhenInsufficientStock() {\n        when(repository.findByProductId(anyString()))\n            .thenReturn(Optional.of(new Inventory(\"PROD-1\", 3)));\n        \n        assertThrows(InsufficientStockException.class, () -> {\n            inventoryService.decrementStock(\"PROD-1\", 5);\n        });\n    }\n}"
            },
            "codeExplanations": {
              "english": "@ExtendWith(MockitoExtension.class) enable karta hai Mockito bina Spring ke. @InjectMocks create karta hai service aur inject karta hai @Mock fields. when().thenReturn() stub karta hai repository behavior. argThat() verify karta hai arguments match custom predicates. ArgumentCaptor capture karta hai arguments mocks ko pass hue detailed assertion ke liye. verifyNoMoreInteractions ensure karta hai ki unexpected calls nahi hue mock ko."
            },
            "keyPoints": [
              "@Mock creates mock instances of dependencies",
              "@InjectMocks creates the subject under test with mocks injected",
              "when().thenReturn() stubs method behavior",
              "verify() checks method was called with specific arguments",
              "ArgumentCaptor captures arguments for verification"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "Purpose"
                  ],
                  "rows": [
                    [
                      "@Mock",
                      "Create mock object"
                    ],
                    [
                      "@Spy",
                      "Partial mock of real object"
                    ],
                    [
                      "@InjectMocks",
                      "Auto-inject mocks into subject"
                    ],
                    [
                      "when().thenReturn()",
                      "Define stub behavior"
                    ],
                    [
                      "verify().never()",
                      "Verify interaction did not happen"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "test-7",
            "title": "Testcontainers",
            "explanations": {
              "english": "Testcontainers ek Java library hai jo provide karta hai lightweight, throwaway instances of databases, message brokers, web browsers, aur other dependencies running in Docker containers. Ye integration testing enable karta hai real infrastructure ke against rather than mocks ya in-memory databases, catching issues jaise SQL dialect differences aur networking problems. Containers @Container annotation se define hote hain aur tests se pehle start hote hain. @DynamicPropertySource inject karta hai connection details (jaise dynamic ports) into Spring properties. Ye ensure karta hai test environments match production closely jabki remaining isolated aur reproducible across different development machines aur CI/CD pipelines."
            },
            "code": {
              "title": "Containerized Integration Tests",
              "language": "java",
              "content": "@SpringBootTest\n@Testcontainers\npublic class MessagingIntegrationTest {\n    \n    @Container\n    static KafkaContainer kafka = new KafkaContainer(\n        DockerImageName.parse(\"confluentinc/cp-kafka:7.5.0\"));\n    \n    @Container\n    static GenericContainer<?> redis = new GenericContainer<>(\"redis:7-alpine\")\n        .withExposedPorts(6379);\n    \n    @DynamicPropertySource\n    static void properties(DynamicPropertyRegistry registry) {\n        registry.add(\"spring.kafka.bootstrap-servers\", kafka::getBootstrapServers);\n        registry.add(\"spring.data.redis.host\", redis::getHost);\n        registry.add(\"spring.data.redis.port\", redis::getFirstMappedPort);\n    }\n    \n    @Autowired\n    private KafkaTemplate<String, OrderEvent> kafkaTemplate;\n    \n    @Autowired\n    private OrderRepository repository;\n    \n    @Test\n    void shouldProcessOrderEventAndCacheResult() throws Exception {\n        OrderEvent event = new OrderEvent(\"ORDER-1\", \"PRODUCT-A\", 5);\n        \n        kafkaTemplate.send(\"orders\", event).get();\n        \n        // Wait for async processing\n        await().atMost(Duration.ofSeconds(5)).untilAsserted(() -> {\n            assertThat(repository.findById(\"ORDER-1\")).isPresent();\n        });\n    }\n}"
            },
            "codeExplanations": {
              "english": "@Testcontainers enable karta hai Testcontainers extension. @Container define karta hai Docker containers jo tests se pehle start hote hain aur baad mein stop hote hain. KafkaContainer provide karta hai convenient methods bootstrap servers ke liye. GenericContainer allow karta hai any Docker image. DynamicPropertySource dynamically override karta hai Spring properties container-specific connection details ke saath (random ports mapped). Ye actual integration test karta hai Kafka aur Redis ke saath bina mocks ke."
            },
            "keyPoints": [
              "Provides real dependencies in Docker containers for tests",
              "Matches production environment (same versions, real network)",
              "@Container manages container lifecycle (start before, stop after)",
              "@DynamicPropertySource injects connection strings into Spring Environment",
              "Supports databases (PostgreSQL, MySQL), brokers (Kafka, RabbitMQ), and cloud services"
            ],
            "extras": {
              "flowDiagram": "Test Start -> Start Docker Containers (Kafka, Redis) -> Run DynamicPropertySource -> Start Spring Context (with real connections) -> Execute Tests -> Stop Containers",
              "comparisonTable": [
                {
                  "headers": [
                    "Approach",
                    "Fidelity",
                    "Speed",
                    "Portability"
                  ],
                  "rows": [
                    [
                      "H2 Database",
                      "Low (different SQL)",
                      "Very Fast",
                      "High"
                    ],
                    [
                      "Testcontainers",
                      "High (same as prod)",
                      "Slow (Docker pull)",
                      "Requires Docker"
                    ],
                    [
                      "@MockBean",
                      "Medium (logic only)",
                      "Fast",
                      "High"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          }
        ]
      },
      {
        "id": "microservices-architecture",
        "title": "Microservices Architecture",
        "description": "Spring Boot use karke cloud-native distributed systems build karne ka comprehensive guide, covering service decomposition, inter-service communication patterns, service discovery, API gateways, containerization, aur resilient microservices patterns.",
        "topics": [
          {
            "id": "micro-1",
            "title": "Monolith vs Microservices",
            "explanations": {
              "english": "Monolithic architectures saari functionality ko single unit ke roop mein deploy karte hain, development aur debugging mein simplicity offer karte hain lekin scalability aur technology diversity limit karte hain. Microservices applications ko independently deployable services mein decompose karte hain jo business capabilities ke saath aligned hain, enabling autonomous scaling, polyglot persistence, aur team independence. However, microservices complexity introduce karte hain distributed transactions, service coordination, network latency, aur operational overhead mein. Decision team size, scaling requirements, aur organizational maturity pe depend karta hai. Kai organizations monoliths se start karte hain aur incrementally services extract karte hain jab boundaries clear hote hain."
            },
            "code": {
              "title": "Architectural Evolution",
              "language": "java",
              "content": "// Monolithic: All code deployed together\n@Entity\npublic class Order {\n    @OneToMany\n    private List<OrderItem> items;\n    @ManyToOne \n    private Customer customer;  // Same database, same codebase\n}\n\n// Microservices: Separate services, separate databases\n// Order Service\n@Service\npublic class OrderService {\n    private final OrderRepository orderRepo;\n    private final CustomerClient customerClient;  // HTTP/RPC call to Customer Service\n    private final InventoryClient inventoryClient;  // HTTP/RPC call to Inventory Service\n}"
            },
            "codeExplanations": {
              "english": "Monolithic example tight coupling dikhata hai single database ke andar. Microservices example demonstrate karta hai kaise OrderService Customer aur Inventory services ke saath coordinate karta hai network calls ke through rather than direct database joins, enforcing service boundaries aur database-per-service pattern."
            },
            "keyPoints": [
              "Monolith: Single codebase, single database, simple deployment",
              "Microservices: Independent deployment, decentralized data management, bounded contexts",
              "Microservices trade code complexity for operational complexity",
              "Database-per-service pattern prevents tight coupling",
              "Start with monolith, extract microservices when scaling demands"
            ],
            "extras": {
              "flowDiagram": "Monolith: Web+Business+Data Logic -> Single Database\nMicroservices: API Gateway -> [Order Service -> Order DB] + [Customer Service -> Customer DB] + [Inventory Service -> Inventory DB]",
              "comparisonTable": [
                {
                  "headers": [
                    "Characteristic",
                    "Monolith",
                    "Microservices"
                  ],
                  "rows": [
                    [
                      "Deployment",
                      "Single artifact",
                      "Multiple independent pipelines"
                    ],
                    [
                      "Scalability",
                      "Scale entire app",
                      "Scale individual services"
                    ],
                    [
                      "Fault Isolation",
                      "Total failure possible",
                      "Partial degradation"
                    ],
                    [
                      "Data Consistency",
                      "ACID transactions",
                      "Eventual consistency, sagas"
                    ],
                    [
                      "Technology",
                      "Single stack",
                      "Polyglot programming"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-2",
            "title": "Bounded Contexts",
            "explanations": {
              "english": "Bounded Contexts Domain-Driven Design mein central hain, explicit boundaries define karte hain jahan domain model consistent aur applicable hai. Har microservice typically ek bounded context ke saath align hota hai, apna data aur business rules own karta hai. Contexts ke paas explicit integration contracts hain (anti-corruption layers) jo internal models leak hone se prevent karte hain. Ubiquitous Language context ke andar ensure karta hai ki terms ka precise meaning ho (e.g., 'Customer' ka matlab alag hai Billing vs Shipping contexts mein). Clear boundaries coupling reduce karte hain aur teams ko enable karte hain models ko independently evolve karne ke liye bina doosre services break kiye."
            },
            "code": {
              "title": "Context Boundary Implementation",
              "language": "java",
              "content": "// Billing Context: Customer has payment methods\npackage com.billing.domain;\npublic class Customer {\n    private CustomerId id;\n    private List<PaymentMethod> paymentMethods;\n    private BillingAddress address;\n    public void charge(Money amount) { ... }\n}\n\n// Shipping Context: Customer has delivery preferences  \npackage com.shipping.domain;\npublic class Customer {\n    private CustomerId id;\n    private List<ShippingAddress> addresses;\n    private DeliveryPreferences preferences;\n    public void updatePreferences(DeliveryPreferences prefs) { ... }\n}\n\n// Anti-Corruption Layer: Order Service translation\n@Service\npublic class OrderCustomerAdapter {\n    public OrderCustomerDTO fetchCustomerForOrder(CustomerId id) {\n        BillingCustomer billing = billingClient.getCustomer(id);\n        ShippingCustomer shipping = shippingClient.getCustomer(id);\n        \n        // Translate to Order Context's view of Customer\n        return new OrderCustomerDTO(\n            id, \n            billing.getDefaultPaymentMethod(),\n            shipping.getDefaultShippingAddress()\n        );\n    }\n}"
            },
            "codeExplanations": {
              "english": "Different contexts define karte hain Customer ko alag alag unki responsibilities ke basis pe. Anti-Corruption Layer (Adapter) translate karta hai external domain models ko local Order context ke model mein, protecting Order service external model changes se aur maintaining conceptual integrity Order bounded context ke andar."
            },
            "keyPoints": [
              "Each microservice owns a bounded context with its ubiquitous language",
              "Same entity name (Customer) can represent different concepts in different contexts",
              "Anti-corruption layers translate between contexts preventing model leakage",
              "Database schemas are internal implementation details of the context",
              "Context maps define integration relationships between services"
            ],
            "extras": {
              "flowDiagram": "Billing Context (Customer+Payment) <- ACL -> Order Context (CustomerRef+Order) <- ACL -> Shipping Context (Customer+Address)",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-3",
            "title": "Team Autonomy",
            "explanations": {
              "english": "Microservices enable karte hain Conway's Law ko reverse mein: architecture organizational structure drive karti hai. Two-pizza teams (5-9 people) complete services own karte hain development se lekar production tak, eliminating cross-team dependencies deployments ke liye. Autonomous teams apni technology stacks, release schedules, aur scaling policies independently choose kar sakte hain. Ye require karta hai platform engineering support self-service infrastructure, CI/CD pipelines, aur observability tools ke liye. Team autonomy velocity increase karti hai lekin strong API contracts, clear SLOs (Service Level Objectives), aur robust inter-team communication require karti hai integration failures prevent karne ke liye."
            },
            "code": {
              "title": "Contract-Driven Development",
              "language": "java",
              "content": "// Consumer-Driven Contract: Order Service defines expectations\npublic interface CustomerClientContract {\n    @GetMapping(\"/customers/{id}\")\n    CustomerDTO getCustomer(@PathVariable String id);\n}\n\n// Team A (Order) writes contract tests\n@SpringBootTest\n@AutoConfigureStubRunner(ids = \"com.company:customer-service:+:stubs:8081\")\npublic class CustomerContractTest {\n    @Test\n    public void shouldReturnCustomerDetails() {\n        CustomerDTO customer = customerClient.getCustomer(\"123\");\n        assertThat(customer.getId()).isNotNull();\n        assertThat(customer.getPaymentMethod()).isNotNull();\n    }\n}\n\n// Team B (Customer) implements to satisfy contracts\n@RestController\npublic class CustomerController implements CustomerClientContract {\n    @Override\n    public CustomerDTO getCustomer(String id) {\n        return customerService.findById(id);\n    }\n}"
            },
            "codeExplanations": {
              "english": "Consumer-Driven Contracts (CDC) using Spring Cloud Contract autonomous teams enable karte hain. Order team define karta hai expectations contracts mein. StubRunner verify karta hai Order service ko Customer stubs ke against. Customer team implement karta hai in contracts satisfy karne ke liye, ensuring API compatibility bina tight coordination ke."
            },
            "keyPoints": [
              "Two-pizza teams own services end-to-end (you build it, you run it)",
              "Autonomy requires self-service platforms for deployment and monitoring",
              "API contracts (OpenAPI, gRPC proto) enable independent evolution",
              "Consumer-Driven Contracts prevent breaking changes",
              "SLOs define acceptable performance and availability between teams"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-4",
            "title": "REST (Feign)",
            "explanations": {
              "english": "Synchronous REST communication HTTP request-response patterns use karta hai real-time service interaction ke liye. OpenFeign ek declarative HTTP client hai jo inter-service calls simplify karta hai dynamic implementations create karke interfaces ke jo @FeignClient se annotated hain. Ye integrate karta hai Ribbon ke saath client-side load balancing ke liye aur Hystrix/Resilience4j ke saath circuit breaking ke liye. Feign boilerplate reduce karta hai RestTemplate ya WebClient ke comparison mein lekin temporal coupling introduce karta hai; agar target service down hai toh caller fail hota hai. Synchronous calls short hone chahiye, cached where possible, aur resilience patterns mein wrapped hone chahiye cascade failures prevent karne ke liye."
            },
            "code": {
              "title": "Feign Client Implementation",
              "language": "java",
              "content": "@FeignClient(\n    name = \"inventory-service\",\n    configuration = InventoryFeignConfig.class,\n    fallbackFactory = InventoryClientFallbackFactory.class\n)\npublic interface InventoryClient {\n    \n    @GetMapping(\"/api/inventory/{productId}\")\n    InventoryStatus checkStock(@PathVariable String productId);\n    \n    @PostMapping(\"/api/inventory/reserve\")\n    ReservationResult reserveStock(@RequestBody ReservationRequest request);\n}\n\n@Component\npublic class InventoryClientFallbackFactory implements FallbackFactory<InventoryClient> {\n    @Override\n    public InventoryClient create(Throwable cause) {\n        return new InventoryClient() {\n            @Override\n            public InventoryStatus checkStock(String productId) {\n                return InventoryStatus.unknown(); // Graceful degradation\n            }\n            \n            @Override\n            public ReservationResult reserveStock(ReservationRequest request) {\n                throw new InventoryServiceUnavailableException(cause);\n            }\n        };\n    }\n}\n\n// Usage\n@Service\npublic class OrderService {\n    private final InventoryClient inventoryClient;\n    \n    public void processOrder(Order order) {\n        InventoryStatus status = inventoryClient.checkStock(order.getProductId());\n        if (status.isAvailable()) {\n            // Proceed\n        }\n    }\n}"
            },
            "codeExplanations": {
              "english": "@FeignClient declare karta hai ek HTTP client interface. Spring dynamically implementation create karta hai. FallbackFactory provide karta hai fallback behavior jab service unavailable ho, cascade failures prevent karne ke liye. Fallback default values return karta hai queries ke liye aur exceptions throw karta hai commands ke liye, following fail-fast principle writes ke liye."
            },
            "keyPoints": [
              "Feign provides declarative REST clients reducing boilerplate code",
              "Integrates with service discovery (Eureka) for dynamic URL resolution",
              "FallbackFactory enables graceful degradation during service outages",
              "Synchronous calls create temporal coupling; use for queries, avoid long chains",
              "Combine with circuit breakers to prevent cascade failures"
            ],
            "extras": {
              "flowDiagram": "Order Service -> Feign Client -> Load Balancer -> Inventory Service Instance 1\n                                         -> Inventory Service Instance 2",
              "comparisonTable": [
                {
                  "headers": [
                    "Client",
                    "Style",
                    "Async Support",
                    "Verbose"
                  ],
                  "rows": [
                    [
                      "RestTemplate",
                      "Imperative",
                      "No",
                      "High"
                    ],
                    [
                      "WebClient",
                      "Reactive",
                      "Yes",
                      "Medium"
                    ],
                    [
                      "OpenFeign",
                      "Declarative",
                      "No",
                      "Low"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-5",
            "title": "gRPC",
            "explanations": {
              "english": "gRPC ek high-performance RPC framework hai jo Protocol Buffers use karta hai efficient binary serialization ke liye aur HTTP/2 transport ke liye. Ye client aur server stubs generate karta hai .proto definitions se, ensuring contract compliance. REST ke against, gRPC strict service contracts use karta hai strongly typed messages ke saath, integration errors reduce karta hai. Ye bi-directional streaming support karta hai real-time communication ke liye aur multiple calls multiplex karta hai single connection pe. However, gRPC HTTP/2 support require karta hai throughout infrastructure mein (load balancers, proxies) aur kam browser-friendly hai REST ke comparison mein, making it ideal for internal service-to-service communication."
            },
            "code": {
              "title": "gRPC Service Definition",
              "language": "protobuf",
              "content": "// inventory.proto\nsyntax = \"proto3\";\n\nservice InventoryService {\n  rpc CheckStock (StockRequest) returns (StockResponse);\n  rpc StreamStockUpdates (stream StockRequest) returns (stream StockUpdate);\n}\n\nmessage StockRequest {\n  string product_id = 1;\n  int32 quantity = 2;\n}\n\nmessage StockResponse {\n  string product_id = 1;\n  bool available = 2;\n  int32 current_stock = 3;\n}"
            },
            "codeExplanations": {
              "english": "Proto file define karta hai service contract using Protocol Buffers. CheckStock ek unary RPC hai. StreamStockUpdates bi-directional streaming demonstrate karta hai. Field numbers (1, 2, 3) backward compatibility ensure karte hain jab new fields add hote hain. Ye generate karta hai Java classes aur stubs type-safe communication ke liye."
            },
            "keyPoints": [
              "Protocol Buffers provide efficient binary serialization (smaller than JSON)",
              "HTTP/2 enables multiplexing, header compression, and bi-directional streaming",
              "Strong typing via generated stubs prevents contract violations",
              "Requires HTTP/2 support in network infrastructure",
              "Ideal for high-performance internal service communication"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "REST/HTTP",
                    "gRPC"
                  ],
                  "rows": [
                    [
                      "Protocol",
                      "HTTP/1.1 or HTTP/2",
                      "HTTP/2"
                    ],
                    [
                      "Serialization",
                      "JSON/XML (text)",
                      "Protocol Buffers (binary)"
                    ],
                    [
                      "Streaming",
                      "Server-Sent Events",
                      "Native bi-directional"
                    ],
                    [
                      "Strong Typing",
                      "Weak (OpenAPI)",
                      "Strong (generated stubs)"
                    ],
                    [
                      "Browser Support",
                      "Excellent",
                      "Requires gRPC-Web proxy"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-6",
            "title": "RabbitMQ",
            "explanations": {
              "english": "RabbitMQ ek message broker hai jo AMQP (Advanced Message Queuing Protocol) implement karta hai reliable asynchronous messaging ke liye. Ye various exchange types support karta hai: direct (point-to-point), topic (publish-subscribe with patterns), fanout (broadcast), aur headers. Messages route hote hain exchanges se queues mein, producers ko consumers se decouple karta hai. RabbitMQ provide karta hai message durability (disk pe persist), acknowledgments (delivery ensure karne ke liye), aur TTL (time-to-live) expiration ke liye. Ye excel karta hai complex routing scenarios aur request-reply patterns mein lekin throughput kam hai Kafka ke comparison mein high-volume streaming ke liye."
            },
            "code": {
              "title": "RabbitMQ Implementation",
              "language": "java",
              "content": "@Configuration\npublic class RabbitConfig {\n    @Bean\n    public TopicExchange orderExchange() {\n        return new TopicExchange(\"orders.exchange\");\n    }\n    \n    @Bean\n    public Queue orderCreatedQueue() {\n        return QueueBuilder.durable(\"orders.created\")\n            .withArgument(\"x-message-ttl\", 60000)\n            .build();\n    }\n    \n    @Bean\n    public Binding binding(Queue orderCreatedQueue, TopicExchange orderExchange) {\n        return BindingBuilder.bind(orderCreatedQueue)\n            .to(orderExchange).with(\"order.created.*\");\n    }\n}\n\n@Service\npublic class OrderEventPublisher {\n    private final RabbitTemplate rabbitTemplate;\n    \n    public void publishOrderCreated(Order order) {\n        OrderCreatedEvent event = new OrderCreatedEvent(order.getId(), order.getAmount());\n        rabbitTemplate.convertAndSend(\"orders.exchange\", \n                                     \"order.created.\" + order.getRegion(), \n                                     event);\n    }\n}\n\n@Component\npublic class InventoryListener {\n    @RabbitListener(queues = \"orders.created\")\n    public void handleOrderCreated(OrderCreatedEvent event) {\n        inventoryService.reserveStock(event.getOrderId(), event.getItems());\n    }\n}"
            },
            "codeExplanations": {
              "english": "TopicExchange route karta hai messages routing key patterns ke basis pe. Durable queue broker restarts survive karti hai 60-second TTL ke saath messages ke liye. Binding connect karta hai queue ko exchange ke saath wildcard pattern ke saath (order.created.*). RabbitTemplate send karta hai messages, jabki @RabbitListener container create karta hai messages consume karne ke liye asynchronously, enabling Inventory service order events pe react karne ke liye bina direct HTTP calls ke."
            },
            "keyPoints": [
              "AMQP protocol with flexible routing (direct, topic, fanout exchanges)",
              "Decouples producers from consumers via queues",
              "Supports message durability, acknowledgments, and TTL",
              "Complex routing logic with exchange-to-exchange bindings",
              "Lower throughput than Kafka but richer messaging semantics"
            ],
            "extras": {
              "flowDiagram": "Order Service -> Exchange (topic) -> Queue (orders.created) -> Inventory Service\n                                    -> Queue (orders.created) -> Shipping Service",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-7",
            "title": "Kafka",
            "explanations": {
              "english": "Apache Kafka ek distributed event streaming platform hai designed for high-throughput, fault-tolerant message processing. Traditional message brokers ke against, Kafka messages ko disk pe persist karta hai configurable retention ke saath, allowing consumers ko events replay karne. Ye ek partitioned log structure use karta hai jahan topics multiple brokers mein split hote hain horizontal scalability ke liye. Producers write karte hain topic partitions mein; consumers read karte hain partitions se consumer groups ke andar, enabling parallel processing. Kafka excel karta hai event sourcing, log aggregation, aur stream processing mein lekin careful partition strategy design require karta hai ordering ensure karne aur hotspots avoid karne ke liye."
            },
            "code": {
              "title": "Kafka Configuration",
              "language": "java",
              "content": "@Configuration\npublic class KafkaConfig {\n    @Bean\n    public ProducerFactory<String, OrderEvent> producerFactory() {\n        Map<String, Object> config = new HashMap<>();\n        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"kafka:9092\");\n        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);\n        config.put(ProducerConfig.ACKS_CONFIG, \"all\"); // Wait for all replicas\n        config.put(ProducerConfig.RETRIES_CONFIG, 3);\n        return new DefaultKafkaProducerFactory<>(config);\n    }\n    \n    @Bean\n    public KafkaTemplate<String, OrderEvent> kafkaTemplate() {\n        return new KafkaTemplate<>(producerFactory());\n    }\n}\n\n@Service\npublic class OrderEventProducer {\n    private final KafkaTemplate<String, OrderEvent> kafkaTemplate;\n    \n    public void sendOrderEvent(OrderEvent event) {\n        // Key ensures same orderId goes to same partition (ordering guarantee)\n        kafkaTemplate.send(\"orders\", event.getOrderId(), event)\n            .whenComplete((result, ex) -> {\n                if (ex != null) {\n                    log.error(\"Failed to send event\", ex);\n                }\n            });\n    }\n}\n\n@Component\npublic class OrderConsumer {\n    @KafkaListener(topics = \"orders\", groupId = \"inventory-service\")\n    public void handleOrder(@Payload OrderEvent event, \n                           @Header(KafkaHeaders.RECEIVED_PARTITION) int partition) {\n        log.info(\"Processing order {} from partition {}\", event.getOrderId(), partition);\n        processEvent(event);\n    }\n}"
            },
            "codeExplanations": {
              "english": "ProducerFactory configure karta hai serializers aur durability (acks=all). OrderId ko message key ke roop mein use karna ensure karta hai ki saare events same order ke liye same partition mein jaate hain, event order maintain karta hai. @KafkaListener consumer group use karta hai load balancing ke liye; multiple instances partitions share karte hain. Consumer groups allow horizontal scaling jabki ensure karta hai ki har message consumer group mein ek consumer dwara process ho."
            },
            "keyPoints": [
              "Distributed commit log with high throughput and durability",
              "Partitions enable parallelism; keys ensure ordering within partitions",
              "Consumer groups provide load balancing and fault tolerance",
              "Messages retained based on time/size policy (not deleted after consumption)",
              "Ideal for event sourcing, metrics, and log aggregation"
            ],
            "extras": {
              "flowDiagram": "Producer -> Topic (orders: Partition 0, Partition 1, Partition 2) -> Consumer Group [Instance A (Partition 0,1), Instance B (Partition 2)]",
              "comparisonTable": [
                {
                  "headers": [
                    "Feature",
                    "RabbitMQ",
                    "Kafka"
                  ],
                  "rows": [
                    [
                      "Model",
                      "Traditional message queue",
                      "Distributed log"
                    ],
                    [
                      "Ordering",
                      "Per queue",
                      "Per partition (with key)"
                    ],
                    [
                      "Retention",
                      "Delete after ack",
                      "Configurable retention"
                    ],
                    [
                      "Throughput",
                      "Moderate",
                      "Very high"
                    ],
                    [
                      "Replay",
                      "No",
                      "Yes (seek to offset)"
                    ],
                    [
                      "Routing",
                      "Complex (exchanges)",
                      "Simple (topics)"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-8",
            "title": "Spring Cloud Gateway Routing",
            "explanations": {
              "english": "Spring Cloud Gateway ek API Gateway provide karta hai built on Spring 5, WebFlux, aur Project Reactor, handling cross-cutting concerns jaise routing, security, aur rate limiting. Ye route karta hai requests downstream services ko using predicates (path, header, query parameters) aur filters (modifying requests/responses). Routes configure kiye ja sakte hain via Java DSL ya YAML. Ye integrate karta hai service discovery (Eureka) ke saath dynamic routing ke liye using lb://service-id syntax. As a reactive gateway, ye high concurrency efficiently handle karta hai lekin careful handling require karta hai blocking operations ka filters mein."
            },
            "code": {
              "title": "Gateway Configuration",
              "language": "java",
              "content": "@Configuration\npublic class GatewayConfig {\n    \n    @Bean\n    public RouteLocator customRouteLocator(RouteLocatorBuilder builder) {\n        return builder.routes()\n            .route(\"order-service\", r -> r\n                .path(\"/api/orders/**\")\n                .and().method(\"GET\", \"POST\")\n                .filters(f -> f\n                    .stripPrefix(2)\n                    .addRequestHeader(\"X-Gateway-Source\", \"api-gateway\")\n                    .circuitBreaker(config -> config\n                        .setName(\"orderServiceCircuitBreaker\")\n                        .setFallbackUri(\"forward:/fallback/orders\"))\n                )\n                .uri(\"lb://order-service\"))\n            .route(\"static\", r -> r\n                .path(\"/ui/**\")\n                .uri(\"https://static-content.example.com \"))\n            .build();\n    }\n}\n\n// application.yml alternative:\n// spring:\n//   cloud:\n//     gateway:\n//       routes:\n//       - id: order-service\n//         uri: lb://order-service\n//         predicates:\n//         - Path=/api/orders/**\n//         - Method=GET,POST\n//         filters:\n//         - StripPrefix=2"
            },
            "codeExplanations": {
              "english": "RouteLocator programmatically routes define karta hai. Order-service route match karta hai /api/orders/**, first 2 path segments strip karta hai (/api), header add karta hai, aur load balancing (lb://) use karta hai requests distribute karne ke liye order-service instances mein. CircuitBreaker filter provide karta hai fallback during outages. Static route forward karta hai external URLs pe frontend assets ke liye."
            },
            "keyPoints": [
              "Entry point for microservices handling cross-cutting concerns",
              "Routes based on predicates (path, method, header) to downstream services",
              "Load balancing integration with Eureka (lb://service-id)",
              "Filters modify requests (StripPrefix, AddHeader) and responses",
              "Built on WebFlux for reactive, non-blocking I/O"
            ],
            "extras": {
              "flowDiagram": "Client -> Gateway:8080 -> [Predicate Match] -> Filter Chain -> lb://order-service\n                                    -> [Predicate Match] -> Filter Chain -> lb://inventory-service",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-9",
            "title": "Rate Limiting",
            "explanations": {
              "english": "Rate limiting API Gateways mein traffic volume control karta hai taaki backend services excessive requests se overwhelm na ho. Algorithms mein include hain token bucket (smooth rate limiting), leaky bucket (smooth output), fixed window, aur sliding window. Spring Cloud Gateway Redis RateLimiter store karta hai burst capacity Redis mein distributed rate limiting ke liye across multiple gateway instances. Limits define kiye ja sakte hain per user (JWT claims ke basis pe), per IP, ya globally. Jab limits exceeded hote hain, gateway return karta hai HTTP 429 (Too Many Requests) optional Retry-After headers ke saath clients ko backoff periods inform karne ke liye."
            },
            "code": {
              "title": "Rate Limiting Configuration",
              "language": "java",
              "content": "@Bean\npublic RateLimiterGatewayFilterFactory filterFactory(RateLimiter rateLimiter) {\n    return new RateLimiterGatewayFilterFactory(rateLimiter);\n}\n\n// application.yml configuration:\n// spring:\n//   cloud:\n//     gateway:\n//       routes:\n//       - id: order-service\n//         uri: lb://order-service\n//         predicates:\n//         - Path=/api/orders/**\n//         filters:\n//         - name: RequestRateLimiter\n//           args:\n//             redis-rate-limiter.replenishRate: 10   # requests per second\n//             redis-rate-limiter.burstCapacity: 20   # max burst\n//             key-resolver: \"#{@userKeyResolver}\"\n\n@Bean\npublic KeyResolver userKeyResolver() {\n    // Rate limit by user (extracted from JWT or Principal)\n    return exchange -> Mono.just(\n        exchange.getPrincipal()\n            .cast(Authentication.class)\n            .map(auth -> auth.getName())\n            .defaultIfEmpty(\"anonymous\")\n    );\n}\n\n@Bean\npublic KeyResolver ipKeyResolver() {\n    // Rate limit by IP address\n    return exchange -> Mono.just(\n        exchange.getRequest().getRemoteAddress().getAddress().getHostAddress()\n    );\n}"
            },
            "codeExplanations": {
              "english": "RequestRateLimiter filter Redis use karta hai request rates track karne ke liye. replenishRate set karta hai steady-state throughput (10/sec), burstCapacity allow karta hai short spikes up to 20. KeyResolver determine karta hai rate limit bucket; userKeyResolver extract karta hai username security context se per-user quotas ke liye, jabki ipKeyResolver limit karta hai client IP pe. Limits exceed hone pe return karta hai 429 Too Many Requests."
            },
            "keyPoints": [
              "Prevents cascade failures by throttling excessive requests",
              "Token bucket algorithm allows bursts while limiting sustained rate",
              "Redis backend enables distributed rate limiting across gateway replicas",
              "KeyResolver strategies: per user, per IP, or global",
              "Returns HTTP 429 with optional Retry-After header for client backoff"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [
                {
                  "headers": [
                    "Algorithm",
                    "Behavior",
                    "Burst Handling"
                  ],
                  "rows": [
                    [
                      "Token Bucket",
                      "Smooth with bursts",
                      "Allows bursts up to bucket size"
                    ],
                    [
                      "Leaky Bucket",
                      "Smooth output",
                      "Queues or drops excess"
                    ],
                    [
                      "Fixed Window",
                      "Resets at interval",
                      "Allows 2x burst at window edges"
                    ],
                    [
                      "Sliding Window",
                      "Precise",
                      "Smooth, no burst at edges"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-10",
            "title": "Gateway Authentication",
            "explanations": {
              "english": "Centralizing authentication API Gateway mein eliminate karta hai individual services ko JWT validation, certificate management, aur OAuth2 flows handle karne ki zarurat. Gateway validate karta hai tokens incoming requests se aur pass karta hai user context (user ID, roles) downstream services ko via headers (X-User-Id, X-Roles). Services trust karte hain headers gateway se (mTLS ya network policies spoofing prevent karte hain). Ye pattern simplify karta hai service code aur provide karta hai single-point security policy enforcement. Gateway handle kar sakta hai cookie-to-token translation browser clients ke liye aur refresh token rotation."
            },
            "code": {
              "title": "Gateway Security Filter",
              "language": "java",
              "content": "@Component\npublic class AuthenticationFilter extends AbstractGatewayFilterFactory<Config> {\n    \n    @Override\n    public GatewayFilter apply(Config config) {\n        return (exchange, chain) -> {\n            String token = extractToken(exchange.getRequest());\n            \n            return validateToken(token)\n                .flatMap(jwt -> {\n                    // Add user context to headers for downstream services\n                    ServerHttpRequest mutatedRequest = exchange.getRequest()\n                        .mutate()\n                        .header(\"X-User-Id\", jwt.getSubject())\n                        .header(\"X-User-Roles\", String.join(\",\", jwt.getRoles()))\n                        .header(\"X-Trace-Id\", UUID.randomUUID().toString())\n                        .build();\n                    \n                    return chain.filter(exchange.mutate()\n                        .request(mutatedRequest)\n                        .build());\n                })\n                .switchIfEmpty(Mono.error(new ResponseStatusException(HttpStatus.UNAUTHORIZED)));\n        };\n    }\n    \n    private Mono<Jwt> validateToken(String token) {\n        // JWT validation logic\n        return Mono.justOrEmpty(token)\n            .map(this::parseAndValidate);\n    }\n}"
            },
            "codeExplanations": {
              "english": "Global filter intercept karta hai saare requests. Ye extract karta hai JWT Authorization header se, validate karta hai signature aur expiration, phir inject karta hai user context as new headers request mein before forwarding downstream services ko. Downstream services ye trusted headers read karte hain instead of JWT parse karna. Agar validation fail hota hai, toh immediately 401 return karta hai bina backend services ko burden kiye."
            },
            "keyPoints": [
              "Centralize JWT/OAuth2 validation in the gateway",
              "Pass user context via headers to downstream services (X-User-Id)",
              "Services trust headers from gateway (secured by mTLS or VPC)",
              "Eliminates duplicate security code in microservices",
              "Simplifies token refresh and cookie handling for browsers"
            ],
            "extras": {
              "flowDiagram": "Client -> [JWT] -> Gateway (Validate) -> [X-User-Id:123, X-Roles:USER] -> Order Service (Trust Headers)",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-11",
            "title": "Eureka Server",
            "explanations": {
              "english": "Eureka Server ek service registry hai providing a REST API service discovery ke liye Netflix OSS architecture mein. Services register karte hain apne aap ko startup pe apne host, port, aur health status ke saath. Server maintain karta hai registry available instances ki aur provide karta hai lookup capabilities clients ko. Ye handle karta hai service deregistration graceful shutdown pe aur expiration instances ki jo heartbeats send karna fail ho jaate hain. Eureka Server replicate karta hai registry data peer nodes ke saath high availability ke liye cluster mode mein. Ye foundation hai client-side service discovery ka jahan clients query karte hain Eureka ko service instances locate karne ke liye before requests karna."
            },
            "code": {
              "title": "Eureka Server Setup",
              "language": "java",
              "content": "@SpringBootApplication\n@EnableEurekaServer\npublic class DiscoveryServerApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(DiscoveryServerApplication.class, args);\n    }\n}\n\n// application.yml:\n// server:\n//   port: 8761\n// eureka:\n//   instance:\n//     hostname: localhost\n//   client:\n//     registerWithEureka: false  # Self-preservation: don't register itself\n//     fetchRegistry: false       # Don't fetch registry as client\n//   server:\n//     enableSelfPreservation: false  # For dev; disabled auto-protection in prod\n\n// Cluster configuration (peer awareness):\n// ---\n// spring:\n//   profiles: peer1\n// eureka:\n//   instance:\n//     hostname: peer1\n//   client:\n//     serviceUrl:\n//       defaultZone: http://peer2:8762/eureka/,http://peer3:8763/eureka/"
            },
            "codeExplanations": {
              "english": "@EnableEurekaServer activate karta hai registry. Standalone mode mein, ye disable karta hai self-registration. Dashboard available hai http://localhost:8761 pe showing registered services aur unka health. Production ke liye, multiple Eureka instances cluster form karte hain peer-to-peer replication ke saath; har peer register karta hai doosron ke saath defaultZone mein registry consistency aur availability ensure karne ke liye."
            },
            "keyPoints": [
              "Service registry for client-side discovery pattern",
              "Services register on startup and send heartbeats every 30 seconds",
              "Self-preservation mode prevents mass deregistration during network partitions",
              "Peer-to-peer replication for high availability clusters",
              "REST API allows querying instances by service ID"
            ],
            "extras": {
              "flowDiagram": "Order Service -> Register -> Eureka Server <- Register <- Inventory Service\n      \\                            /\n       \\                          /\n        Query -> Available Instances",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-12",
            "title": "Eureka Client",
            "explanations": {
              "english": "Eureka Client enable karta hai applications ko Eureka Server ke saath register karne ke liye aur doosri services discover karne ke liye. Startup pe, client register karta hai apna metadata (instance ID, host, port, health check URL) aur send karta hai periodic heartbeats lease renew karne ke liye. Ye locally registry cache karta hai resilience ke liye, periodically refresh karta hai server se. Jab doosri service ko call karta hai, client query karta hai Eureka available instances ke liye aur use karta hai Ribbon ya Spring Cloud LoadBalancer ek select karne ke liye. Registration include karta hai metadata jaise zone information affinity-based routing ke liye aur virtualization awareness."
            },
            "code": {
              "title": "Eureka Client Configuration",
              "language": "java",
              "content": "@SpringBootApplication\n@EnableDiscoveryClient\npublic class OrderServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(OrderServiceApplication.class, args);\n    }\n}\n\n// application.yml:\n// spring:\n//   application:\n//     name: order-service  # Service ID used for discovery\n// eureka:\n//   client:\n//     serviceUrl:\n//       defaultZone: http://eureka1:8761/eureka/,http://eureka2:8762/eureka/\n//     healthcheck:\n//       enabled: true  # Use Spring Boot health endpoint\n//   instance:\n//     preferIpAddress: true  # Register IP instead of hostname\n//     metadataMap:\n//       zone: zone1\n//       profile: production\n\n// Discovering services programmatically:\n@Autowired\nprivate DiscoveryClient discoveryClient;\n\npublic void callInventoryService() {\n    List<ServiceInstance> instances = discoveryClient\n        .getInstances(\"inventory-service\");\n    \n    if (!instances.isEmpty()) {\n        ServiceInstance instance = instances.get(0);\n        String url = instance.getUri() + \"/api/inventory\";\n        // Make request using RestTemplate/WebClient\n    }\n}"
            },
            "codeExplanations": {
              "english": "@EnableDiscoveryClient activate karta hai Eureka registration. Application register karta hai 'order-service' name ke saath. Multiple Eureka servers defaultZone mein failover provide karte hain. preferIpAddress ensure karta hai ki IP addresses use hote hain registration mein (sensible containerized environments mein). DiscoveryClient provide karta hai programmatic lookup instances ka, although typically Ribbon/Feign ye automatically handle karte hain load-balanced RestTemplates ke through."
            },
            "keyPoints": [
              "Registers service metadata and health endpoints with Eureka",
              "Sends heartbeats every 30s (lease renewal)",
              "Caches registry locally, refreshing every 30s",
              "Uses service ID (spring.application.name) for lookups",
              "Supports zone-aware registration for latency-based routing"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-13",
            "title": "Client-side Load Balancing",
            "explanations": {
              "english": "Client-side load balancing selection logic consumer ke andar rakhta hai rather than external load balancers mein. Spring Cloud LoadBalancer (deprecated Ribbon ko replace karke) query karta hai Eureka available service instances ke liye aur select karta hai ek algorithm use karke (RoundRobin, Random, ya custom). Ye eliminate karta hai network hop central load balancer ko, latency reduce karta hai aur single points of failure kam karta hai. Load balancer integrate karta hai RestTemplate, WebClient, aur Feign ke saath @LoadBalanced annotations ke through. Ye health status respect karta hai aur implement kar sakta hai zone affinity same availability zone mein instances prefer karne ke liye."
            },
            "code": {
              "title": "Load Balancer Configuration",
              "language": "java",
              "content": "@Configuration\npublic class LoadBalancerConfig {\n    \n    @Bean\n    @LoadBalanced  // Enables client-side load balancing\n    public RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n    \n    @Bean\n    @LoadBalanced\n    public WebClient.Builder loadBalancedWebClientBuilder() {\n        return WebClient.builder();\n    }\n}\n\n@Service\npublic class OrderService {\n    @Autowired\n    private RestTemplate restTemplate;\n    \n    public InventoryStatus checkInventory(String productId) {\n        // URL uses service ID instead of hostname\n        // Load balancer selects instance from Eureka\n        return restTemplate.getForObject(\n            \"http://inventory-service/api/inventory/{id}\",\n            InventoryStatus.class,\n            productId\n        );\n    }\n}\n\n// Custom load balancing rule (e.g., zone affinity)\npublic class ZoneAffinityRule implements ReactorServiceInstanceLoadBalancer {\n    @Override\n    public Mono<Response<ServiceInstance>> choose(Request request) {\n        // Prefer instances in same zone, fallback to others\n        String myZone = getCurrentZone();\n        return serviceInstanceSupplier.get()\n            .map(instances -> instances.stream()\n                .filter(i -> myZone.equals(i.getMetadata().get(\"zone\")))\n                .findFirst()\n                .orElse(instances.get(ThreadLocalRandom.current().nextInt(instances.size()))));\n    }\n}"
            },
            "codeExplanations": {
              "english": "@LoadBalanced interceptor replace karta hai 'http://inventory-service' Eureka se actual host:port ke saath load balancing algorithm use karke. RestTemplate simple rehta hai logical service names use karke. ZoneAffinityRule demonstrate karta hai custom selection logic same-zone instances prefer karne ke liye lower latency ke liye, random selection pe fallback agar local zone instances unavailable ho."
            },
            "keyPoints": [
              "Client chooses instance from Eureka registry instead of central LB",
              "Eliminates network hop and single point of failure",
              "@LoadBalanced annotation enables for RestTemplate/WebClient",
              "Default RoundRobin, supports custom selection algorithms",
              "Zone affinity reduces cross-AZ latency costs"
            ],
            "extras": {
              "flowDiagram": "Order Service -> Eureka (Get instances [A, B, C]) -> Load Balancer (Select B) -> HTTP to Instance B",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "Server-side LB",
                    "Client-side LB"
                  ],
                  "rows": [
                    [
                      "Latency",
                      "Extra hop",
                      "Direct connection"
                    ],
                    [
                      "Failure Mode",
                      "LB is SPOF",
                      "Resilient (local cache)"
                    ],
                    [
                      "Routing Logic",
                      "Centralized",
                      "Distributed per client"
                    ],
                    [
                      "Cost",
                      "Hardware/ALB expense",
                      "Library overhead"
                    ],
                    [
                      "SSL",
                      "Terminate at LB",
                      "End-to-end TLS"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-14",
            "title": "Docker Multi-stage Builds",
            "explanations": {
              "english": "Multi-stage Docker builds optimize karte hain container images build environment ko runtime environment se separate karke. Pehla stage use karta hai full JDK aur build tools (Maven/Gradle) compile aur package karne ke liye application. Final stage copy karta hai sirf built artifact (JAR) minimal JRE ya distroless base image mein, excluding build tools aur source code. Ye significantly reduce karta hai image size, attack surface, aur startup time. Spring Boot 2.3+ support karta hai layered JARs jo dependencies aur application code separate karte hain better Docker layer caching ke liye rebuilds ke dauraan."
            },
            "code": {
              "title": "Multi-stage Dockerfile",
              "language": "dockerfile",
              "content": "# Stage 1: Build\nFROM maven:3.9-eclipse-temurin-17 AS builder\nWORKDIR /build\nCOPY pom.xml .\nCOPY src ./src\nRUN mvn clean package -DskipTests\n\n# Stage 2: Extract layers for caching\nFROM eclipse-temurin:17-jre-alpine AS extractor\nWORKDIR /extracted\nCOPY --from=builder /build/target/*.jar app.jar\nRUN java -Djarmode=layertools -jar app.jar extract\n\n# Stage 3: Runtime\nFROM eclipse-temurin:17-jre-alpine\nWORKDIR /app\n\n# Create non-root user\nRUN addgroup -S spring && adduser -S spring -G spring\nUSER spring:spring\n\n# Copy layers (dependencies change less frequently than code)\nCOPY --from=extracter /extracted/dependencies/ ./\nCOPY --from=extracter /extracted/spring-boot-loader/ ./\nCOPY --from=extracter /extracted/snapshot-dependencies/ ./\nCOPY --from=extracter /extracted/application/ ./\n\nEXPOSE 8080\nENTRYPOINT [\"java\", \"org.springframework.boot.loader.launch.JarLauncher\"]"
            },
            "codeExplanations": {
              "english": "Stage 1 build karta hai application using Maven. Stage 2 extract karta hai Spring Boot layered JAR dependencies ko application code se separate karne ke liye. Stage 3 create karta hai runtime image using Alpine Linux (minimal size) non-root user ke saath security ke liye. Layer ordering ensure karta hai ki dependency changes application layer cache invalidate nahi karte, speeding up rebuilds."
            },
            "keyPoints": [
              "Build in full JDK image, run in minimal JRE image",
              "Reduces final image size from 500MB+ to ~100MB",
              "Removes build tools and source code from production image",
              "Layered JARs optimize Docker cache (dependencies rarely change)",
              "Run as non-root user for security hardening"
            ],
            "extras": {
              "flowDiagram": "Stage 1 (Build): JDK + Maven -> Compile -> JAR\nStage 2 (Extract): JRE -> Explode JAR -> Layers\nStage 3 (Runtime): Minimal JRE + Layers -> Run",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-15",
            "title": "Kubernetes Deployments",
            "explanations": {
              "english": "Kubernetes Deployments manage karte hain stateless application updates declaratively, ensuring desired number of replicas run aur healthy hain. Deployment resource define karta hai container image, resource limits (CPU/memory), environment variables, aur health probes. Rolling updates ensure zero downtime gradually old pods ko new ones se replace karke, honoring maxSurge aur maxUnavailable constraints. Rollbacks revert karte hain previous replica sets ko agar new versions readiness check fail kare. Spring Boot apps integrate karte hain liveness aur readiness probes ke through Actuator endpoints, allowing Kubernetes ko unhealthy instances restart karne aur unready instances ko service rotation se remove karne."
            },
            "code": {
              "title": "Deployment Configuration",
              "language": "yaml",
              "content": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-service\n  labels:\n    app: order-service\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1          # Allow 1 extra pod during update\n      maxUnavailable: 0    # Ensure no downtime\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: app\n        image: registry/order-service:1.2.3\n        ports:\n        - containerPort: 8080\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1024Mi\"\n            cpu: \"1000m\"\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: \"kubernetes,production\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: password\n        livenessProbe:\n          httpGet:\n            path: /actuator/health/liveness\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /actuator/health/readiness\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 5"
            },
            "codeExplanations": {
              "english": "Deployment specify karta hai 3 replicas RollingUpdate strategy ke saath ensuring zero maxUnavailable pods. Resource requests aur limits noisy neighbor issues prevent karte hain. Secrets securely mount karte hain database passwords. Liveness probe (60s initial delay) detect karta hai deadlocks aur restart karta hai pods. Readiness probe (20s delay) determine karta hai kab pod traffic receive kar sakta hai, database connections ready hone ka wait karta hai."
            },
            "keyPoints": [
              "Declarative updates with rollout and rollback capabilities",
              "Rolling updates ensure zero-downtime deployments",
              "Resource requests/limits control scheduling and prevent OOM",
              "Liveness probes restart stuck containers",
              "Readiness probes control traffic flow during startup"
            ],
            "extras": {
              "flowDiagram": "Deployment Controller -> ReplicaSet (v1) -> Pods [Pod1, Pod2, Pod3]\nUpdate triggered -> Create ReplicaSet (v2) -> Scale up v2 pods -> Scale down v1 pods",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-16",
            "title": "Kubernetes Services",
            "explanations": {
              "english": "Kubernetes Services provide karte hain stable networking endpoints pods ke liye, individual containers ke ephemeral nature ko abstract karte hue. ClusterIP (default) expose karta hai service internal IP pe, accessible sirf cluster ke andar inter-service communication ke liye. NodePort expose karta hai port har node ke IP pe external access ke liye. LoadBalancer provision karta hai external load balancer (cloud-provider specific) aur assign karta hai public IP. Headless services (ClusterIP: None) return karte hain pod IPs directly client-side service discovery ke liye. Services use karte hain label selectors traffic route karne ke liye matching pods ko aur support karte hain session affinity agar needed ho."
            },
            "code": {
              "title": "Service Definitions",
              "language": "yaml",
              "content": "# Internal service (ClusterIP)\napiVersion: v1\nkind: Service\nmetadata:\n  name: order-service\nspec:\n  type: ClusterIP\n  selector:\n    app: order-service\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n\n# External access via cloud load balancer\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-gateway-lb\nspec:\n  type: LoadBalancer\n  selector:\n    app: api-gateway\n  ports:\n  - port: 443\n    targetPort: 8080\n  sessionAffinity: None\n\n# Headless service for StatefulSets/client-side discovery\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-headless\nspec:\n  type: ClusterIP\n  clusterIP: None  # Headless\n  selector:\n    app: kafka\n  ports:\n  - port: 9092"
            },
            "codeExplanations": {
              "english": "Order-service ClusterIP expose karta hai port 80 internally, routing karta hai container port 8080 pe. Pods select kiye jaate hain label app=order-service se. LoadBalancer service create karta hai cloud provider load balancer public IP ke saath API Gateway ke liye. Headless Kafka service return karta hai individual pod IPs direct communication ke liye jo Kafka brokers require karte hain, allowing clients to connect specific instances ko."
            },
            "keyPoints": [
              "ClusterIP: Internal cluster access only (default)",
              "NodePort: Exposes port on each node's IP",
              "LoadBalancer: External load balancer with public IP (cloud)",
              "Headless (clusterIP: None): Returns pod IPs for direct access",
              "Label selectors determine which pods receive traffic"
            ],
            "extras": {
              "flowDiagram": "User -> LoadBalancer -> [Gateway Pod1, Gateway Pod2]\nInternal: Order Pod -> ClusterIP -> [Inventory Pod1, Inventory Pod2]",
              "comparisonTable": [
                {
                  "headers": [
                    "Type",
                    "Scope",
                    "Use Case"
                  ],
                  "rows": [
                    [
                      "ClusterIP",
                      "Internal cluster only",
                      "Microservice-to-microservice"
                    ],
                    [
                      "NodePort",
                      "External via <NodeIP>:Port",
                      "Dev/testing access"
                    ],
                    [
                      "LoadBalancer",
                      "External via cloud LB",
                      "Production public APIs"
                    ],
                    [
                      "ExternalName",
                      "DNS alias",
                      "External service reference"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-17",
            "title": "Kubernetes Ingress",
            "explanations": {
              "english": "Ingress manage karta hai external HTTP/HTTPS access ko services ke andar cluster ke, providing Layer 7 routing capabilities. Ye offer karta hai features jaise SSL termination, name-based virtual hosting, path-based routing (/api -> service-a, /ui -> service-b), aur rate limiting. Ingress Controllers (NGINX, Traefik, HAProxy) implement karte hain Ingress resource specifications. TLS certificates manage hote hain via Secrets. Ingress eliminate karta hai multiple LoadBalancer services ki zarurat (expensive cloud providers mein) multiplexing traffic single entry point se. Advanced configurations mein include hain rewrite rules, CORS handling, aur connection timeouts."
            },
            "code": {
              "title": "Ingress Configuration",
              "language": "yaml",
              "content": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - api.example.com\n    secretName: api-tls-secret\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /orders\n        pathType: Prefix\n        backend:\n          service:\n            name: order-service\n            port:\n              number: 80\n      - path: /inventory\n        pathType: Prefix\n        backend:\n          service:\n            name: inventory-service\n            port:\n              number: 80\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: gateway-service\n            port:\n              number: 80"
            },
            "codeExplanations": {
              "english": "Ingress route karta hai traffic URL paths ke basis pe different backend services ko. Annotations configure karte hain NGINX-specific features: SSL redirect, rate limiting, aur URL rewriting. cert-manager annotation automate karta hai TLS certificate issuance Let's Encrypt se. TLS section reference karta hai secret containing certificate aur key. PathType: Prefix match karta hai kisi bhi path ko jo specified string se start hota hai."
            },
            "keyPoints": [
              "Layer 7 HTTP routing (path-based, host-based)",
              "Single entry point for multiple services (saves LoadBalancer costs)",
              "SSL termination at ingress controller",
              "Annotations configure controller-specific features (rate limits, rewrites)",
              "Requires Ingress Controller deployment (NGINX, Traefik)"
            ],
            "extras": {
              "flowDiagram": "Internet -> Ingress Controller -> [api.example.com/orders -> Order Service]\n                                  -> [api.example.com/inventory -> Inventory Service]",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-18",
            "title": "Kubernetes ConfigMaps",
            "explanations": {
              "english": "ConfigMaps decouple karte hain configuration artifacts ko image content se environment-specific settings enable karne ke liye bina containers rebuild kiye. Ye store karte hain non-sensitive configuration key-value pairs ke roop mein ya entire files (application.yml). ConfigMaps inject kiye ja sakte hain pods mein environment variables, command-line arguments, ya mounted volumes ke roop mein. Sensitive data (passwords, tokens) ke liye, use Secrets (base64 encoded, encrypted at rest in etcd). Spring Boot mount kar sakta hai ConfigMaps ko application.properties ke roop mein ya use Spring Cloud Kubernetes library config changes watch karne aur application context refresh karne ke liye bina pods restart kiye."
            },
            "code": {
              "title": "ConfigMap Usage",
              "language": "yaml",
              "content": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: order-service-config\ndata:\n  application-k8s.yml: |\n    server:\n      port: 8080\n    spring:\n      datasource:\n        url: jdbc:postgresql://postgres:5432/orders\n      kafka:\n        bootstrap-servers: kafka:9092\n    management:\n      endpoints:\n        web:\n          exposure:\n            include: health,info,metrics\n  LOG_LEVEL: INFO\n  FEATURE_FLAG_NEW_UI: \"true\"\n\n---\napiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: order-service\n        image: order-service:1.0\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: \"kubernetes\"\n        - name: LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: order-service-config\n              key: LOG_LEVEL\n        volumeMounts:\n        - name: config-volume\n          mountPath: /config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: order-service-config\n          items:\n          - key: application-k8s.yml\n            path: application-k8s.yml"
            },
            "codeExplanations": {
              "english": "ConfigMap contain karta hai complete YAML configuration file aur simple key-value pairs. Deployment consume karta hai isko do tarah se: LOG_LEVEL inject kiya jaata hai environment variable ke roop mein using configMapKeyRef, aur application-k8s.yml mount kiya jaata hai volume ke roop mein /config pe. Spring Boot mounted file read karta hai apni configuration ka part ke roop mein. Ye allow karta hai configuration update karna ConfigMap change karke aur pods restart karke, bina image rebuild kiye."
            },
            "keyPoints": [
              "Decouple configuration from container images",
              "Store non-sensitive data (use Secrets for passwords)",
              "Mount as files or inject as environment variables",
              "Update ConfigMap to change configuration across pods",
              "Spring Cloud Kubernetes enables hot reloading of ConfigMap changes"
            ],
            "extras": {
              "flowDiagram": "ConfigMap -> Mounted Volume -> /config/application.yml -> Spring Boot reads\n         -> Env Var -> JVM System Property",
              "comparisonTable": [
                {
                  "headers": [
                    "Aspect",
                    "ConfigMap",
                    "Secret"
                  ],
                  "rows": [
                    [
                      "Content",
                      "Plain text",
                      "Base64 encoded"
                    ],
                    [
                      "Use Case",
                      "Configuration",
                      "Passwords, tokens, keys"
                    ],
                    [
                      "Size Limit",
                      "1 MB",
                      "1 MB"
                    ],
                    [
                      "Encryption",
                      "No (plain in etcd)",
                      "Yes (at rest in etcd)"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-19",
            "title": "Spring Boot in Microservices",
            "explanations": {
              "english": "Spring Boot foundational framework ka kaam karta hai microservices ke liye, providing embedded servers containerized deployment ke liye, actuator endpoints health checks aur metrics ke liye, aur auto-configuration rapid service creation ke liye. Ye seamlessly integrate karta hai Spring Cloud components ke saath service discovery, configuration management, aur circuit breakers ke liye. Fat JAR packaging containerization simplify karta hai, jabki Spring Boot DevTools local development efficiency enhance karta hai. Iske opinionated defaults boilerplate reduce karte hain, allowing teams ko focus karne business logic pe jabki consistency maintain karte hain dozens of services mein ek microservices ecosystem mein."
            },
            "code": {
              "title": "Microservice Bootstrap",
              "language": "java",
              "content": "@SpringBootApplication\n@EnableDiscoveryClient\npublic class OrderServiceApplication {\n    public static void main(String[] args) {\n        SpringApplication.run(OrderServiceApplication.class, args);\n    }\n}\n\n// Bootstrap configuration for cloud environments\n@SpringBootApplication\n@EnableDiscoveryClient\n@EnableCircuitBreaker\n@EnableFeignClients\npublic class Application {\n    public static void main(String[] args) {\n        new SpringApplicationBuilder(Application.class)\n            .properties(\n                \"spring.application.name=order-service\",\n                \"server.port=0\",  // Random port for multiple instances\n                \"management.endpoints.web.exposure.include=*\"\n            )\n            .run(args);\n    }\n}"
            },
            "codeExplanations": {
              "english": "OrderServiceApplication dikhata hai minimal microservice service discovery enabled ke saath. Second example demonstrate karta hai programmatic configuration suitable for cloud environments: server.port=0 random ports use karta hai (essential local development ke liye multiple instances ke saath), aur full actuator exposure enable karta hai comprehensive monitoring. @EnableCircuitBreaker aur @EnableFeignClients activate karte hain resilience aur declarative HTTP clients."
            },
            "keyPoints": [
              "Embedded servers (Tomcat/Netty) eliminate external server dependencies",
              "Fat JARs simplify Docker containerization",
              "Actuator provides production-ready health and metrics endpoints",
              "Random port assignment (server.port=0) supports local scaling",
              "Spring Cloud integration enables distributed patterns"
            ],
            "extras": {
              "flowDiagram": "",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-20",
            "title": "Spring Cloud Integration",
            "explanations": {
              "english": "Spring Cloud provide karta hai tools common patterns build karne ke liye distributed systems mein. Config Server centralize karta hai external configuration across environments aur services. Gateway handle karta hai API routing, load balancing, aur cross-cutting concerns. Circuit Breaker (Resilience4j) prevent karta hai cascade failures. Sleuth aur Zipkin enable karte hain distributed tracing. OpenFeign simplify karta hai HTTP client creation. Ye components integrate karte hain Spring Boot ke saath via auto-configuration aur annotations, providing cohesive platform for cloud-native microservices bina vendor lock-in ke specific cloud providers ko."
            },
            "code": {
              "title": "Cloud Dependencies",
              "language": "xml",
              "content": "<dependencies>\n    <!-- Service Discovery -->\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n    </dependency>\n    \n    <!-- API Gateway -->\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-gateway</artifactId>\n    </dependency>\n    \n    <!-- Resilience -->\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-circuitbreaker-resilience4j</artifactId>\n    </dependency>\n    \n    <!-- Distributed Tracing -->\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-sleuth</artifactId>\n    </dependency>\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-sleuth-zipkin</artifactId>\n    </dependency>\n</dependencies>"
            },
            "codeExplanations": {
              "english": "Ye Spring Cloud starters provide karte hain microservices toolkit: Eureka Client service registration ke liye, Gateway API management ke liye, Resilience4j circuit breaking aur bulkheading ke liye, aur Sleuth Zipkin ke saath distributed tracing ke liye. Ye leverage karte hain Spring Boot auto-configuration minimize karne ke liye explicit configuration jabki enabling production-grade distributed patterns."
            },
            "keyPoints": [
              "Config Server: Centralized external configuration",
              "Gateway: Routing, filtering, and cross-cutting concerns",
              "Circuit Breaker: Resilience against service failures",
              "Sleuth: Distributed tracing and correlation IDs",
              "OpenFeign: Declarative HTTP clients with load balancing"
            ],
            "extras": {
              "flowDiagram": "Config Server -> Git Repo\nServices -> Eureka Registry\nGateway -> Services\nTraces -> Zipkin",
              "comparisonTable": [],
              "examples": []
            }
          },
          {
            "id": "micro-21",
            "title": "Observability",
            "explanations": {
              "english": "Observability microservices mein encompass karta hai metrics, logging, aur distributed tracing system behavior understand karne ke liye. Micrometer instrument karta hai code aur export karta hai metrics ko Prometheus, Datadog, ya CloudWatch ko. Distributed tracing (Sleuth/Zipkin ya OpenTelemetry ke saath) follow karta hai requests across service boundaries using correlation IDs (traceId, spanId) inject kiye MDC aur HTTP headers mein. Centralized logging aggregate karta hai logs saare services se ELK ya Splunk mein. Health indicators report karte hain service-specific status. Together, ye provide karte hain telemetry necessary diagnose karne failures, analyze performance bottlenecks, aur alert anomalies distributed architectures mein."
            },
            "code": {
              "title": "Distributed Tracing",
              "language": "java",
              "content": "@Configuration\npublic class TracingConfig {\n    \n    @Bean\n    public WebClient.Builder tracedWebClientBuilder(Tracer tracer) {\n        return WebClient.builder()\n            .filter((request, next) -> {\n                // Propagate trace IDs to downstream services\n                Span span = tracer.nextSpan().name(\"http.client\").start();\n                try (Tracer.SpanInScope ws = tracer.withSpanInScope(span)) {\n                    return next.exchange(\n                        ClientRequest.from(request)\n                            .header(\"X-B3-TraceId\", span.context().traceId())\n                            .header(\"X-B3-SpanId\", span.context().spanId())\n                            .build()\n                    ).doFinally(sig -> span.end());\n                }\n            });\n    }\n}\n\n// Log format includes trace/span IDs\n// %d{yyyy-MM-dd HH:mm:ss} [%thread] [%X{traceId},%X{spanId}] %-5level %logger - %msg%n\n// Output: 2024-01-15 10:00:00 [http-nio-8080-exec-1] [abc123,def456] INFO c.e.OrderService - Processing order"
            },
            "codeExplanations": {
              "english": "WebClient filter propagate karta hai tracing context (TraceId, SpanId) via HTTP headers (B3 propagation format) downstream services ko. Ye create karta hai trace tree spanning multiple services. Logs include karte hain ye IDs via MDC (%X{traceId}), allowing correlation log entries ka across distributed services. Zipkin aggregate karta hai ye spans visualize karne ke liye request flows aur latency."
            },
            "keyPoints": [
              "Micrometer for metrics (counters, timers, gauges)",
              "Distributed tracing follows requests across service boundaries",
              "TraceId and SpanId propagation via HTTP headers (B3 or W3C)",
              "Centralized logging (ELK/Splunk) aggregates logs from all pods",
              "Health endpoints (liveness/readiness) enable orchestration awareness"
            ],
            "extras": {
              "flowDiagram": "Request -> Gateway [Trace: abc, Span: 1] -> Order Service [Trace: abc, Span: 2] -> Inventory Service [Trace: abc, Span: 3]\nAll report to Zipkin -> Visualize trace tree",
              "comparisonTable": [
                {
                  "headers": [
                    "Signal",
                    "Tooling",
                    "Purpose"
                  ],
                  "rows": [
                    [
                      "Metrics",
                      "Micrometer/Prometheus",
                      "Performance, capacity"
                    ],
                    [
                      "Logging",
                      "ELK/Splunk",
                      "Debugging, audit trails"
                    ],
                    [
                      "Tracing",
                      "Zipkin/Jaeger",
                      "Request flow, latency"
                    ],
                    [
                      "Profiling",
                      "Async Profiler",
                      "CPU/Memory analysis"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-22",
            "title": "Resilience",
            "explanations": {
              "english": "Resilience patterns prevent karte hain cascade failures distributed systems mein jahan services doosri services pe depend karte hain. Circuit Breakers (Resilience4j) stop karte hain requests failing services ko, returning fallback responses ya errors immediately threshold failures ke baad, allowing recovery time. Bulkheads isolate karte hain failures limiting concurrent calls specific services ko, preventing ek slow dependency se saare threads exhaust ho jaaye. Retries exponential backoff ke saath handle karte hain transient failures. Timeouts prevent karte hain indefinite blocking. Rate limiters control karte hain throughput. Ye patterns combined ensure karte hain partial system functionality degradation rather than total system failure jab dependencies stressed ya unavailable ho."
            },
            "code": {
              "title": "Resilience4j Implementation",
              "language": "java",
              "content": "@Service\npublic class InventoryServiceClient {\n    private final InventoryFeignClient client;\n    \n    @CircuitBreaker(name = \"inventory\", fallbackMethod = \"getDefaultStock\")\n    @Retry(name = \"inventory\")\n    @Bulkhead(name = \"inventory\", type = Type.THREADPOOL)\n    @TimeLimiter(name = \"inventory\")\n    public CompletableFuture<StockStatus> checkStock(String productId) {\n        return CompletableFuture.supplyAsync(() -> client.checkAvailability(productId));\n    }\n    \n    public CompletableFuture<StockStatus> getDefaultStock(String productId, Exception ex) {\n        log.warn(\"Inventory service down, returning default for {}\", productId, ex);\n        return CompletableFuture.completedFuture(\n            StockStatus.builder().available(false).source(\"CACHE\").build()\n        );\n    }\n}\n\n// application.yml:\n// resilience4j:\n//   circuitbreaker:\n//     instances:\n//       inventory:\n//         slidingWindowSize: 10\n//         permittedNumberOfCallsInHalfOpenState: 3\n//         waitDurationInOpenState: 10s\n//         failureRateThreshold: 50\n//   timelimiter:\n//     instances:\n//       inventory:\n//         timeoutDuration: 2s"
            },
            "codeExplanations": {
              "english": "@CircuitBreaker open ho jaata hai 50% failure rate ke baad 10 calls mein, bypass karta hai inventory service 10 seconds ke liye half-open retries se pehle. @Retry attempt karta hai transient failures. @Bulkhead (threadpool) limit karta hai concurrent inventory calls thread starvation prevent karne ke liye. @TimeLimiter enforce karta hai 2-second timeouts. Fallback method return karta hai cached/default data jab circuit open hota hai, ensuring order processing continue karta hai even if inventory checking fail ho."
            },
            "keyPoints": [
              "Circuit Breaker: Fail fast when dependency is unhealthy",
              "Bulkhead: Limit resources per dependency (thread isolation)",
              "Retry: Handle transient failures with backoff",
              "Timeout: Prevent indefinite blocking",
              "Fallback: Provide degraded functionality during outages"
            ],
            "extras": {
              "flowDiagram": "Request -> Circuit Breaker (Closed) -> Inventory Service\n                    |\n                    -> Open -> Fallback (Cache/Default)",
              "comparisonTable": [
                {
                  "headers": [
                    "Pattern",
                    "Problem Solved",
                    "Action"
                  ],
                  "rows": [
                    [
                      "Circuit Breaker",
                      "Cascade failure",
                      "Stop calling failing service"
                    ],
                    [
                      "Bulkhead",
                      "Resource exhaustion",
                      "Isolate thread pools per service"
                    ],
                    [
                      "Retry",
                      "Transient failures",
                      "Retry with backoff"
                    ],
                    [
                      "Timeout",
                      "Latency accumulation",
                      "Fail if too slow"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          },
          {
            "id": "micro-23",
            "title": "Cloud-Native Patterns",
            "explanations": {
              "english": "Cloud-native patterns leverage karte hain platform capabilities scalability aur resilience ke liye. Externalized configuration (ConfigMaps/Secrets) separate karta hai code environment se. Stateless processes enable karte hain horizontal scaling; session data store hoti hai Redis ya databases mein. Health probes enable karte hain orchestration decisions. Graceful shutdown handle karta hai SIGTERM completing in-flight requests terminate karne se pehle. Event sourcing aur CQRS decouple karte hain read/write models. Sidecar pattern deploy karta hai auxiliary containers (proxies, log shippers) alongside app containers. Ye 12-Factor App principles ensure karte hain applications thrive karein dynamic, containerized environments mein ephemeral infrastructure ke saath."
            },
            "code": {
              "title": "Cloud-Native Implementation",
              "language": "java",
              "content": "@Component\npublic class GracefulShutdown implements ApplicationListener<ContextClosedEvent> {\n    private final ExecutorService executorService;\n    \n    @Override\n    public void onApplicationEvent(ContextClosedEvent event) {\n        log.info(\"Received shutdown signal, draining connections...\");\n        executorService.shutdown();\n        try {\n            if (!executorService.awaitTermination(30, TimeUnit.SECONDS)) {\n                executorService.shutdownNow();\n            }\n        } catch (InterruptedException e) {\n            executorService.shutdownNow();\n        }\n    }\n}\n\n// Externalized Session with Redis\n@Configuration\n@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)\npublic class SessionConfig {\n    @Bean\n    public LettuceConnectionFactory connectionFactory() {\n        return new LettuceConnectionFactory();\n    }\n}\n\n// Stateless service - no local state\n@Service\npublic class OrderService {\n    // All state externalized: Database for persistence, Redis for sessions/cache\n    // Service can be killed/restarted without data loss\n}"
            },
            "codeExplanations": {
              "english": "GracefulShutdown implement karta hai graceful degradation pattern, Kubernetes SIGTERM handle karta hai draining in-flight requests within 30 seconds force shutdown se pehle. @EnableRedisHttpSession externalize karta hai HTTP sessions ko Redis mein, making application stateless aur allowing horizontal scaling bina sticky sessions ke. OrderService contain nahi karta koi instance-specific state, adhering to 12-Factor App principles cloud-native deployments ke liye."
            },
            "keyPoints": [
              "Stateless processes: Share-nothing architecture enables horizontal scaling",
              "Externalized configuration: Environment variables and ConfigMaps",
              "Graceful shutdown: Handle SIGTERM to complete requests before exit",
              "Port binding: Export services via ports, not domain sockets",
              "Concurrency: Scale via process model (container replicas)",
              "Disposability: Fast startup and graceful shutdown for rapid scaling"
            ],
            "extras": {
              "flowDiagram": "Kubelet -> SIGTERM -> Graceful Shutdown (30s) -> Drain Requests -> Exit 0\n            -> SIGKILL (if stuck) -> Force Kill",
              "comparisonTable": [
                {
                  "headers": [
                    "Traditional",
                    "Cloud-Native"
                  ],
                  "rows": [
                    [
                      "Stateful sessions",
                      "Externalized to Redis"
                    ],
                    [
                      "File system storage",
                      "Object storage (S3)"
                    ],
                    [
                      "Static scaling",
                      "Dynamic horizontal scaling"
                    ],
                    [
                      "Deployment scripts",
                      "Container orchestration"
                    ],
                    [
                      "Fixed infrastructure",
                      "Ephemeral, replaceable instances"
                    ]
                  ]
                }
              ],
              "examples": []
            }
          }
        ]
      }
    ]
  }
]